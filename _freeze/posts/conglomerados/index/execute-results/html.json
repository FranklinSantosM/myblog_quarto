{
  "hash": "1cb464a6a9e8f7d7cd4a255854bb5f8b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Potato Diversity Analysis:A Step-by-Step Guide\"\ndescription: |\n  Comprehensive approach to analyzing the genetic diversity of potato using various statistical and multivariate analysis methods in R.\ncategories: [Diversity, multivariate, Plant Breeding]\nauthor:\n  - name: Franklin Santos\n    url: https://www.franklinsantosm.com\n    affiliation: Universidad Austral de Chile\n    orcid: 0000-0002-7509-2910\n    email: franklin25santos@gmail.com\ndate: \"2024-07-28\"\nimage: fs.png\ncitation: true\neditor_options: \n  chunk_output_type: console\n---\n\n```{=html}\n<style>\nbody {\ntext-align: justify}\n</style>\n```\n\n\n\n## Introduction\n\nIn this article, we will delve into a comprehensive approach to analyzing the genetic diversity of potato using various statistical and multivariate analysis methods in R. The analysis encompasses techniques such as **k-means clustering**, **hierarchical clustering**, **DBSCAN**, and **Principal Component Analysis (PCA)**, supported by clear visualizations to facilitate the interpretation of results. This guide aims to provide a robust framework for researchers and practitioners working in the field of plant genetics and diversity analysis.\n\n## Data Loading and Preparation\n\nThe first step in any analysis is to load and prepare the data. Here, we use R to read the data from an Excel file, select relevant columns, and scale the data. Scaling ensures that each variable contributes equally to the analysis, which is particularly important in clustering and PCA.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Load the dataset from an Excel file\ndf <- read_excel(\"/Users/franklin/Documents/R/myblog/posts/conglomerados/DATOS_R_JUAN JOSE.xlsx\")\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 54\n   `N°` codigo_colecta nombre_comun  Departamento Provincia Municipio Localidad\n  <dbl> <chr>          <chr>         <chr>        <chr>     <chr>     <chr>    \n1     1 JCH 001        JACHA         POTOSI       Bustillos Chayanta  Jacha    \n2     2 JCH 002        TANI TANI     POTOSI       Bustillos Chayanta  Jacha    \n3     3 JCH 003        TUNANTE       POTOSI       Bustillos Chayanta  Jacha    \n4     4 JCH 004        QOLLU  PEPINO POTOSI       Bustillos Chayanta  Jacha    \n5     5 JCH 005        QHOLLU YURAQ  POTOSI       Bustillos Chayanta  Jacha    \n6     6 JCH 006        QHOLLU PUCA   POTOSI       Bustillos Chayanta  Jacha    \n# ℹ 47 more variables: Latitud <dbl>, Longitud <dbl>, Altitud <dbl>,\n#   CAMPAÑA <dbl>, REGENERACION <chr>, BLOQ <dbl>, SURC <dbl>, RDTO <dbl>,\n#   NTUBER <dbl>, N_caja <dbl>, Ev_Almacen <chr>, HPL <dbl>, CTL <dbl>,\n#   FAL <dbl>, TDS <dbl>, NFL <dbl>, NIF <dbl>, NIP <dbl>, GFL <dbl>,\n#   FCL <dbl>, CPF <dbl>, INT <dbl>, CSF <dbl>, DCS <dbl>, CPL <dbl>,\n#   CLZ <dbl>, PAN <dbl>, PPT <dbl>, CBY <dbl>, FBY <dbl>, FGR <dbl>,\n#   FRA <dbl>, POJ <dbl>, CPPL <dbl>, INTC <dbl>, CSP <dbl>, DSCPL <dbl>, …\n```\n\n\n:::\n\n```{.r .cell-code}\n# Column selection for the analysis (excluding TDS due to lack of variation)\ndb <- df %>% \n  select(2, 4, 15, 16, 19:21, 23:47)\n\n# Data preparation: remove non-numeric columns and scale the data\nsdf <- db %>% \n  select(-2) %>% \n  column_to_rownames(\"codigo_colecta\")\n\nsdf <- scale(sdf)\nhead(sdf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              RDTO     NTUBER         HPL        CTL        FAL        NFL\nJCH 001 -1.0364935  2.3631177 -2.33535670  1.5974622  0.2021708 -0.2949048\nJCH 002 -1.0332533  1.3262372 -2.33535670  2.3526261 -3.0325614  1.3193108\nJCH 003 -0.8518015  0.2422258 -1.21610741 -0.6680296 -1.4151953 -0.2949048\nJCH 004 -0.4208535 -1.4309223 -0.09685811  0.0871343  0.2021708  2.9335265\nJCH 005 -1.2114649 -1.1010057 -0.09685811 -0.6680296 -1.4151953 -1.9091204\nJCH 006 -0.5277805 -0.2526490 -0.09685811  0.0871343  0.2021708 -0.2949048\n               NIF        NIP        GFL        FCL        CPF         INT\nJCH 001  2.0689872  1.3677450  2.6330006  0.3720828  0.3365174  0.08133921\nJCH 002  0.3475899  1.3677450  2.6330006 -2.7236462  0.3365174 -1.12712898\nJCH 003 -1.3738075 -0.7241003 -0.3761429  0.3720828 -0.4081168 -1.12712898\nJCH 004  2.0689872 -0.7241003 -0.3761429  0.3720828  0.3365174  1.28980739\nJCH 005 -1.3738075 -0.7241003 -0.3761429  0.3720828  0.3365174  0.08133921\nJCH 006 -1.3738075  1.3677450 -0.3761429  0.3720828  0.3365174 -1.12712898\n               CSF        DCS        CPL        CLZ        PAN         PPT\nJCH 001 -0.2849313 -0.3623917 -0.5231525 -0.4795775 -0.4143268  0.09001093\nJCH 002 -0.8140895 -1.2189538  1.4919534  0.9454527 -0.4143268  0.09001093\nJCH 003 -0.2849313  1.3507325 -1.5307055 -1.1920925 -0.4143268 -1.31415965\nJCH 004 -0.2849313 -0.3623917  1.4919534  0.9454527 -0.4143268  1.02612466\nJCH 005 -0.2849313 -0.3623917  0.9881770 -0.4795775  2.9002873 -0.84610279\nJCH 006 -0.2849313 -0.3623917 -0.5231525 -0.4795775  2.9002873  1.02612466\n               CBY        FBY       FGR        FRA       POJ       CPPL\nJCH 001  0.6475703 -0.2236502  0.364163 -0.6069687 -0.558308 -0.2817993\nJCH 002  0.6475703 -0.2236502  0.364163 -0.6069687 -0.558308 -1.3284824\nJCH 003  0.6475703 -0.2236502  0.364163 -0.6069687 -2.086309  0.4159894\nJCH 004  0.6475703 -0.2236502 -1.568130 -0.2798975 -0.558308 -0.9795880\nJCH 005 -1.2362705 -0.2236502  1.137080  0.7013162 -0.558308 -0.9795880\nJCH 006 -0.7653103 -0.2236502  1.137080  0.7013162 -2.086309  0.7648838\n              INTC        CSP      DSCPL        CPP       CSPL       DCSP\nJCH 001 -0.9149821  0.4009646  0.1217388  0.8023403 -0.4827367 -0.4143496\nJCH 002 -0.9149821 -0.2506029  0.1217388  0.8023403 -0.4827367 -0.4143496\nJCH 003  1.4940847 -1.8795218 -2.0801452  0.8023403 -0.4827367 -0.4143496\nJCH 004  0.2895513  0.4009646  0.1217388  2.4070210 -0.4827367 -0.4143496\nJCH 005  0.2895513  0.4009646 -1.5296742 -0.8023403 -0.4827367 -0.4143496\nJCH 006  0.2895513 -0.5763867  1.7731517  0.8023403 -0.4827367 -0.4143496\n```\n\n\n:::\n:::\n\n\n## Clustering: K-means Method\n\n**K-means clustering** is a partitioning method that divides data into a predetermined number of clusters, minimizing the within-cluster variance. The following steps include determining the optimal number of clusters and performing the clustering analysis.\n\n### Determining the Optimal Number of Clusters\n\nDetermining the optimal number of clusters is crucial for meaningful clustering. We employ several methods, including the **within-cluster sum of squares (WSS)**, **silhouette analysis**, and **Gap Statistic** to identify the most appropriate number of clusters for the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(factoextra)\n\n# Compute distance matrix and visualize\ndistance <- get_dist(sdf)\nfviz_dist(distance, gradient = list(low = \"#00AFBB\", mid = \"white\", high = \"#FC4E07\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Elbow method for WSS\nfviz_nbclust(sdf, kmeans, method = \"wss\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Silhouette method\nfviz_nbclust(sdf, kmeans, method = \"silhouette\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Gap Statistic\nlibrary(cluster)\nset.seed(123)\ngap_stat <- clusGap(sdf, FUN = kmeans, nstart = 25, K.max = 10, B = 50)\nprint(gap_stat, method = \"firstmax\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = sdf, FUNcluster = kmeans, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --> Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 5.279958 5.652502 0.3725440 0.01149949\n [2,] 5.228562 5.598937 0.3703751 0.01042953\n [3,] 5.183834 5.560603 0.3767692 0.01100671\n [4,] 5.148115 5.529050 0.3809349 0.01071279\n [5,] 5.116327 5.501342 0.3850155 0.01052125\n [6,] 5.088564 5.476447 0.3878835 0.01032929\n [7,] 5.063318 5.453360 0.3900420 0.01027040\n [8,] 5.039872 5.431245 0.3913732 0.01002459\n [9,] 5.015518 5.410428 0.3949102 0.01009456\n[10,] 4.996567 5.390239 0.3936718 0.01019443\n```\n\n\n:::\n\n```{.r .cell-code}\nfviz_gap_stat(gap_stat)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-4.png){width=672}\n:::\n:::\n\n\nThe **WSS** method, often referred to as the \"elbow method,\" helps to identify the point where the marginal gain in explanatory power diminishes significantly, indicating the optimal number of clusters. The **silhouette method** provides a measure of how similar an object is to its own cluster compared to other clusters. The **Gap Statistic** compares the total within intra-cluster variation for different numbers of clusters with their expected values under null reference distribution of the data.\n\n### Applying the K-means Algorithm\n\nAfter identifying the optimal number of clusters, we apply the k-means algorithm and visualize the clusters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nkmeans_resultados <- kmeans(sdf, centers = 4)  # Replace 4 with the optimal number of clusters\n\n# Visualization of clusters\nfviz_cluster(kmeans_resultados, data = sdf)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe results from k-means clustering provide insight into how the samples are grouped based on their genetic characteristics. Each cluster represents a group of samples that share similar traits, and the visualizations help in understanding the distribution of these clusters within the dataset.\n\n## Hierarchical Clustering\n\n**Hierarchical clustering** is another clustering technique that creates a hierarchy of clusters, which can be visualized as a tree or dendrogram. This method is particularly useful for visualizing the relationships between clusters and understanding the structure of the data.\n\n### Generating the Dendrogram\n\nWe begin by calculating the distance matrix using the Euclidean distance and applying the hierarchical clustering algorithm using Ward’s method, which minimizes the variance within clusters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dendextend)\n\n# Compute distance matrix\ndistancias <- dist(sdf, method = \"euclidean\")\n\n# Perform hierarchical clustering\nres.hc <- hclust(d = distancias, method = \"ward.D2\")\n\n# Plot dendrogram\nplot(res.hc, cex = 0.6, hang = -1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Cut the dendrogram to form clusters\ngrupos <- cutree(res.hc, k = 4)  # Replace 4 with the desired number of clusters\n```\n:::\n\n\nThe dendrogram allows us to visualize how the clusters are formed and how individual data points or groups of points merge as the level of clustering increases. By cutting the dendrogram at a particular height, we can extract a specific number of clusters that represent the underlying structure of the data.\n\n### Assessment and Visualization\n\nDifferent methods of hierarchical clustering can be compared to evaluate which method best captures the structure of the data. We then visualize the dendrogram with colored clusters for better interpretability.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Comparison of different linkage methods\nm <- c(\"average\", \"single\", \"complete\", \"ward\")\nnames(m) <- c(\"average\", \"single\", \"complete\", \"ward\")\n\nac <- function(x) {\n  agnes(sdf, method = x)$ac\n}\n\nmap_dbl(m, ac)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  average    single  complete      ward \n0.5463403 0.4630005 0.6197599 0.7498075 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize dendrogram with colored clusters\nfviz_dend(res.hc, cex = 0.5, k = 4, \n          rect = TRUE, \n          k_colors = \"jco\", \n          rect_border = \"jco\", \n          rect_fill = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at <https://github.com/kassambara/factoextra/issues>.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Circular\nfviz_dend(res.hc, cex = 0.6, lwd = 0.7, k = 4,\n                 rect = TRUE,\n                 k_colors = \"jco\",\n                 rect_border = \"jco\",\n                 rect_fill = TRUE,\n                 type = \"circular\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\nHere, we assess different linkage methods such as average, single, complete, and Ward’s method. The agglomerative coefficient (AC) is computed for each method, with higher values indicating a better fit. The final dendrogram is color-coded to represent the different clusters, providing a clear visual summary of the hierarchical structure.\n\n## Density-Based Clustering: DBSCAN\n\n**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** is a powerful clustering technique that identifies clusters based on density, allowing for the detection of outliers. Unlike k-means, DBSCAN does not require the number of clusters to be specified a priori, making it particularly useful for datasets with irregular cluster shapes.\n\n### Finding the Optimal Epsilon and Applying DBSCAN\n\nThe epsilon parameter controls the neighborhood radius for density estimation. We begin by identifying an appropriate epsilon value through a k-NN distance plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dbscan)\n\n# Identify optimal epsilon using k-NN distance plot\nkNNdistplot(sdf, k = 5)\nabline(h = 0.5, col = \"red\")  # Adjust according to the plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Apply DBSCAN algorithm\ndbscan_resultados <- dbscan(sdf, eps = 0.5, minPts = 5)\n\n# Visualize DBSCAN results\nfviz_cluster(dbscan_resultados, data = sdf)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\nThe k-NN distance plot helps to identify a natural choice for epsilon by observing where the plot exhibits the most pronounced change in slope. The DBSCAN algorithm is then applied, and the results are visualized. Clusters are identified based on dense regions of points, with outliers being points that do not belong to any cluster.\n\n## Principal Component Analysis (PCA)\n\n**Principal Component Analysis (PCA)** is a dimensionality reduction technique that transforms the data into a set of orthogonal components, explaining the maximum variance with the fewest components. PCA is especially useful for visualizing high-dimensional data in two or three dimensions.\n\n### Performing PCA and Visualizing Results\n\nWe perform PCA on the scaled data and visualize the results, focusing on the variance explained by each component and the relationship between variables and samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform PCA\nacp_resultados <- prcomp(sdf, center = TRUE, scale. = TRUE)\n\n# Summary of PCA results\nsummary(acp_resultados)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8711 1.7506 1.57675 1.44454 1.32993 1.26967 1.25193\nProportion of Variance 0.1167 0.1022 0.08287 0.06956 0.05896 0.05374 0.05224\nCumulative Proportion  0.1167 0.2189 0.30173 0.37129 0.43024 0.48398 0.53622\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.14515 1.09504 1.04044 1.01979 0.97386 0.93209 0.92048\nProportion of Variance 0.04371 0.03997 0.03608 0.03467 0.03161 0.02896 0.02824\nCumulative Proportion  0.57994 0.61991 0.65599 0.69066 0.72227 0.75123 0.77947\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.87393 0.81854 0.81299 0.77542 0.75455 0.68666 0.65538\nProportion of Variance 0.02546 0.02233 0.02203 0.02004 0.01898 0.01572 0.01432\nCumulative Proportion  0.80493 0.82726 0.84930 0.86934 0.88832 0.90403 0.91835\n                         PC22    PC23    PC24    PC25    PC26   PC27    PC28\nStandard deviation     0.6318 0.61667 0.58666 0.54725 0.53177 0.4837 0.48009\nProportion of Variance 0.0133 0.01268 0.01147 0.00998 0.00943 0.0078 0.00768\nCumulative Proportion  0.9316 0.94433 0.95580 0.96579 0.97521 0.9830 0.99069\n                          PC29    PC30\nStandard deviation     0.43358 0.30199\nProportion of Variance 0.00627 0.00304\nCumulative Proportion  0.99696 1.00000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Scree plot showing explained variance by each component\nfviz_eig(acp_resultados)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot of individuals in the space of the first two principal components\nfviz_pca_ind(acp_resultados, geom.ind = \"point\", pointshape = 21, pointsize = 2, fill.ind = \"blue\", col.ind = \"black\", repel = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot of variables in the space of the first two principal components\nfviz_pca_var(acp_resultados, col.var = \"contrib\", gradient.cols = c(\"blue\", \"yellow\", \"red\"), repel = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Biplot showing both individuals and variables\nfviz_pca_biplot(acp_resultados, repel = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-4.png){width=672}\n:::\n:::\n\n\nThe scree plot shows the proportion of variance explained by each principal component, aiding in the selection of the number of components to retain. The PCA biplots allow us to interpret the contribution of variables to the principal components and observe the distribution of samples along these components. This visualization is key to understanding the underlying structure of the data and the relationships between different genetic traits.\n\n## Multiple Factor Analysis (MFA)\n\nFor datasets containing both quantitative and qualitative variables, **Multiple Factor Analysis (MFA)** is applied. MFA is an extension of PCA that can handle mixed data types and is particularly useful in genetics studies where both types of data are often present.\n\n### Performing MFA and Interpreting Results\n\nWe perform MFA on the mixed dataset and visualize the results, focusing on the contributions of quantitative variables and the distribution of individuals across groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert character variables to factors\ndf2 <-\n\n db %>% \n  column_to_rownames(\"codigo_colecta\") %>% \n  mutate_if(is.character, as.factor)\n\nlibrary(FactoMineR)\n\n# Perform MFA\nres.famd <- FAMD(df2, graph = F)\nprint(res.famd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n*The results are available in the following objects:\n\n  name          description                             \n1 \"$eig\"        \"eigenvalues and inertia\"               \n2 \"$var\"        \"Results for the variables\"             \n3 \"$ind\"        \"results for the individuals\"           \n4 \"$quali.var\"  \"Results for the qualitative variables\" \n5 \"$quanti.var\" \"Results for the quantitative variables\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Scree plot showing the percentage of variance explained by each component\nfviz_screeplot(res.famd)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot showing the contribution of quantitative variables\nquanti.var <- get_famd_var(res.famd, \"quanti.var\")\nfviz_famd_var(res.famd, \"quanti.var\", repel = TRUE, col.var = \"black\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Visualization of individuals by group with confidence ellipses\nfviz_mfa_ind(res.famd, \n             habillage = \"Departamento\",\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\", \"darkgreen\"),\n             addEllipses = F, \n             ellipse.type = \"confidence\", \n             repel = TRUE,\n             label = \"none\"\n             )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n:::\n\n\nThe MFA analysis allows us to decompose the data into factors that explain the most variation, considering both types of variables. The scree plot helps determine how many factors to retain, while the visualizations provide insights into the contributions of individual variables and the grouping of samples based on both qualitative and quantitative characteristics.\n\n## Conclusion\n\nThis workflow provides a comprehensive and detailed framework for conducting genetic diversity analysis in potato using various clustering and multivariate analysis techniques in R. By employing different methods, researchers can gain a deeper understanding of the genetic structure within their dataset, identify meaningful patterns of variation, and ultimately make informed decisions in breeding programs and conservation efforts. The choice of method depends on the specific nature of the data and the research objectives, whether it is to identify distinct genetic clusters, reduce data dimensionality, or both.\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}