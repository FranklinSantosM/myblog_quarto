{
  "hash": "8628072f33f940270748b0f6317abeaa",
  "result": {
    "markdown": "---\ntitle: \"Análisis de Componentes Principales\"\ndescription: |\n  El análisis de componentes principales nos permite resumir y visualizar la información de un conjunto de datos que contiene observaciones descritos por múltiples variables cuantitativas inter-correlacionadas.\ncategories:\n  - Biplot\n  - Componentes Principales\n  - Correlation\nauthor:\n  - name: Franklin Santos\n    url: https://franklinsantos.com\n    affiliation: AgriTech Bolivia\n    orcid: 0000-0002-7509-2910\ndate: \"2020-12-15\"\nimage: featured.png\n---\n\n```{=html}\n<style>\nbody {\ntext-align: justify}\n</style>\n```\n\n## Introducción\n\nEl **análisis de componentes principales (PCA)** nos permite resumir y visualizar la información en un conjunto de datos que contiene individuos/observaciones descritos por múltiples variables cuantitativas inter-correlacionadas. Cada variable podría considerarse como una dimensión diferente. Si tiene más de 3 variables en sus conjuntos de datos, podría ser muy difícil visualizar un hiperespacio multidimensional.\n\nEl análisis de componentes principales se utiliza para extraer la información importante de una tabla de datos multivariados y para expresar esta información como un conjunto de algunas variables nuevas llamadas **componentes principales**. Estas nuevas variables corresponden a una combinación lineal de las originales. El número de componentes principales es menor o igual al número de variables originales.\n\nLa información de un conjunto de datos dado corresponde a la *variación total* que contiene. El objetivo de PCA es identificar direcciones (o componentes principales) a lo largo de los cuales la variación en los datos es máxima.\n\nEn otras palabras, PCA reduce la dimensionalidad de un dato multivariado a dos o tres componentes principales, que se pueden visualizar gráficamente, con una mínima pérdida de información.\n\n> En este capítulo, describimos la idea básica de PCA y demostramos cómo calcular y visualizar PCA usando el software R. Además, mostraremos cómo identificar las variables más importantes que explican las variaciones en un conjunto de datos.\n\nEn conjunto, el objetivo principal del análisis de componentes principales es:\n\n> -   identificar patrones ocultos en un conjunto de datos,\n> -   reducir la dimensionalidad de los datos **eliminando el ruido** y la **redundancia** en los datos,\n> -   identificar variables correlacionadas\n\n## Metodología de análisis\n\nInstale los dos paquetes de la siguiente manera:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(c(\"FactoMineR\", \"factoextra\"))\n```\n:::\n\n\nCargar paquetes en R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n```\n:::\n:::\n\n\n### Cargando datos\n\nUsaremos los conjuntos de datos de demostración **decathlon2** del paquete `factoextra`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(decathlon2)\n#head(decathlon2)\n```\n:::\n\n\nDe esta base de datos seleccionamos variables cuantitativas para el análisis de componentes principales:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#seleccionamos 27 filas y las primeras 10 variables\ndecathlon2.active <- decathlon2[1:23, 1:10]\nhead(decathlon2.active[, 1:7], 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        X100m Long.jump Shot.put High.jump X400m X110m.hurdle Discus\nSEBRLE  11.04      7.58    14.83      2.07 49.81        14.69  43.75\nCLAY    10.76      7.40    14.26      1.86 49.37        14.05  50.72\nBERNARD 11.02      7.23    14.25      1.92 48.93        14.99  40.87\nYURKOV  11.34      7.09    15.19      2.10 50.42        15.31  46.26\n```\n:::\n:::\n\n\n### Código R\n\nSe puede utilizar la función `PCA()` \\[paquete *FactoMineR*\\]. Un formato simplificado es:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPCA(X, scale.unit = TRUE, ncp = 5, graph = TRUE)\n```\n:::\n\n\n> -   **X**: un marco de datos. Las filas son individuos y las columnas son variables numéricas\n> -   **scale.unit**: un valor lógico. Si es `TRUE`, los datos se escalan a la varianza de la unidad antes del análisis. Esta estandarización a la misma escala evita que algunas variables se conviertan en dominantes solo por sus grandes unidades de medida. Hace que la variable sea comparable.\n> -   **ncp**: número de dimensiones mantenidas en los resultados finales.\n> -   **graph**: un valor lógico. Si es `TRUE`, se muestra un gráfico.\n\nEl código R a continuación, calcula el análisis de componentes principales de las variables seleccionadas o activas:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"FactoMineR\")\nres.pca <- PCA(decathlon2.active, graph = FALSE)\n```\n:::\n\n\n> El objeto que se crea usando la función `PCA()` contiene mucha información que se encuentra en muchas listas y matrices diferentes. Estos valores se describen en la siguiente sección.\n\n### Extraiga y visualice valores propios/varianzas:\n\nLos valores propios o varianzas miden la cantidad de variación retenida por cada componente principal. Las varianzas son grandes para los primeros componentes y pequeños para las siguientes. Es decir, las primeras PC corresponden a las direcciones con la cantidad máxima de variación en el conjunto de datos.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extraer valores propios/varianzas\nget_eig(res.pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       eigenvalue variance.percent cumulative.variance.percent\nDim.1   4.1242133        41.242133                    41.24213\nDim.2   1.8385309        18.385309                    59.62744\nDim.3   1.2391403        12.391403                    72.01885\nDim.4   0.8194402         8.194402                    80.21325\nDim.5   0.7015528         7.015528                    87.22878\nDim.6   0.4228828         4.228828                    91.45760\nDim.7   0.3025817         3.025817                    94.48342\nDim.8   0.2744700         2.744700                    97.22812\nDim.9   0.1552169         1.552169                    98.78029\nDim.10  0.1219710         1.219710                   100.00000\n```\n:::\n\n```{.r .cell-code}\n# Visualizar valores propios/variaciones\nfviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nLa suma de todos los valores propios da una varianza total de 10.\n\nLa proporción de variación explicada por cada valor propio se da en la segunda columna. Por ejemplo, 4,124 dividido por 10 es igual a 0,4124, o aproximadamente el 41,24% de la variación se explica por este primer valor propio. El porcentaje acumulado explicado se obtiene sumando las sucesivas proporciones de variación explicadas para obtener el total acumulado. Por ejemplo, 41,242% más 18,385% es igual a 59,627%, y así sucesivamente. Por lo tanto, aproximadamente el 59,627% de la variación se explica por los dos primeros valores propios juntos.\n\nLos valores propios se pueden utilizar para determinar la cantidad de componentes principales que se deben retener después de la PCA (Kaiser 1961):\n\n> -   Un *valor propio* \\> 1 indica que los PCs representan más varianza que la contabilizada por una de las variables originales en los datos estandarizados. Esto se usa comúnmente como un punto de corte para el cual se retienen las PCs. Esto es válido solo cuando los datos están estandarizados.\n> -   También puede limitar el número de componentes a ese número que representa una cierta fracción de la varianza total. Por ejemplo, si está satisfecho con el 70% de la varianza total explicada, utilice el número de componentes para lograrlo.\n\nDesafortunadamente, no existe una forma objetiva bien aceptada de decidir cuántos componentes principales son suficientes. Esto dependerá del campo de aplicación específico y del conjunto de datos específico. En la práctica, tendemos a mirar los primeros componentes principales para encontrar patrones interesantes en los datos.\n\nEn nuestro análisis, los tres primeros componentes principales explican el 72% de la variación. Este es un porcentaje aceptablemente grande.\n\n### Gráfica círculo de correlación\n\nLa correlación entre una variable y un componente principal (PC) se utiliza como coordenadas de la variable en la PC. La representación de las variables difiere del gráfico de las observaciones: las observaciones están representadas por sus proyecciones, pero las variables están representadas por sus correlaciones (Abdi y Williams 2010).\n\nEs posible controlar los colores de las variables usando sus contribuciones (\"contrib\") a los ejes principales:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Control variable colors using their contributions\nfviz_pca_var(res.pca, col.var=\"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE # Avoid text overlapping\n             )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nLa gráfica anterior también se conoce como gráficas de correlación variable. Muestra las relaciones entre todas las variables. Se puede interpretar de la siguiente manera:\n\n-   Las variables correlacionadas positivamente se agrupan.\n-   Las variables correlacionadas negativamente se colocan en lados opuestos del origen de la gráfica (cuadrantes opuestos).\n-   La distancia entre las variables y el origen mide la calidad de las variables en el mapa de factores. Las variables que están lejos del origen están bien representadas en el mapa de factores.\n\n### Contribucion de las variables a los ejes principales:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Contributions of variables to PC1\nfviz_contrib(res.pca, choice = \"var\", axes = 1, top = 10)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Contributions of variables to PC2\nfviz_contrib(res.pca, choice = \"var\", axes = 2, top = 10)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\nLa línea discontinua roja en el gráfico anterior indica la contribución promedio esperada. Si la contribución de las variables fuera uniforme, el valor esperado sería 1/longitud(variables) = 1/10 = 10%. Para un componente dado, una variable con una contribución mayor que este límite podría considerarse importante para contribuir al componente.\n\n> Se puede observar que las variables `X100m`, `Long.jump` y `Pole.vault` contribuyen más a las dimensiones 1 y 2.\n\n### Biplot\n\nPara hacer un biplot simple de individuos y variables, escriba esto:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Biplot of individuals and variables\nfviz_pca_biplot(res.pca, repel = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nEn biplot, debe centrarse principalmente en la dirección de las variables pero no en sus posiciones absolutas en la gráfica.\n\nEn términos generales, un biplot se puede interpretar de la siguiente manera:\n\n-   un individuo que está en el mismo lado de una variable dada tiene un valor alto para esta variable;\n-   un individuo que está en el lado opuesto de una variable dada tiene un valor bajo para esta variable.\n\nNota: Este post fue extraido del libro **\"Multivariate Analisis II\"**\n\n## Referencias\n\nKassambara A. 2017. Multivariate Analysis: Practical Guide to Principal Component Methods in R.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}