[
  {
    "objectID": "posts/maps/index.html",
    "href": "posts/maps/index.html",
    "title": "Mapas de Bolivia",
    "section": "",
    "text": "El paquete rnaturalearth es una herramienta excelente para mantener y facilitar la interacción con los datos cartográficos de Natural Earth. Para producir mapas con ggplot2, se necesitan los siguientes paquetes.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rnaturalearth)\n\n\n# America del sur\nsam = ne_countries(continent = \"south america\",\n                   returnclass = \"sf\",\n                   scale = 50)\np1 = ggplot() +\n  geom_sf(data = sam, fill = \"white\") +\n  theme_light() +\n  xlim(c(-90, -35))\n\n# Imagen de Bolivia y destacar La Paz\n\nbolivia = ne_states(country = \"bolivia\", returnclass = \"sf\") %>%\n  mutate(scat = ifelse(postal == \"LP\", \"La Paz\", \"Otros\"))\n\np2 = \n  p1 +\n  geom_sf(data = bolivia, aes(fill = scat))\np2"
  },
  {
    "objectID": "posts/Analisis_correspondencia/index.html",
    "href": "posts/Analisis_correspondencia/index.html",
    "title": "Análisis de correspondencia con R",
    "section": "",
    "text": "El análisis de correspondencia (CA) es una extensión del análisis de componentes principales (Capítulo 4) adecuado para explorar relaciones entre variables cualitativas (o datos categóricos). Al igual que el análisis de componentes principales, proporciona una solución para resumir y visualizar conjuntos de datos en gráficos de dos dimensiones.\nAquí, describimos el análisis de correspondencia simple, que se utiliza para analizar frecuencias formadas por dos datos categóricos, una tabla de datos conocida como tabla de contingencia, tambien se conoce como tablas cruzadas. Proporciona puntuaciones de factores (coordenadas) para los puntos de fila y columna de la tabla de contingencia. Estas coordenadas se utilizan para visualizar gráficamente la asociación entre elementos de fila y columna en la tabla de contingencia.\nAl analizar una tabla de contingencia bidireccional, una pregunta típica es si ciertos elementos de fila están asociados con algunos elementos de elementos de columna. El análisis de correspondencia es un enfoque geométrico para visualizar las filas y columnas de una tabla de contingencia bidireccional como puntos en un espacio de baja dimensión, de modo que las posiciones de los puntos de fila y columna sean consistentes con sus asociaciones en la tabla. El objetivo es tener una visión global de los datos que sea útil para la interpretación.\nEn el capítulo actual, mostraremos cómo calcular e interpretar el análisis de correspondencia usando dos paquetes R: i) FactoMineR para el análisis y ii) factoextra para la visualización de datos. Además, mostraremos cómo revelar las variables más importantes que explican las variaciones en un conjunto de datos."
  },
  {
    "objectID": "posts/Analisis_correspondencia/index.html#procedimiento-computacional-en-r",
    "href": "posts/Analisis_correspondencia/index.html#procedimiento-computacional-en-r",
    "title": "Análisis de correspondencia con R",
    "section": "Procedimiento computacional en R",
    "text": "Procedimiento computacional en R\nCargar paquetes\n\nlibrary(FactoMineR)\nlibrary(factoextra)\n\n\nFormato de datos\nLos datos deben ser una tabla de contingencia (resultados de tablas cruzadas). Usaremos los conjuntos de datos de demostración de tareas domésticas disponibles en el paquete factoextra R.\n\ndata(housetasks)\n\nLos datos son una tabla de contingencia que contiene 13 tareas del hogar y su reparto en la pareja:\n\nlas filas son las diferentes tareas\nLos valores son las frecuencias de las tareas realizadas:\n\nsolo por la esposa “wife only”\nalternativamente “Alternating”\nsolo por el marido “husband only”\no conjuntamente “jointly\n\n\nLos datos se ilustran en la siguiente salida:\n\n\n           Wife Alternating Husband Jointly\nLaundry     156          14       2       4\nMain_meal   124          20       5       4\nDinner       77          11       7      13\nBreakfeast   82          36      15       7\nTidying      53          11       1      57\nDishes       32          24       4      53\nShopping     33          23       9      55\nOfficial     12          46      23      15\nDriving      10          51      75       3\nFinances     13          13      21      66\nInsurance     8           1      53      77\nRepairs       0           3     160       2\nHolidays      0           1       6     153\n\n\n\n\nGráfica de tablas de contingencia y prueba de chi-cuadrado\nLa tabla de contingencia anterior no es muy grande. Por lo tanto, es fácil inspeccionar e interpretar visualmente los perfiles de filas y columnas:\n\nEs evidente que las tareas de la casa (lavandería, comida principal y cena) las realiza con más frecuencia la “esposa”.\nLas reparaciones y la conducción las realiza predominantemente el marido.\nLos días festivos se asocian con frecuencia con la columna “conjuntamente”\n\nEl análisis de datos exploratorios y la visualización de tablas de contingencia se cubrieron en nuestro artículo anterior: Prueba de independencia de chi-cuadrado en R. Brevemente, la tabla de contingencia se puede visualizar utilizando las funciones balloonplot() [paquete gplots] y mosaicplot() [paquete graphics]:\n\nlibrary(gplots)\n#1. Convierte el dato como una tabla\ndt <- as.table(as.matrix(housetasks))\n# 2. Graph\nballoonplot(t(dt), main =\"housetasks\", xlab =\"\", ylab=\"\",\n            label = FALSE, show.margins = FALSE)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTenga en cuenta que las sumas de filas y columnas se imprimen de forma predeterminada en los márgenes inferior y derecho, respectivamente. Estos valores están ocultos, en el gráfico anterior, utilizando el argumento show.margins = FALSE.\n\n\nPara una tabla de contingencia pequeña, puede utilizar la prueba de chi-cuadrado para evaluar si existe una dependencia significativa entre las categorías de filas y columnas:\n\nchisq <- chisq.test(housetasks)\nchisq\n\n\n    Pearson's Chi-squared test\n\ndata:  housetasks\nX-squared = 1944.5, df = 36, p-value < 2.2e-16\n\n\n\n\n\n\n\n\n\n\n\n\nEn nuestro ejemplo, las variables de fila y columna están asociadas de manera estadísticamente significativa (valor p = r chisq$p.value).\n\n\n\n\nCódigo R para calcular CA (Analisis de Correspondencia)\nSe puede utilizar la función CA() [paquete FactoMiner]. Un formato simplificado es:\n\nCA(X, ncp = 5, graph = TRUE)\n\n\n\nX: es un marco de datos (tabla de contingencia)\nncp: número de dimensiones mantenidas en los resultados finales.\ngraph: un valor lógico. Si es TRUE, se muestra un gráfico.\n\n\nPara calcular el análisis de correspondencia, escriba esto:\n\nlibrary(\"FactoMineR\")\n#housetasks es nuestro marco de datos\nres.ca <- CA(housetasks, graph = FALSE)\n\nLa salida de la función CA() es una lista que incluye:\n\nprint(res.ca)\n\n**Results of the Correspondence Analysis (CA)**\nThe row variable has  13  categories; the column variable has 4 categories\nThe chi square of independence between the two variables is equal to 1944.456 (p-value =  0 ).\n*The results are available in the following objects:\n\n   name              description                   \n1  \"$eig\"            \"eigenvalues\"                 \n2  \"$col\"            \"results for the columns\"     \n3  \"$col$coord\"      \"coord. for the columns\"      \n4  \"$col$cos2\"       \"cos2 for the columns\"        \n5  \"$col$contrib\"    \"contributions of the columns\"\n6  \"$row\"            \"results for the rows\"        \n7  \"$row$coord\"      \"coord. for the rows\"         \n8  \"$row$cos2\"       \"cos2 for the rows\"           \n9  \"$row$contrib\"    \"contributions of the rows\"   \n10 \"$call\"           \"summary called parameters\"   \n11 \"$call$marge.col\" \"weights of the columns\"      \n12 \"$call$marge.row\" \"weights of the rows\"         \n\n\n\nEl objeto que se crea usando la función CA() contiene mucha información que se encuentra en muchas listas y matrices diferentes. Estos valores se describen en la siguiente sección."
  },
  {
    "objectID": "posts/Analisis_correspondencia/index.html#visualización-e-interpretación",
    "href": "posts/Analisis_correspondencia/index.html#visualización-e-interpretación",
    "title": "Análisis de correspondencia con R",
    "section": "Visualización e interpretación",
    "text": "Visualización e interpretación\nUsaremos las siguientes funciones en factoextra para ayudar en la interpretación y visualización del análisis de correspondencia:\n\nget_eigenvalue(res.ca): Extraiga los autovalores/varianzas retenidos por cada dimensión (eje)\nfviz_eig(res.ca): Visualiza los valores propios\nget_ca_row(res.ca), get_ca_col(res.ca): Extrae los resultados para filas y columnas, respectivamente.\nfviz_ca_row(res.ca), fviz_ca_col(res.ca): Visualiza los resultados para filas y columnas, respectivamente.\nfviz_ca_biplot(res.ca): Crea un biplot de filas y columnas.\n\nEn las siguientes secciones, ilustraremos cada una de estas funciones.\n\nSignificación estadística\nPara interpretar el análisis de correspondencia, el primer paso es evaluar si existe una dependencia significativa entre las filas y las columnas.\nUn método riguroso consiste en utilizar la estadística de chi-cuadrado para examinar la asociación entre las variables de fila y columna. Esto aparece en la parte superior y es generado por la función summary(res.ca) o print(res.ca), ver sección “Gráfica de tablas de contingencia y prueba de chi-cuadrado”. Una estadística de chi-cuadrado alta significa un vínculo fuerte entre las variables de fila y columna o sea, la probabilidad de chi cuadrado deberá ser inferior a 0.05.\n\nEn nuestro ejemplo, la asociación es muy significativa (chi-cuadrado: 1944,456, p = 0).\n\n\n# Chi-square statistics\nchi2 <- 1944.456\nchi2\n\n[1] 1944.456\n\n# Degree of freedom\ndf <- (nrow(housetasks) - 1) * (ncol(housetasks) - 1)\ndf\n\n[1] 36\n\n# P-value\npval <- pchisq(chi2, df = df, lower.tail = FALSE)\npval\n\n[1] 0\n\n\n\n\nValores propios/Varianzas\nRecuerde que examinamos los valores propios para determinar el número de ejes a considerar. Los valores propios y la proporción de varianzas retenidas por los diferentes ejes se pueden extraer utilizando la función get_eigenvalue() [paquete factoextra]. Los valores propios son grandes para el primer eje y pequeños para el eje siguiente.\n\nlibrary(\"factoextra\")\neig.val <- get_eigenvalue(res.ca)\neig.val\n\n      eigenvalue variance.percent cumulative.variance.percent\nDim.1  0.5428893         48.69222                    48.69222\nDim.2  0.4450028         39.91269                    88.60491\nDim.3  0.1270484         11.39509                   100.00000\n\n\nLos valores propios corresponden a la cantidad de información retenida por cada eje. Las dimensiones se ordenan de forma decreciente y se enumeran de acuerdo con la cantidad de variación explicada en la solución. La dimensión 1 explica la mayor variación en la solución, seguida de la dimensión 2 y así sucesivamente.\nEl porcentaje acumulado explicado se obtiene sumando las sucesivas proporciones de variación explicadas para obtener el total acumulado. Por ejemplo, 48,69% más 39,91% es igual a 88,6% y así sucesivamente. Por lo tanto, aproximadamente el 88,6% de la variación se explica por las dos primeras dimensiones.\nLos valores propios se pueden utilizar para determinar el número de ejes que se deben retener. No existe una “regla general” para elegir el número de dimensiones que se deben mantener para la interpretación de los datos. Depende de la pregunta de investigación y la necesidad del investigador. Por ejemplo, si está satisfecho con el 80% de las variaciones totales explicadas, utilice la cantidad de dimensiones necesarias para lograrlo.\n\n\n\n\n\n\nTenga en cuenta\n\n\n\nSe logra una buena reducción de dimensión cuando las primeras dimensiones representan una gran proporción de la variabilidad.\n\n\nEn nuestro análisis, los dos primeros ejes explican el 88,6% de la variación. Este es un porcentaje aceptablemente grande.\nUn método alternativo para determinar el número de dimensiones es mirar una gráfica de Scree Plot, que es el diagrama de valores propios/varianzas ordenados de mayor a menor. El número de componentes se determina en el punto, más allá del cual los valores propios restantes son todos relativamente pequeños y de tamaño comparable.\nEl gráfico Scree plot se puede realizar usando la función fviz_eig() o fviz_screeplot() del paquete factoextra.\n\nfviz_screeplot(res.ca, addlabels = TRUE, ylim = c(0, 50))\n\n\n\n\n\n\n\n\n\n\nConsideración\n\n\n\nSe puede considerar que el punto en el que el gráfico de pedregal muestra una curvatura (denominado “codo”) indica una dimensionalidad óptima.\n\n\nTambién es posible calcular un valor propio promedio por encima del cual el eje debe mantenerse en la solución.\n\n\nNuestros datos contienen 13 filas y 4 columnas.\nSi los datos fueran aleatorios, el valor esperado del valor propio para cada eje sería 1/(nrow(housetasks)-1) = 1/12 = 8,33% en términos de filas.\nAsimismo, el eje promedio debe representar 1/(ncol(housetasks)-1) = 1/3 = 33,33% en términos de las 4 columnas.\n\n\n\n\n\n\n\n\nSegún (M. T. Bendixen 1995):\n\n\n\nCualquier eje con una contribución mayor que el máximo de estos dos porcentajes debe ser considerado como importante e incluido en la solución para la interpretación de los datos.\n\n\nEl código R a continuación, dibuja el gráfico de pantalla con una línea discontinua roja que especifica el valor propio promedio:\n\nfviz_screeplot(res.ca) +\n  geom_hline(yintercept=33.33, linetype=2, color=\"red\")\n\n\n\n\nSegún el gráfico anterior, solo las dimensiones 1 y 2 deben usarse en la solución. La dimensión 3 explica sólo el 11,4% de la inercia total, que está por debajo del valor propio medio (33,33%) y es demasiado poco para guardarlo para un análisis más detallado.\n\nTenga en cuenta que puede utilizar más de 2 dimensiones. Sin embargo, es poco probable que las dimensiones complementarias contribuyan de manera significativa a la interpretación de la naturaleza de la asociación entre filas y columnas.\n\nLas dimensiones 1 y 2 explican aproximadamente el 48,7% y el 39,9% de la inercia total, respectivamente. Esto corresponde a un total acumulado del 88,6% de la inercia total retenida por las 2 dimensiones. Cuanto mayor sea la retención, más sutileza en los datos originales se retiene en la solución de baja dimensión (M. Bendixen 2003).\n\n\nBiplot\nLa función fviz_ca_biplot() del paquete factoextra se puede utilizar para dibujar el biplot de las variables de filas y columnas.\n\n# repel= TRUE para evitar la superposición de texto (lenta si tiene muchos puntos)\nfviz_ca_biplot(res.ca, repel = TRUE)\n\n\n\n\nEl gráfico anterior se llama gráfico simétrico y muestra un patrón global dentro de los datos. Las filas están representadas por puntos azules y las columnas por triángulos rojos.\nLa distancia entre cualquier punto de fila o columna da una medida de su similitud (o disimilitud). Los puntos de fila con un perfil similar se cierran en el mapa de factores. Lo mismo es válido para los puntos de columna.\n\n\n\n\n\n\nEste gráfico muestra que:\n\n\n\n\nLas tareas de la casa, como la cena, el desayuno y la ropa, las hace la esposa con más frecuencia.\nLa conducción y las reparaciones las realiza el marido.\nEl gráfico simétrico representa los perfiles de fila y columna simultáneamente en un espacio común. En este caso, solo se puede interpretar realmente la distancia entre puntos de fila o la distancia entre puntos de columna.\n¡La distancia entre cualquier elemento de fila y columna no es significativa! Solo puede hacer declaraciones generales sobre el patrón observado.\nPara interpretar la distancia entre los puntos de columna y fila, los perfiles de columna deben presentarse en el espacio de fila o viceversa. Este tipo de mapa se llama biplot asimétrico y se analiza al final de este artículo.\n\n\n\nEl siguiente paso para la interpretación es determinar qué variables de fila y columna contribuyen más en la definición de las diferentes dimensiones retenidas en el modelo.\n\n\nOpciones de biplot\nBiplot es una visualización gráfica de filas y columnas en 2 o 3 dimensiones. Ya hemos descrito cómo crear CA biplots en la sección anterior. Aquí, describiremos diferentes tipos de biplots de CA.\n\nBiplot simétrico\nComo se mencionó anteriormente, el gráfico estándar del análisis de correspondencia es un biplot simétrico en el que tanto las filas (puntos azules) como las columnas (triángulos rojos) se representan en el mismo espacio utilizando las coordenadas principales. Estas coordenadas representan los perfiles de fila y columna. En este caso, solo se puede interpretar realmente la distancia entre puntos de fila o la distancia entre puntos de columna.\n\n\n\n\n\n\n\n\n\n\nCon la gráfica simétrica, la distancia entre filas y columnas no se puede interpretar. Solo se pueden hacer declaraciones generales sobre el patrón.\n\n\n\nfviz_ca_biplot(res.ca, repel = TRUE)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTenga en cuenta que, para interpretar la distancia entre los puntos de la columna y los puntos de la fila, la forma más sencilla es hacer una gráfica asimétrica. Esto significa que los perfiles de columna deben presentarse en el espacio de la fila o viceversa.\n\n\n\n\nBiplot asimétrico\nPara hacer un biplot asimétrico, los puntos de las filas (o columnas) se trazan a partir de las coordenadas estándar (S) y los perfiles de las columnas (o las filas) se trazan a partir de las coordenadas principales (P) (M. Bendixen 2003).\nPara un eje dado, las coordenadas estándar y principal se relacionan de la siguiente manera:\nP = sqrt(valor propio) X S\n\nP : la coordenada principal de una fila (o una columna) en el eje\nvalor propio : el valor propio del eje\n\nDependiendo de la situación, se pueden configurar otros tipos de visualización utilizando el mapa de argumentos (Nenadic y Greenacre 2007) en la función fviz_ca_biplot() [in factoextra].\nLas opciones permitidas para el mapa de argumentos son:\n\n“rowprincipal” o “colprincipal”: estos son los llamados biplots asimétricos, con filas en coordenadas principales y columnas en coordenadas estándar, o viceversa (también conocido como preservar métricas de filas o preservar métricas de columnas, respectivamente).\n\n“rowprincipal”: las columnas se representan en el espacio de las filas\n“colprincipal”: las filas se representan en el espacio de la columna\n\n“symbiplot”: tanto las filas como las columnas se escalan para tener varianzas iguales a los valores singulares (raíces cuadradas de los valores propios), lo que da un biplot simétrico pero no conserva las métricas de filas o columnas.\n“rowgab” o “colgab”: mapas asimétricos propuestos por Gabriel y Odoroff (Gabriel y Odoroff 1990):\n\n“rowgab”: filas en coordenadas principales y columnas en coordenadas estándar multiplicadas por la masa.\n“colgab”: columnas en coordenadas principales y filas en coordenadas estándar multiplicadas por la masa.\n\n“rowgreen” o “colgreen”: Los llamados biplots de contribución que muestran visualmente los puntos que más contribuyen (Greenacre 2006b).\n\n“rowgreen”: filas en coordenadas principales y columnas en coordenadas estándar multiplicadas por la raíz cuadrada de la masa.\n“colgreen”: columnas en coordenadas principales y filas en coordenadas estándar multiplicadas por la raíz cuadrada de la masa.\n\n\nEl siguiente código R dibuja un biplot asimétrico estándar:\n\nfviz_ca_biplot(res.ca,\n               map =\"rowprincipal\", arrow = c(TRUE, TRUE),\n               repel = TRUE)\n\n\n\n\nUsamos el argumento flechas, que es un vector de dos lógicas que especifican si la gráfica debe contener puntos (FALSE, predeterminado) o flechas (TRUE). El primer valor establece las filas y el segundo valor establece el columnas.\nSi el ángulo entre dos flechas es agudo, entonces existe una fuerte asociación entre la fila y la columna correspondientes.\nPara interpretar la distancia entre filas y una columna, debe proyectar perpendicularmente puntos de fila en la flecha de la columna.\n\n\nBiplot de contribución\nEn el biplot simétrico estándar (mencionado en la sección anterior), es difícil conocer los puntos que más contribuyen a la solución de la CA.\nMichael Greenacre propuso una nueva escala mostrada (llamada biplot de contribución) que incorpora la contribución de puntos (M. Greenacre 2013). En esta visualización, los puntos que contribuyen muy poco a la solución, están cerca del centro de la biplot y son relativamente poco importantes para la interpretación.\n\nSe puede dibujar un biplot de contribución usando el argumento map = \"rowgreen\" o map = \"colgreen\".\n\nEn primer lugar, hay que decidir si analizar las contribuciones de filas o columnas a la definición de los ejes.\nEn nuestro ejemplo interpretaremos la contribución de las filas a los ejes. Se utiliza el argumento map = \"colgreen\". En este caso, recuerde que las columnas están en coordenadas principales y las filas en coordenadas estándar multiplicadas por la raíz cuadrada de la masa. Para una fila dada, el cuadrado de la nueva coordenada en un eje i es exactamente la contribución de esta fila a la inercia del eje i.\n\nfviz_ca_biplot(res.ca, map =\"colgreen\", arrow = c(TRUE, FALSE),\n               repel = TRUE)\n\n\n\n\nEn el gráfico anterior, la posición de los puntos del perfil de la columna no cambia con respecto a la del biplot convencional. Sin embargo, las distancias de los puntos de fila desde el origen de la gráfica están relacionadas con sus contribuciones al mapa de factores bidimensionales.\nCuanto más cerca esté una flecha (en términos de distancia angular) de un eje, mayor será la contribución de la categoría de fila en ese eje en relación con el otro eje. Si la flecha está a medio camino entre los dos, su categoría de fila contribuye a los dos ejes en la misma medida.\n\n\nEs evidente que la categoría de fila Reparaciones (Repairs) tiene una contribución importante al polo positivo de la primera dimensión, mientras que las categorías Lavandería (Laundry) y Comida principal (Main_meal) tienen una contribución importante al polo negativo de la primera dimensión;\nLa dimensión 2 se define principalmente por la categoría de fila Vacaciones (Holidays).\nLa categoría de fila Conducción (Driving) contribuye a los dos ejes en la misma medida.\n\n\n\n\n\nDescripción de las dimensiones\nPara identificar fácilmente los puntos de fila y columna que están más asociados con las dimensiones principales, puede usar la función dimdesc() [en FactoMineR]. Las variables de fila/columna se ordenan por sus coordenadas en la salida dimdesc().\n\n# Descripción de la dimensión\nres.desc <- dimdesc(res.ca, axes = c(1,2))\n\nDescripción de la dimensión 1:\n\n# Descripción de la dimensión 1 por puntos de fila\nhead(res.desc[[1]]$row, 4)\n\n                coord\nLaundry    -0.9918368\nMain_meal  -0.8755855\nDinner     -0.6925740\nBreakfeast -0.5086002\n\n# Descripción de la dimensión 1 por puntos de columna\nhead(res.desc [[1]]$col, 4)\n\n                  coord\nWife        -0.83762154\nAlternating -0.06218462\nJointly      0.14942609\nHusband      1.16091847\n\n\nDescripción de la dimensión 2:\n\n# Descripción de la dimensión 2 por puntos de fila\nres.desc[[2]]$fila\n\nNULL\n\n# Descripción de la dimensión 1 por puntos de columna\nres.desc[[2]]$col\n\n                 coord\nJointly     -1.0265791\nAlternating  0.2915938\nWife         0.3652207\nHusband      0.6019199"
  },
  {
    "objectID": "posts/componentes_principales/index.html",
    "href": "posts/componentes_principales/index.html",
    "title": "Análisis de Componentes Principales",
    "section": "",
    "text": "El análisis de componentes principales (PCA) nos permite resumir y visualizar la información en un conjunto de datos que contiene individuos/observaciones descritos por múltiples variables cuantitativas inter-correlacionadas. Cada variable podría considerarse como una dimensión diferente. Si tiene más de 3 variables en sus conjuntos de datos, podría ser muy difícil visualizar un hiperespacio multidimensional.\nEl análisis de componentes principales se utiliza para extraer la información importante de una tabla de datos multivariados y para expresar esta información como un conjunto de algunas variables nuevas llamadas componentes principales. Estas nuevas variables corresponden a una combinación lineal de las originales. El número de componentes principales es menor o igual al número de variables originales.\nLa información de un conjunto de datos dado corresponde a la variación total que contiene. El objetivo de PCA es identificar direcciones (o componentes principales) a lo largo de los cuales la variación en los datos es máxima.\nEn otras palabras, PCA reduce la dimensionalidad de un dato multivariado a dos o tres componentes principales, que se pueden visualizar gráficamente, con una mínima pérdida de información.\n\nEn este capítulo, describimos la idea básica de PCA y demostramos cómo calcular y visualizar PCA usando el software R. Además, mostraremos cómo identificar las variables más importantes que explican las variaciones en un conjunto de datos.\n\nEn conjunto, el objetivo principal del análisis de componentes principales es:\n\n\nidentificar patrones ocultos en un conjunto de datos,\nreducir la dimensionalidad de los datos eliminando el ruido y la redundancia en los datos,\nidentificar variables correlacionadas"
  },
  {
    "objectID": "posts/componentes_principales/index.html#metodología-de-análisis",
    "href": "posts/componentes_principales/index.html#metodología-de-análisis",
    "title": "Análisis de Componentes Principales",
    "section": "Metodología de análisis",
    "text": "Metodología de análisis\nInstale los dos paquetes de la siguiente manera:\n\ninstall.packages(c(\"FactoMineR\", \"factoextra\"))\n\nCargar paquetes en R:\n\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\n\nCargando datos\nUsaremos los conjuntos de datos de demostración decathlon2 del paquete factoextra:\n\ndata(decathlon2)\n#head(decathlon2)\n\nDe esta base de datos seleccionamos variables cuantitativas para el análisis de componentes principales:\n\n#seleccionamos 27 filas y las primeras 10 variables\ndecathlon2.active <- decathlon2[1:23, 1:10]\nhead(decathlon2.active[, 1:7], 4)\n\n        X100m Long.jump Shot.put High.jump X400m X110m.hurdle Discus\nSEBRLE  11.04      7.58    14.83      2.07 49.81        14.69  43.75\nCLAY    10.76      7.40    14.26      1.86 49.37        14.05  50.72\nBERNARD 11.02      7.23    14.25      1.92 48.93        14.99  40.87\nYURKOV  11.34      7.09    15.19      2.10 50.42        15.31  46.26\n\n\n\n\nCódigo R\nSe puede utilizar la función PCA() [paquete FactoMineR]. Un formato simplificado es:\n\nPCA(X, scale.unit = TRUE, ncp = 5, graph = TRUE)\n\n\n\nX: un marco de datos. Las filas son individuos y las columnas son variables numéricas\nscale.unit: un valor lógico. Si es TRUE, los datos se escalan a la varianza de la unidad antes del análisis. Esta estandarización a la misma escala evita que algunas variables se conviertan en dominantes solo por sus grandes unidades de medida. Hace que la variable sea comparable.\nncp: número de dimensiones mantenidas en los resultados finales.\ngraph: un valor lógico. Si es TRUE, se muestra un gráfico.\n\n\nEl código R a continuación, calcula el análisis de componentes principales de las variables seleccionadas o activas:\n\nlibrary(\"FactoMineR\")\nres.pca <- PCA(decathlon2.active, graph = FALSE)\n\n\nEl objeto que se crea usando la función PCA() contiene mucha información que se encuentra en muchas listas y matrices diferentes. Estos valores se describen en la siguiente sección.\n\n\n\nExtraiga y visualice valores propios/varianzas:\nLos valores propios o varianzas miden la cantidad de variación retenida por cada componente principal. Las varianzas son grandes para los primeros componentes y pequeños para las siguientes. Es decir, las primeras PC corresponden a las direcciones con la cantidad máxima de variación en el conjunto de datos.\n\n# Extraer valores propios/varianzas\nget_eig(res.pca)\n\n       eigenvalue variance.percent cumulative.variance.percent\nDim.1   4.1242133        41.242133                    41.24213\nDim.2   1.8385309        18.385309                    59.62744\nDim.3   1.2391403        12.391403                    72.01885\nDim.4   0.8194402         8.194402                    80.21325\nDim.5   0.7015528         7.015528                    87.22878\nDim.6   0.4228828         4.228828                    91.45760\nDim.7   0.3025817         3.025817                    94.48342\nDim.8   0.2744700         2.744700                    97.22812\nDim.9   0.1552169         1.552169                    98.78029\nDim.10  0.1219710         1.219710                   100.00000\n\n# Visualizar valores propios/variaciones\nfviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))\n\n\n\n\nLa suma de todos los valores propios da una varianza total de 10.\nLa proporción de variación explicada por cada valor propio se da en la segunda columna. Por ejemplo, 4,124 dividido por 10 es igual a 0,4124, o aproximadamente el 41,24% de la variación se explica por este primer valor propio. El porcentaje acumulado explicado se obtiene sumando las sucesivas proporciones de variación explicadas para obtener el total acumulado. Por ejemplo, 41,242% más 18,385% es igual a 59,627%, y así sucesivamente. Por lo tanto, aproximadamente el 59,627% de la variación se explica por los dos primeros valores propios juntos.\nLos valores propios se pueden utilizar para determinar la cantidad de componentes principales que se deben retener después de la PCA (Kaiser 1961):\n\n\nUn valor propio > 1 indica que los PCs representan más varianza que la contabilizada por una de las variables originales en los datos estandarizados. Esto se usa comúnmente como un punto de corte para el cual se retienen las PCs. Esto es válido solo cuando los datos están estandarizados.\nTambién puede limitar el número de componentes a ese número que representa una cierta fracción de la varianza total. Por ejemplo, si está satisfecho con el 70% de la varianza total explicada, utilice el número de componentes para lograrlo.\n\n\nDesafortunadamente, no existe una forma objetiva bien aceptada de decidir cuántos componentes principales son suficientes. Esto dependerá del campo de aplicación específico y del conjunto de datos específico. En la práctica, tendemos a mirar los primeros componentes principales para encontrar patrones interesantes en los datos.\nEn nuestro análisis, los tres primeros componentes principales explican el 72% de la variación. Este es un porcentaje aceptablemente grande.\n\n\nGráfica círculo de correlación\nLa correlación entre una variable y un componente principal (PC) se utiliza como coordenadas de la variable en la PC. La representación de las variables difiere del gráfico de las observaciones: las observaciones están representadas por sus proyecciones, pero las variables están representadas por sus correlaciones (Abdi y Williams 2010).\nEs posible controlar los colores de las variables usando sus contribuciones (“contrib”) a los ejes principales:\n\n# Control variable colors using their contributions\nfviz_pca_var(res.pca, col.var=\"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE # Avoid text overlapping\n             )\n\n\n\n\nLa gráfica anterior también se conoce como gráficas de correlación variable. Muestra las relaciones entre todas las variables. Se puede interpretar de la siguiente manera:\n\nLas variables correlacionadas positivamente se agrupan.\nLas variables correlacionadas negativamente se colocan en lados opuestos del origen de la gráfica (cuadrantes opuestos).\nLa distancia entre las variables y el origen mide la calidad de las variables en el mapa de factores. Las variables que están lejos del origen están bien representadas en el mapa de factores.\n\n\n\nContribucion de las variables a los ejes principales:\n\n# Contributions of variables to PC1\nfviz_contrib(res.pca, choice = \"var\", axes = 1, top = 10)\n\n\n\n# Contributions of variables to PC2\nfviz_contrib(res.pca, choice = \"var\", axes = 2, top = 10)\n\n\n\n\nLa línea discontinua roja en el gráfico anterior indica la contribución promedio esperada. Si la contribución de las variables fuera uniforme, el valor esperado sería 1/longitud(variables) = 1/10 = 10%. Para un componente dado, una variable con una contribución mayor que este límite podría considerarse importante para contribuir al componente.\n\nSe puede observar que las variables X100m, Long.jump y Pole.vault contribuyen más a las dimensiones 1 y 2.\n\n\n\nBiplot\nPara hacer un biplot simple de individuos y variables, escriba esto:\n\n# Biplot of individuals and variables\nfviz_pca_biplot(res.pca, repel = TRUE)\n\n\n\n\nEn biplot, debe centrarse principalmente en la dirección de las variables pero no en sus posiciones absolutas en la gráfica.\nEn términos generales, un biplot se puede interpretar de la siguiente manera:\n\nun individuo que está en el mismo lado de una variable dada tiene un valor alto para esta variable;\nun individuo que está en el lado opuesto de una variable dada tiene un valor bajo para esta variable.\n\nNota: Este post fue extraido del libro “Multivariate Analisis II”"
  },
  {
    "objectID": "posts/componentes_principales/index.html#referencias",
    "href": "posts/componentes_principales/index.html#referencias",
    "title": "Análisis de Componentes Principales",
    "section": "Referencias",
    "text": "Referencias\nKassambara A. 2017. Multivariate Analysis: Practical Guide to Principal Component Methods in R."
  },
  {
    "objectID": "posts/agridata/index.html",
    "href": "posts/agridata/index.html",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "",
    "text": "El análisis de datos es una etapa esencial en el proceso de investigación y consiste en examinar y analizar los datos recopilados de manera detallada para extraer información y obtener conclusiones relevantes en función a los objetivos de la investigación. Para realizar el análisis de datos, se pueden utilizar diferentes herramientas y técnicas, generalmente se construyen tablas y figuras. Estos resultados son usados para tomar decisiones y formular conclusiones sobre los datos, y también pueden ser utilizados para mejorar y optimizar procesos y sistemas. El análisis de datos es importante ya que permite a los investigadores obtener una visión más profunda y detallada de los datos y tomar decisiones más informadas y precisas.\nLa etapa de análisis de datos puede resultar un reto difícil para un recien egresado o tesista. Esto debido a la falta de experiencia o conocimiento en el uso de herramientas y técnicas de análisis de datos. Debido a ello, queremos facilitar algunas herramientas de análisis, al mismo tiempo, se pretende que los interesados puedan entrenarse en el uso de R. Ya que esta herramienta estadística ofrece un sin fin de paquetes que nos ayudan a realizar los análisis de manera sencilla pero de mucha categoría científica."
  },
  {
    "objectID": "posts/agridata/index.html#librerias",
    "href": "posts/agridata/index.html#librerias",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "Librerias",
    "text": "Librerias"
  },
  {
    "objectID": "posts/agridata/index.html#base-de-datos",
    "href": "posts/agridata/index.html#base-de-datos",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "Base de datos",
    "text": "Base de datos\nPara esta oportunidad daremos uso de la base de datos de iris. Estos datos estarán disponibles al momento de instalar R."
  },
  {
    "objectID": "posts/agridata/index.html#análisis-descriptivo",
    "href": "posts/agridata/index.html#análisis-descriptivo",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "Análisis descriptivo",
    "text": "Análisis descriptivo\nGeneralmente se puede presentar en una tabla los resultados descriptivos.\n\n\n\n\nAnálisis descriptivo\n \n  \n    variable \n    mean \n    se \n    kurt \n    skew \n    min \n    max \n  \n \n\n  \n    Petal.Length \n    3.76 \n    0.14 \n    -1.40 \n    -0.27 \n    1.0 \n    6.9 \n  \n  \n    Petal.Width \n    1.20 \n    0.06 \n    -1.34 \n    -0.10 \n    0.1 \n    2.5 \n  \n  \n    Sepal.Length \n    5.84 \n    0.07 \n    -0.55 \n    0.31 \n    4.3 \n    7.9 \n  \n  \n    Sepal.Width \n    3.06 \n    0.04 \n    0.23 \n    0.32 \n    2.0 \n    4.4 \n  \n\n\n\n\n\nLa otra opción podría ser una figura boxplot"
  },
  {
    "objectID": "posts/agridata/index.html#análisis-de-varianza",
    "href": "posts/agridata/index.html#análisis-de-varianza",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "Análisis de varianza",
    "text": "Análisis de varianza\n\n\n\nGeneralmente en la mayoría de las tesis se presenta cada ANOVA para cada variable, la cual no es muy redundante en todo el documento. Además, en las revistas científicas de impacto, este tipo de tablas no es permitido si se tiene varias variables.\n\n\nAnalysis of Variance Table\n\nResponse: Sepal.Width\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nSpecies     2 11.345  5.6725   49.16 < 2.2e-16 ***\nResiduals 147 16.962  0.1154                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSin embargo, se podría resumir de esta manera. En la tabla solamente se presenta aquellas variables con significancia estadística.\n\n\n\n\nAnálisis de varianza\n \n  \n    FV \n    DF \n    Sepal.Length \n    Sepal.Width \n    Petal.Length \n    Petal.Width \n  \n \n\n  \n    Species \n    2 \n    31.61 ** \n    5.67 ** \n    218.55 ** \n    40.21 ** \n  \n  \n    Residuals \n    147 \n    0.27 \n    0.12 \n    0.19 \n    0.04 \n  \n\n\n\n\n\nUna vez identificada las diferencias estadisticas se procede a realizar pruebas de promedio para seleccionar genotipos sobresalientes.\n\n\n\nHay otra manera de representar comparaciones graficamente\n\n\n\n\n\n\n\n\nTambien existe otra opción con el paquete AgroR\n\n\n\n\n\n\n-----------------------------------------------------------------\nNormality of errors\n-----------------------------------------------------------------\n                         Method Statistic   p.value\n Shapiro-Wilk normality test(W) 0.9878974 0.2188639\n\n\n-----------------------------------------------------------------\nHomogeneity of Variances\n-----------------------------------------------------------------\n                              Method Statistic      p.value\n Bartlett test(Bartlett's K-squared)   16.0057 0.0003345076\n\n\n-----------------------------------------------------------------\nIndependence from errors\n-----------------------------------------------------------------\n                 Method Statistic   p.value\n Durbin-Watson test(DW)  2.043002 0.5401261\n\n\n-----------------------------------------------------------------\nAdditional Information\n-----------------------------------------------------------------\n\nCV (%) =  8.81\nMStrat/MST =  0.99\nMean =  5.8433\nMedian =  5.8\nPossible outliers =  107\n\n-----------------------------------------------------------------\nAnalysis of Variance\n-----------------------------------------------------------------\n           Df   Sum Sq    Mean.Sq  F value        Pr(F)\ntrat        2 63.21213 31.6060667 119.2645 1.669669e-31\nResiduals 147 38.95620  0.2650082                      \n\n\n\n\n-----------------------------------------------------------------\nMultiple Comparison Test: Tukey HSD\n-----------------------------------------------------------------\n            resp groups\nvirginica  6.588      a\nversicolor 5.936      b\nsetosa     5.006      c"
  },
  {
    "objectID": "posts/agridata/index.html#análisis-multivariado",
    "href": "posts/agridata/index.html#análisis-multivariado",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "Análisis multivariado",
    "text": "Análisis multivariado\n\n\n\n\n\n\n\n\n\n-------------------------------------------------------------------------------\nPrincipal Component Analysis\n-------------------------------------------------------------------------------\n# A tibble: 22 × 4\n   PC    Eigenvalues `Variance (%)` `Cum. variance (%)`\n   <chr>       <dbl>          <dbl>               <dbl>\n 1 PC1         14.2            64.4                64.4\n 2 PC2          5.24           23.8                88.2\n 3 PC3          2.58           11.8               100  \n 4 PC4          0               0                 100  \n 5 PC5          0               0                 100  \n 6 PC6          0               0                 100  \n 7 PC7          0               0                 100  \n 8 PC8          0               0                 100  \n 9 PC9          0               0                 100  \n10 PC10         0               0                 100  \n# … with 12 more rows\n-------------------------------------------------------------------------------\nFactor Analysis - factorial loadings after rotation-\n-------------------------------------------------------------------------------\n# A tibble: 22 × 6\n   VAR     FA1   FA2   FA3 Communality Uniquenesses\n   <chr> <dbl> <dbl> <dbl>       <dbl>        <dbl>\n 1 NNCF  -0.72 -0.67 -0.17           1            0\n 2 WNCF  -0.65 -0.75 -0.14           1            0\n 3 AWNCF  0.63  0.49  0.6            1            0\n 4 WUE    1     0.04  0.03           1            0\n 5 NDBF  -0.11  0.07 -0.99           1            0\n 6 NDFF  -0.16 -0.24 -0.96           1            0\n 7 NDBH  -0.17  0.07 -0.98           1            0\n 8 PHYL   0.6   0.79 -0.12           1            0\n 9 TA     0.84  0.1   0.54           1            0\n10 NCF   -0.84 -0.42 -0.35           1            0\n# … with 12 more rows\n-------------------------------------------------------------------------------\nComunalit Mean: 1 \n-------------------------------------------------------------------------------\nSelection differential \n-------------------------------------------------------------------------------\n# A tibble: 22 × 8\n   VAR    Factor       Xo       Xs        SD  SDperc sense     goal\n   <chr>  <chr>     <dbl>    <dbl>     <dbl>   <dbl> <chr>    <dbl>\n 1 NNCF   FA1        7.20     7.86    0.650    9.02  increase   100\n 2 AWNCF  FA1        8.74     8.70   -0.0425  -0.486 increase     0\n 3 WUE    FA1      116.     101.    -15.0    -12.9   increase     0\n 4 TA     FA1        1.42     1.35   -0.0700  -4.93  increase     0\n 5 NCF    FA1       23.5     25.0     1.52     6.48  increase   100\n 6 TNF    FA1       30.7     32.8     2.16     7.04  increase   100\n 7 WCF    FA1      359.     395.     36.9     10.3   increase   100\n 8 TWF    FA1      413.     442.     29.1      7.05  increase   100\n 9 FY     FA1    30957.   33137.   2180.       7.04  increase   100\n10 TSS_TA FA1        5.39     5.66    0.268    4.96  increase   100\n# … with 12 more rows\n------------------------------------------------------------------------------\nSelected genotypes\n-------------------------------------------------------------------------------\nNAC_CAM IMP_ALB\n-------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\nJoining, by = \"VAR\""
  },
  {
    "objectID": "posts/tidyexam/index.html",
    "href": "posts/tidyexam/index.html",
    "title": "Ordenar datos con el paquete Tidyverse",
    "section": "",
    "text": "La ordenación de datos es una de las tareas mas importantes despues de concluir la investigación. En las ciencias agrícolas, generalmente la investigación concluye con la evaluación de la cosecha del cultivo. Generalmente nuestros datos pueden estar organizados en un libro de campo; sin embargo, en otras áreas no es así.\nEn este blog replicaré un ejemplo de ordenación de datos con el paquete Tidyverse del libro R4DS. El dataset datos::oms contiene datos de tuberculosis (TB) detallados por año, país, edad, sexo y método de diagnóstico. Los datos provienen del Informe de Tuberculosis de la Organización Mundial de la Salud 2014, disponible en http://www.who.int/tb/country/data/download/en/."
  },
  {
    "objectID": "posts/tidyexam/index.html#procedimiento-de-ordenación",
    "href": "posts/tidyexam/index.html#procedimiento-de-ordenación",
    "title": "Ordenar datos con el paquete Tidyverse",
    "section": "Procedimiento de ordenación",
    "text": "Procedimiento de ordenación\n\nCargar el paquete tidyverse\nEl primer paso es instalar el paquete tidyverse del CRAN de R. Posterior a esto es cargar el paquete en nuestra consola de R.\n\nlibrary(tidyverse)\n#En el paquete datos se encuentra la base de datos para este ejemplo\nlibrary(datos)\n\nA continuación observación el estado de los datos de oms.\n\nhead(oms)\n\n# A tibble: 6 × 60\n  pais       iso2  iso3   anio nuevos_…¹ nuevo…² nuevo…³ nuevo…⁴ nuevo…⁵ nuevo…⁶\n  <chr>      <chr> <chr> <int>     <int>   <int>   <int>   <int>   <int>   <int>\n1 Afganistán AF    AFG    1980        NA      NA      NA      NA      NA      NA\n2 Afganistán AF    AFG    1981        NA      NA      NA      NA      NA      NA\n3 Afganistán AF    AFG    1982        NA      NA      NA      NA      NA      NA\n4 Afganistán AF    AFG    1983        NA      NA      NA      NA      NA      NA\n5 Afganistán AF    AFG    1984        NA      NA      NA      NA      NA      NA\n6 Afganistán AF    AFG    1985        NA      NA      NA      NA      NA      NA\n# … with 50 more variables: nuevos_fpp_h65 <int>, nuevos_fpp_m014 <int>,\n#   nuevos_fpp_m1524 <int>, nuevos_fpp_m2534 <int>, nuevos_fpp_m3544 <int>,\n#   nuevos_fpp_m4554 <int>, nuevos_fpp_m5564 <int>, nuevos_fpp_m65 <int>,\n#   nuevos_fpn_h014 <int>, nuevos_fpn_h1524 <int>, nuevos_fpn_h2534 <int>,\n#   nuevos_fpn_h3544 <int>, nuevos_fpn_h4554 <int>, nuevos_fpn_h5564 <int>,\n#   nuevos_fpn_h65 <int>, nuevos_fpn_m014 <int>, nuevos_fpn_m1524 <int>,\n#   nuevos_fpn_m2534 <int>, nuevos_fpn_m3544 <int>, nuevos_fpn_m4554 <int>, …\n\n\nEn la salida se observa un ejemplo muy típico de una base de datos de la vida real. Contiene columnas redundantes, códigos extraños de variables y muchos valores faltantes. Practicamente, la base de datos oms está desordenado, por tanto, se necesita ordenarlo de manera sencilla con tidyverse.\n\n\nPasos de ordenación\nNecesitamos agrupar todas las columnas desde nuevos_fpp_h014 hasta recaidas_m65. No sabemos aún qué representa esto, por lo que le daremos el nombre genérico de \"clave\". Sabemos que las celdas representan la cuenta de casos, por lo que usaremos la variable casos.\nExisten múltiples valores faltantes en la representación actual, por lo que de momento usaremos na.rm para centrarnos en los valores que están presentes.\n\noms1 <- oms %>%\n  pivot_longer(\n    cols = nuevos_fpp_h014:nuevosrecaida_m65, \n    names_to = \"clave\", \n    values_to = \"casos\", \n    values_drop_na = TRUE\n  )\noms1\n\n# A tibble: 76,046 × 6\n   pais       iso2  iso3   anio clave            casos\n   <chr>      <chr> <chr> <int> <chr>            <int>\n 1 Afganistán AF    AFG    1997 nuevos_fpp_h014      0\n 2 Afganistán AF    AFG    1997 nuevos_fpp_h1524    10\n 3 Afganistán AF    AFG    1997 nuevos_fpp_h2534     6\n 4 Afganistán AF    AFG    1997 nuevos_fpp_h3544     3\n 5 Afganistán AF    AFG    1997 nuevos_fpp_h4554     5\n 6 Afganistán AF    AFG    1997 nuevos_fpp_h5564     2\n 7 Afganistán AF    AFG    1997 nuevos_fpp_h65       0\n 8 Afganistán AF    AFG    1997 nuevos_fpp_m014      5\n 9 Afganistán AF    AFG    1997 nuevos_fpp_m1524    38\n10 Afganistán AF    AFG    1997 nuevos_fpp_m2534    36\n# … with 76,036 more rows\n\n\nPara visualizar el conteo de valores en la nueva columna clave:\n\noms1 %>%\n  count(clave)\n\n# A tibble: 56 × 2\n   clave               n\n   <chr>           <int>\n 1 nuevos_ep_h014   1038\n 2 nuevos_ep_h1524  1026\n 3 nuevos_ep_h2534  1020\n 4 nuevos_ep_h3544  1024\n 5 nuevos_ep_h4554  1020\n 6 nuevos_ep_h5564  1015\n 7 nuevos_ep_h65    1018\n 8 nuevos_ep_m014   1032\n 9 nuevos_ep_m1524  1021\n10 nuevos_ep_m2534  1021\n# … with 46 more rows\n\n\nPara entender el significado de cada variable, se dispone de un diccionario de datos a mano. Este dice lo siguiente:\n\nLo que aparece antes del primer _ en las columnas indica si la columna contiene casos nuevos o antiguos de tuberculosis. En este dataset, cada columna contiene nuevos casos.\nLo que aparece luego de indicar si se refiere casos nuevos o antiguos es el tipo de tuberculosis:\n\n\nrecaida se refiere a casos reincidentes\nep se refiere a tuberculosis extra pulmonar\nfpn se refiere a casos de tuberculosis pulmonar que no se pueden detectar mediante examen de frotis pulmonar (frotis pulmonar negativo)\nfpp se refiere a casos de tuberculosis pulmonar que se pueden detectar mediante examen de frotis pulmonar (frotis pulmonar positivo)\n\n\nLa letra que aparece después del último _ se refiere al sexo de los pacientes. El conjunto de datos agrupa en hombres (h) y mujeres (m).\nLos números finales se refieren al grupo etareo que se ha organizado en siete categorías:\n\n\n014 = 0 - 14 años de edad\n1524 = 15 – 24 años de edad\n2534 = 25 – 34 años de edad\n3544 = 35 – 44 años de edad\n4554 = 45 – 54 años de edad\n5564 = 55 – 64 años de edad\n65 = 65 o más años de edad\n\nNecesitamos hacer un pequeño cambio al formato de los nombres de las columnas: desafortunadamente lo nombres de las columnas son ligeramente inconsistentes debido a que en lugar de nuevos_recaida tenemos nuevosrecaida (es difícil darse cuenta de esto en esta parte, pero si no lo arreglas habrá errores en los pasos siguientes). Para esto, la idea básica es bastante simple: reemplazar los caracteres “nuevosrecaida” por “nuevos_recaida”. Esto genera nombres de variables consistentes.\n\noms2 <- oms1 %>%\n  mutate(clave = stringr::str_replace(clave, \"nuevosrecaida\", \"nuevos_recaida\"))\noms2\n\n# A tibble: 76,046 × 6\n   pais       iso2  iso3   anio clave            casos\n   <chr>      <chr> <chr> <int> <chr>            <int>\n 1 Afganistán AF    AFG    1997 nuevos_fpp_h014      0\n 2 Afganistán AF    AFG    1997 nuevos_fpp_h1524    10\n 3 Afganistán AF    AFG    1997 nuevos_fpp_h2534     6\n 4 Afganistán AF    AFG    1997 nuevos_fpp_h3544     3\n 5 Afganistán AF    AFG    1997 nuevos_fpp_h4554     5\n 6 Afganistán AF    AFG    1997 nuevos_fpp_h5564     2\n 7 Afganistán AF    AFG    1997 nuevos_fpp_h65       0\n 8 Afganistán AF    AFG    1997 nuevos_fpp_m014      5\n 9 Afganistán AF    AFG    1997 nuevos_fpp_m1524    38\n10 Afganistán AF    AFG    1997 nuevos_fpp_m2534    36\n# … with 76,036 more rows\n\n\nUna vez reemplazado, nos facilita separar los valores en cada código aplicando separate() dos veces. La primera aplicación dividirá los códigos en cada _.\n\noms3 <- oms2 %>%\n  separate(clave, c(\"nuevos\", \"tipo\", \"sexo_edad\"), sep = \"_\")\noms3\n\n# A tibble: 76,046 × 8\n   pais       iso2  iso3   anio nuevos tipo  sexo_edad casos\n   <chr>      <chr> <chr> <int> <chr>  <chr> <chr>     <int>\n 1 Afganistán AF    AFG    1997 nuevos fpp   h014          0\n 2 Afganistán AF    AFG    1997 nuevos fpp   h1524        10\n 3 Afganistán AF    AFG    1997 nuevos fpp   h2534         6\n 4 Afganistán AF    AFG    1997 nuevos fpp   h3544         3\n 5 Afganistán AF    AFG    1997 nuevos fpp   h4554         5\n 6 Afganistán AF    AFG    1997 nuevos fpp   h5564         2\n 7 Afganistán AF    AFG    1997 nuevos fpp   h65           0\n 8 Afganistán AF    AFG    1997 nuevos fpp   m014          5\n 9 Afganistán AF    AFG    1997 nuevos fpp   m1524        38\n10 Afganistán AF    AFG    1997 nuevos fpp   m2534        36\n# … with 76,036 more rows\n\n\nA continuación podemos eliminar la columna nuevos, ya que es constante en este dataset. Además eliminaremos iso2 e iso3 ya que son redundantes.\n\noms3 %>%\n  count(nuevos)\n\n# A tibble: 1 × 2\n  nuevos     n\n  <chr>  <int>\n1 nuevos 76046\n\noms4 <- oms3 %>%\n  select(-nuevos, -iso2, -iso3)\n\nLuego separamos sexo_edad en sexo y edad dividiendo luego del primer carácter:\n\noms5 <- oms4 %>%\n  separate(sexo_edad, c(\"sexo\", \"edad\"), sep = 1)\noms5\n\n# A tibble: 76,046 × 6\n   pais        anio tipo  sexo  edad  casos\n   <chr>      <int> <chr> <chr> <chr> <int>\n 1 Afganistán  1997 fpp   h     014       0\n 2 Afganistán  1997 fpp   h     1524     10\n 3 Afganistán  1997 fpp   h     2534      6\n 4 Afganistán  1997 fpp   h     3544      3\n 5 Afganistán  1997 fpp   h     4554      5\n 6 Afganistán  1997 fpp   h     5564      2\n 7 Afganistán  1997 fpp   h     65        0\n 8 Afganistán  1997 fpp   m     014       5\n 9 Afganistán  1997 fpp   m     1524     38\n10 Afganistán  1997 fpp   m     2534     36\n# … with 76,036 more rows\n\n\n¡Ahora la base de datos oms está ordenado!"
  },
  {
    "objectID": "posts/tidyexam/index.html#resumen",
    "href": "posts/tidyexam/index.html#resumen",
    "title": "Ordenar datos con el paquete Tidyverse",
    "section": "Resumen",
    "text": "Resumen\nEn la anterior sección se hizo el procedimiento de ordenación paso a paso, asignando los resultados intermedios a nuevas variables. Esta no es la forma típica de trabajo. En realidad, los códigos debería ser de manera incremental usando pipes (\"%>%):\n\nfsdata<- oms %>%\n  pivot_longer(\n    cols = nuevos_fpp_h014:nuevosrecaida_m65,\n    names_to = \"clave\", \n    values_to = \"valor\", \n    values_drop_na = TRUE) %>%\n  mutate(clave = stringr::str_replace(clave, \"nuevosrecaida\", \"nuevos_recaida\")) %>%\n  separate(clave, c(\"nuevos\", \"tipo\", \"sexo_edad\")) %>%\n  select(-nuevos, -iso2, -iso3) %>%\n  separate(sexo_edad, c(\"sexo\", \"edad\"), sep = 1)\nfsdata\n\n# A tibble: 76,046 × 6\n   pais        anio tipo  sexo  edad  valor\n   <chr>      <int> <chr> <chr> <chr> <int>\n 1 Afganistán  1997 fpp   h     014       0\n 2 Afganistán  1997 fpp   h     1524     10\n 3 Afganistán  1997 fpp   h     2534      6\n 4 Afganistán  1997 fpp   h     3544      3\n 5 Afganistán  1997 fpp   h     4554      5\n 6 Afganistán  1997 fpp   h     5564      2\n 7 Afganistán  1997 fpp   h     65        0\n 8 Afganistán  1997 fpp   m     014       5\n 9 Afganistán  1997 fpp   m     1524     38\n10 Afganistán  1997 fpp   m     2534     36\n# … with 76,036 more rows"
  },
  {
    "objectID": "posts/tidyexam/index.html#conclusión",
    "href": "posts/tidyexam/index.html#conclusión",
    "title": "Ordenar datos con el paquete Tidyverse",
    "section": "Conclusión",
    "text": "Conclusión\nEs un ejemplo muy bueno para practicar y usar las diferentes funciones de tidyverse en la ordenación de datos."
  },
  {
    "objectID": "posts/anova/index.html",
    "href": "posts/anova/index.html",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "",
    "text": "El análisis de varianza es una prueba estadística para determinar si dos o más medias poblacionales son diferentes entre si. En otras palabras, se usa para comparar dos o más grupos para ver si son significativamente diferentes.\nEn el resto del post lo comentaremos desde un punto de vista más práctico y en particular abordaremos los siguientes puntos:\n\nel objetivo del análisis de varianza y cuándo debe usarse\ncómo realizar el ANVA en R\ncómo interpretar los resultados del ANVA\ncomprender la noción de prueba de promedios e interpretar los resultados\ncómo visualizar los resultados de ANVA y pruebas de promedio"
  },
  {
    "objectID": "posts/anova/index.html#datos",
    "href": "posts/anova/index.html#datos",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Datos",
    "text": "Datos\nEl dato que se utilizará es iris, que se encuentra en la base de datos de R. Estos datos como tratamientos tienen tres especies (setosa, versicolor y virginica) y cuatro variables (Sepal.Length, Sepal.Width, Petal.Length y Petal.Width) cuantitativas\n\n#paquetes R a utilizar\nlibrary(tidyverse)\nlibrary(easyanova)\nlibrary(car)\nlibrary(lattice)\nlibrary(multcomp)\nlibrary(ggpubr)\nlibrary(rstatix)\n\nLa librería de easyanova es un paquete para realizar análisis de experimentos agrícolas y animales. Las funciones de esta librería son fáciles de usar. Realiza análisis en varios diseños, con datos balanceados y no balanceados.\nSalida de datos a utilizar:\n\n#datos\ntibble(iris)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 140 more rows\n\n\n\n#inspección de datos\np <- ggplot(iris) +\n  aes(x = Species, y = Sepal.Width, color = Species) +\n  geom_jitter() +\n  theme(legend.position = \"none\")\n\nlibrary(plotly)\nfig <- ggplotly(p)\n\nfig"
  },
  {
    "objectID": "posts/anova/index.html#objetivo-del-anva",
    "href": "posts/anova/index.html#objetivo-del-anva",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Objetivo del ANVA",
    "text": "Objetivo del ANVA\nComo se mencionó en la introducción, el ANVA se usa para comparar grupos (en la práctica, 3 o más grupos). De manera más general, se utiliza para:\n\nestudiar si las mediciones son similares en diferentes modalidades (también llamadas niveles o tratamientos en el contexto de ANVA) de una variable categórica\ncomparar el impacto de los diferentes niveles de una variable categórica sobre una variable cuantitativa\nexplicar una variable cuantitativa basada en una variable cualitativa"
  },
  {
    "objectID": "posts/anova/index.html#supuestos-subyacentes-de-anva",
    "href": "posts/anova/index.html#supuestos-subyacentes-de-anva",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Supuestos subyacentes de ANVA",
    "text": "Supuestos subyacentes de ANVA\nComo ocurre con muchas pruebas estadísticas, hay algunas suposiciones que deben cumplirse para poder interpretar los resultados. Cuando no se cumplen uno o varios supuestos, aunque técnicamente es posible realizar estas pruebas, sería incorrecto interpretar los resultados y confiar en las conclusiones.\n\n1. Tipo de variable\nLas variables dependientes Sepal.Length, Sepal.Width, Petal.Length y Petal.Width es una variable cuantitativa y la variable independiente Species es cualitativa (con 3 niveles correspondientes a las 3 especies). Así que tenemos una combinación de los dos tipos de variables y se cumple este supuesto.\n\n\n2. Independencia\nSe asume la independencia de las observaciones ya que los datos se han recopilado de una parte de la población seleccionada al azar y las mediciones dentro y entre las 3 muestras no están relacionadas.\nEl supuesto de independencia se verifica con mayor frecuencia con base en el diseño del experimento y en el buen control de las condiciones experimentales, como es el caso aquí. Sin embargo, si realmente desea probarlo de manera más formal, puede probarlo mediante una prueba estadística: la prueba de Durbin-Watson (en R: durbinWatsonTest(res_lm) donde res_lm es un modelo lineal). La hipótesis nula de esta prueba especifica un coeficiente de autocorrelación = 0, mientras que la hipótesis alternativa especifica un coeficiente de autocorrelación ≠ 0.\n\n\n3. Normalidad\nRecuerde que la normalidad de los residuos se puede probar visualmente mediante un histograma y un gráfico QQ, y/o formalmente mediante una prueba de normalidad (prueba de Shapiro-Wilk, por ejemplo).\nAntes de verificar el supuesto de normalidad, primero debemos calcular el ANVA. Luego guardamos los resultados en res_aov:\n\nres_aov <- aov(Sepal.Width ~ Species,\n  data = iris\n)\n\nAhora podemos comprobar la normalidad visualmente:\n\npar(mfrow = c(1, 2)) # combine plots\n\n# histogram\nhist(res_aov$residuals)\n\n# QQ-plot\nqqPlot(res_aov$residuals,\n  id = FALSE # id = FALSE to remove point identification\n)\n\n\n\n\nA partir del histograma y el gráfico QQ anteriores, ya podemos ver que el supuesto de normalidad parece cumplirse. De hecho, el histograma forma aproximadamente una curva de campana, lo que indica que los residuos siguen una distribución normal. Además, los puntos en las gráficas QQ siguen aproximadamente la línea recta y la mayoría de ellos están dentro de las bandas de confianza, lo que también indica que los residuos siguen aproximadamente una distribución normal.\nAlgunos investigadores se detienen aquí y asumen que se cumple la normalidad, mientras que otros también prueban la suposición a través de una prueba estadística formal. Es su elección probarlo (i) solo visualmente, (ii) solo a través de una prueba de normalidad, o (iii) tanto visualmente como a través de una prueba de normalidad. Sin embargo, tenga en cuenta los dos puntos siguientes:\n\n\nANVA es bastante robusto a pequeñas desviaciones de la normalidad. Esto significa que no es un problema (desde la perspectiva de la interpretación de los resultados de ANVA) si un pequeño número de puntos se desvía ligeramente de la normalidad,\nLas pruebas de normalidad son a veces bastante conservadoras, lo que significa que la hipótesis nula de normalidad puede rechazarse debido a una desviación limitada de la normalidad. Este es especialmente el caso con muestras grandes, ya que la potencia de la prueba aumenta con el tamaño de la muestra.\n\n\nEn la práctica, se tiende a preferir el (i) enfoque visual solamente, pero nuevamente, esto es una cuestión de elección personal y también depende del contexto del análisis. Tambien, puede utilizar la prueba de Shapiro-Wilk o la prueba de Kolmogorov-Smirnov, entre otras.\n\n\n4. Igualdad de varianzas - homogeneidad\nSuponiendo que los residuos siguen una distribución normal, ahora es el momento de comprobar si las varianzas son iguales entre especies o no. El resultado tendrá un impacto en si usamos el ANVA o la prueba de Welch.\nEsto se puede verificar nuevamente visualmente, a través de una gráfica de caja o gráfica de puntos, o más formalmente a través de una prueba estadística (la prueba de Levene, entre otras).\nVisualmente tenemos:\n\n# Boxplot\nboxplot(Sepal.Width ~ Species,\n  data = iris\n)\n\n\n\n\n\n# Dotplot\n\ndotplot(Sepal.Width ~ Species,\n  data = iris\n)\n\n\n\n\nTanto la gráfica de boxplot como la gráfica de puntos muestran una variación similar para las diferentes especies. En el boxplot, esto se puede ver por el hecho de que las cajas y los bigotes tienen un tamaño comparable para todas las especies. Hay un par de valores atípicos como lo muestran los puntos fuera de los bigotes, pero esto no cambia el hecho de que la dispersión es más o menos la misma entre las diferentes especies.\nEn la gráfica de puntos, esto se puede ver por el hecho de que los puntos para las 3 especies tienen más o menos el mismo rango, un signo de la dispersión y, por lo tanto, la varianza es similar.\nAl igual que el supuesto de normalidad, si cree que el enfoque visual no es suficiente, puede probar formalmente la igualdad de las varianzas con una prueba de Levene o de Bartlett. Observe que la prueba de Levene es menos sensible a las desviaciones de la distribución normal que la prueba de Bartlett.\nLas hipótesis nula y alternativa para ambas pruebas son:\n\nH0: las variaciones son iguales\nH1: al menos una varianza es diferente\n\nEn R, la prueba de Levene se puede realizar gracias a la función leveneTest() del paquete {car}:\n\n# Levene's test\n\nleveneTest(Sepal.Width ~ Species,\n  data = iris\n)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   2  0.5902 0.5555\n      147               \n\n\nSiendo el p-valor mayor que el nivel de significancia de 0.05, no rechazamos la hipótesis nula, por lo que no podemos rechazar la hipótesis de que las varianzas son iguales entre especies (p-valor = 0.556).\nEste resultado también está en línea con el enfoque visual, por lo que la homogeneidad de las variaciones se cumple tanto visual como formalmente."
  },
  {
    "objectID": "posts/anova/index.html#análisis-de-varianza-en-r",
    "href": "posts/anova/index.html#análisis-de-varianza-en-r",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Análisis de varianza en R",
    "text": "Análisis de varianza en R\nEl ANVA puede ayudarnos a hacer inferencias sobre la población dada la muestra en cuestión y ayudarnos a responder la pregunta de investigación “¿Existe diferencia en ancho de sépalo para las 3 especies?”.\nEl ANVA en R se puede realizar de varias formas, de las cuales tres se presentan a continuación:\na). Con la función oneway.test():\n\n# primer metodo:\noneway.test(Sepal.Width ~ Species,\n  data = iris,\n  var.equal = TRUE # asumiendo varianzas iguales\n)\n\n\n    One-way analysis of means\n\ndata:  Sepal.Width and Species\nF = 49.16, num df = 2, denom df = 147, p-value < 2.2e-16\n\n\nb). Con las funciones de summary() y aov():\n\n# 2nd method:\nres_aov <- aov(Sepal.Width ~ Species,\n  data = iris\n)\n\nsummary(res_aov)\n\n             Df Sum Sq Mean Sq F value Pr(>F)    \nSpecies       2  11.35   5.672   49.16 <2e-16 ***\nResiduals   147  16.96   0.115                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nComo puede ver en los dos resultados anteriores, la estadística de prueba (F = en el primer método y el valor F en el segundo) y el p-valor (p-valor en el primer método y Pr (> F) en el segundo) son exactamente iguales para ambos métodos, lo que significa que en caso de variaciones iguales, los resultados y las conclusiones no cambiarán.\nLa ventaja del primer método es que es fácil cambiar del ANVA (utilizado cuando las variaciones son iguales) a la prueba de Welch (utilizado cuando las variaciones son desiguales). Esto se puede hacer reemplazando nvar.equal = TRUE por var.equal = FALSE, como se presenta a continuación:\n\noneway.test(Sepal.Width ~ Species,\n  data = iris,\n  var.equal = FALSE # asumiendo variaciones desiguales\n)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  Sepal.Width and Species\nF = 45.012, num df = 2.000, denom df = 97.402, p-value = 1.433e-14\n\n\nSin embargo, la ventaja del segundo método es que:\n\nSe imprime la tabla ANVA completa (con grados de libertad, cuadrados medios, etc.), lo que puede ser de interés en algunos casos (teóricos).\nlos resultados del ANVA (res_aov) se pueden guardar para su uso posterior (especialmente útil para pruebas de promedio)\n\n\nInterpretaciones de los resultados del ANVA\nDado que el p-valor es menor que 0.05, rechazamos la hipótesis nula, por lo que rechazamos la hipótesis de que todas las medias son iguales. Por tanto, podemos concluir que al menos una especie es diferente a las otras en términos del ancho de sépalo (p-valor <2.2e-16).\n\n\n¿Que sigue?\nSi no se rechaza la hipótesis nula (p-valor ≥ 0,05), significa que no rechazamos la hipótesis de que todos los grupos son iguales. El ANVA más o menos se detiene aquí. Por supuesto, se pueden realizar otros tipos de análisis, pero, dados los datos disponibles, no pudimos probar que al menos un grupo fuera diferente, por lo que generalmente no avanzamos más con el ANVA.\nPor el contrario, si y solo si se rechaza la hipótesis nula (como es nuestro caso ya que el p-valor < 0.05), probamos que al menos un grupo es diferente. Podemos decidir detenernos aquí si solo estamos interesados en probar si todas las especies son iguales en términos de ancho de sépalo.\nPero la mayoría de las veces, cuando demostramos gracias a un ANVA que al menos un grupo es diferente, también nos interesa saber cuál es diferente. Para probar esto, necesitamos usar otros tipos de prueba, denominados pruebas de promedio o pruebas de comparación múltiple por pares. Esta familia de pruebas estadísticas es el tema de las siguientes secciones."
  },
  {
    "objectID": "posts/anova/index.html#pruebas-de-promedio-en-r-y-su-interpretación",
    "href": "posts/anova/index.html#pruebas-de-promedio-en-r-y-su-interpretación",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Pruebas de promedio en R y su interpretación",
    "text": "Pruebas de promedio en R y su interpretación\nLas pruebas de promedio son una familia de pruebas estadísticas, por lo que hay varias. Las más utilizadas son las pruebas Tukey HSD y Dunnett:\n\nTukey HSD se utiliza para comparar todos los grupos entre sí (por lo que todas las posibles comparaciones de 2 grupos).\nDunnett se utiliza para hacer comparaciones con un grupo de referencia. Por ejemplo, considere 2 grupos de tratamiento y un grupo de control. Si solo desea comparar los 2 grupos de tratamiento con respecto al grupo de control y no desea comparar los 2 grupos de tratamiento entre sí, se prefiere la prueba de Dunnett.\n\nAmbas pruebas se presentan en las siguientes secciones.\n\nPrueba de Tukey HSD\nEn nuestro caso, dado que no existe una especie de “referencia” y nos interesa comparar todas las especies, vamos a utilizar la prueba de Tukey HSD.\nEn R, la prueba de Tukey HSD se realiza de la siguiente manera. Aquí es donde el segundo método para realizar el ANVA resulta útil porque los resultados (res_aov) se reutilizan para la prueba de promedios:\n\n# Prueba de Tukey HSD:\npost_test <- glht(res_aov,\n  linfct = mcp(Species = \"Tukey\")\n)\nsummary(post_test)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = Sepal.Width ~ Species, data = iris)\n\nLinear Hypotheses:\n                            Estimate Std. Error t value Pr(>|t|)    \nversicolor - setosa == 0    -0.65800    0.06794  -9.685  < 1e-04 ***\nvirginica - setosa == 0     -0.45400    0.06794  -6.683  < 1e-04 ***\nvirginica - versicolor == 0  0.20400    0.06794   3.003  0.00871 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nEn el resultado de la prueba Tukey HSD, nos interesa la tabla que se muestra después de las Hipótesis lineales: más precisamente, en la primera y última columna de la tabla. La primera columna muestra las comparaciones que se han realizado; la última columna (Pr(>|t|)) muestra los p-valores ajustados para cada comparación (con la hipótesis nula siendo los dos grupos iguales y la hipótesis alternativa siendo los dos grupos diferentes).\nSon estos p-valores ajustados los que se utilizan para probar si dos grupos son significativamente diferentes o no. En nuestro ejemplo, probamos:\n\nversicolor vs setosa (línea versicolor - setosa == 0)\nvirginica vs setosa (línea virginica - setosa == 0)\nvirginica vs versicolor (línea virginica - versicolor == 0)\n\nLos tres p-valores son menores que 0.05, por lo que rechazamos la hipótesis nula para todas las comparaciones, lo que significa que todas las especies son significativamente diferentes en términos de ancho de sépalo.\nTenga en cuenta que la prueba Tukey HSD también se puede realizar en R con la función TukeyHSD():\n\nTukeyHSD(res_aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Width ~ Species, data = iris)\n\n$Species\n                       diff         lwr        upr     p adj\nversicolor-setosa    -0.658 -0.81885528 -0.4971447 0.0000000\nvirginica-setosa     -0.454 -0.61485528 -0.2931447 0.0000000\nvirginica-versicolor  0.204  0.04314472  0.3648553 0.0087802\n\n\nCon este código, es la columna p adj (también la última columna) la que interesa. Tenga en cuenta que las conclusiones son las mismas que las anteriores: todas las especies son significativamente diferentes en términos de ancho de sépalo."
  },
  {
    "objectID": "posts/anova/index.html#visualización-de-anva-y-pruebas-de-promedio",
    "href": "posts/anova/index.html#visualización-de-anva-y-pruebas-de-promedio",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Visualización de ANVA y pruebas de promedio",
    "text": "Visualización de ANVA y pruebas de promedio\nPara realizar de forma más fácil un análisis de varianza, se puede usar la librería easyanova para analizar diferentes diseños experimentales.\n\nAnálisis de varianza con easyanova\nPara proceder con ANVA los datos de iris se selecciona y ordena para dar uso con el paquete easyanova.\n\nfsdata <- iris %>%\n  dplyr::select(Species, Sepal.Width)\ntibble(fsdata)\n\n# A tibble: 150 × 2\n   Species Sepal.Width\n   <fct>         <dbl>\n 1 setosa          3.5\n 2 setosa          3  \n 3 setosa          3.2\n 4 setosa          3.1\n 5 setosa          3.6\n 6 setosa          3.9\n 7 setosa          3.4\n 8 setosa          3.4\n 9 setosa          2.9\n10 setosa          3.1\n# … with 140 more rows\n\n\n\n# Análisis de varianza para DCA\n\nr1 <- ea1(data = fsdata, # Base de datos\n          design = 1, # Diseño experimental: 1=DCA, 2=DBCA, etc.\n          alpha = 0.05) # Probabilidad estadística\n\n\n\nr1\n\n$`Analysis of variance`\n            df type I SS mean square F value    p>F\ntreatments   2   11.3449      5.6725   49.16 <0.001\nResiduals  147   16.9620      0.1154       -      -\n\n$Means\n   treatment  mean standard.error tukey snk duncan t scott_knott\n1     setosa 3.428          0.048     a   a      a a           a\n2  virginica 2.974          0.048     b   b      b b           b\n3 versicolor 2.770          0.048     c   c      c c           c\n\n$`Multiple comparison test`\n                    pair contrast p(tukey) p(snk) p(duncan)   p(t)\n1     setosa - virginica    0.454   0.0000 0.0000    0.0000 0.0000\n2    setosa - versicolor    0.658   0.0000 0.0000    0.0000 0.0000\n3 virginica - versicolor    0.204   0.0087 0.0031    0.0031 0.0031\n\n$`Residual analysis`\n$`Residual analysis`$`residual analysis`\n                               values\np.value Shapiro-Wilk test      0.3230\np.value Bartlett test          0.3515\ncoefficient of variation (%)  11.1100\nfirst value most discrepant   42.0000\nsecond value most discrepant  16.0000\nthird value most discrepant  118.0000\n\n$`Residual analysis`$residuals\n     1      2      3      4      5      6      7      8      9     10     11 \n 0.072 -0.428 -0.228 -0.328  0.172  0.472 -0.028 -0.028 -0.528 -0.328  0.272 \n    12     13     14     15     16     17     18     19     20     21     22 \n-0.028 -0.428 -0.428  0.572  0.972  0.472  0.072  0.372  0.372 -0.028  0.272 \n    23     24     25     26     27     28     29     30     31     32     33 \n 0.172 -0.128 -0.028 -0.428 -0.028  0.072 -0.028 -0.228 -0.328 -0.028  0.672 \n    34     35     36     37     38     39     40     41     42     43     44 \n 0.772 -0.328 -0.228  0.072  0.172 -0.428 -0.028  0.072 -1.128 -0.228  0.072 \n    45     46     47     48     49     50     51     52     53     54     55 \n 0.372 -0.428  0.372 -0.228  0.272 -0.128  0.430  0.430  0.330 -0.470  0.030 \n    56     57     58     59     60     61     62     63     64     65     66 \n 0.030  0.530 -0.370  0.130 -0.070 -0.770  0.230 -0.570  0.130  0.130  0.330 \n    67     68     69     70     71     72     73     74     75     76     77 \n 0.230 -0.070 -0.570 -0.270  0.430  0.030 -0.270  0.030  0.130  0.230  0.030 \n    78     79     80     81     82     83     84     85     86     87     88 \n 0.230  0.130 -0.170 -0.370 -0.370 -0.070 -0.070  0.230  0.630  0.330 -0.470 \n    89     90     91     92     93     94     95     96     97     98     99 \n 0.230 -0.270 -0.170  0.230 -0.170 -0.470 -0.070  0.230  0.130  0.130 -0.270 \n   100    101    102    103    104    105    106    107    108    109    110 \n 0.030  0.326 -0.274  0.026 -0.074  0.026  0.026 -0.474 -0.074 -0.474  0.626 \n   111    112    113    114    115    116    117    118    119    120    121 \n 0.226 -0.274  0.026 -0.474 -0.174  0.226  0.026  0.826 -0.374 -0.774  0.226 \n   122    123    124    125    126    127    128    129    130    131    132 \n-0.174 -0.174 -0.274  0.326  0.226 -0.174  0.026 -0.174  0.026 -0.174  0.826 \n   133    134    135    136    137    138    139    140    141    142    143 \n-0.174 -0.174 -0.374  0.026  0.426  0.126  0.026  0.126  0.126  0.126 -0.274 \n   144    145    146    147    148    149    150 \n 0.226  0.326  0.026 -0.474  0.026  0.426  0.026 \n\n$`Residual analysis`$`standardized residuals`\n          1           2           3           4           5           6 \n 0.21339641 -1.26852308 -0.67575529 -0.97213918  0.50978030  1.39893200 \n          7           8           9          10          11          12 \n-0.08298749 -0.08298749 -1.56490698 -0.97213918  0.80616420 -0.08298749 \n         13          14          15          16          17          18 \n-1.26852308 -1.26852308  1.69531589  2.88085148  1.39893200  0.21339641 \n         19          20          21          22          23          24 \n 1.10254810  1.10254810 -0.08298749  0.80616420  0.50978030 -0.37937139 \n         25          26          27          28          29          30 \n-0.08298749 -1.26852308 -0.08298749  0.21339641 -0.08298749 -0.67575529 \n         31          32          33          34          35          36 \n-0.97213918 -0.08298749  1.99169979  2.28808369 -0.97213918 -0.67575529 \n         37          38          39          40          41          42 \n 0.21339641  0.50978030 -1.26852308 -0.08298749  0.21339641 -3.34321036 \n         43          44          45          46          47          48 \n-0.67575529  0.21339641  1.10254810 -1.26852308  1.10254810 -0.67575529 \n         49          50          51          52          53          54 \n 0.80616420 -0.37937139  1.27445076  1.27445076  0.97806686 -1.39300432 \n         55          56          57          58          59          60 \n 0.08891517  0.08891517  1.57083466 -1.09662042  0.38529907 -0.20746873 \n         61          62          63          64          65          66 \n-2.28215601  0.68168296 -1.68938822  0.38529907  0.38529907  0.97806686 \n         67          68          69          70          71          72 \n 0.68168296 -0.20746873 -1.68938822 -0.80023652  1.27445076  0.08891517 \n         73          74          75          76          77          78 \n-0.80023652  0.08891517  0.38529907  0.68168296  0.08891517  0.68168296 \n         79          80          81          82          83          84 \n 0.38529907 -0.50385263 -1.09662042 -1.09662042 -0.20746873 -0.20746873 \n         85          86          87          88          89          90 \n 0.68168296  1.86721855  0.97806686 -1.39300432  0.68168296 -0.80023652 \n         91          92          93          94          95          96 \n-0.50385263  0.68168296 -0.50385263 -1.39300432 -0.20746873  0.68168296 \n         97          98          99         100         101         102 \n 0.38529907  0.38529907 -0.80023652  0.08891517  0.96621151 -0.81209188 \n        103         104         105         106         107         108 \n 0.07705981 -0.21932408  0.07705981  0.07705981 -1.40485967 -0.21932408 \n        109         110         111         112         113         114 \n-1.40485967  1.85536320  0.66982761 -0.81209188  0.07705981 -1.40485967 \n        115         116         117         118         119         120 \n-0.51570798  0.66982761  0.07705981  2.44813099 -1.10847578 -2.29401137 \n        121         122         123         124         125         126 \n 0.66982761 -0.51570798 -0.51570798 -0.81209188  0.96621151  0.66982761 \n        127         128         129         130         131         132 \n-0.51570798  0.07705981 -0.51570798  0.07705981 -0.51570798  2.44813099 \n        133         134         135         136         137         138 \n-0.51570798 -0.51570798 -1.10847578  0.07705981  1.26259540  0.37344371 \n        139         140         141         142         143         144 \n 0.07705981  0.37344371  0.37344371  0.37344371 -0.81209188  0.66982761 \n        145         146         147         148         149         150 \n 0.96621151  0.07705981 -1.40485967  0.07705981  1.26259540  0.07705981 \n\n\nEn la salida se puede observar el resultado de análisis de varianza, prueba de promedios y comparación múltiple de medias. Estas salidas son muy fáciles de obtener y poder interpretar las mismas. Asimismo, se puede verificar la normalidad y coeficiente de variación de los datos.\n\n\nVisualización de la prueba de promedios\nSi está interesado en incluir resultados de ANVA y pruebas de promedio directamente en los boxplot, aquí hay un fragmento de código que puede ser de su interés:\n\n#paquete para p-valor en la visualización de prueba de promedios\n\ndat <- iris\n# Editar desde aquí\nx <- which(names(dat) == \"Species\") #variable de agrupación\ny <- which(names(dat) == \"Sepal.Width\") \n#variables para la prueba de promedios\n          #| names(dat) == \"Sepal.Length\"\n          #| names(dat) == \"Petal.Length\"\n          #| names(dat) == \"Petal.Width\")\nmethod1 <- \"anova\" # Una de \"anova\" o \"kruskal.test\"\nmethod2 <- \"t.test\" # Una de \"wilcox.test\" o \"t.test\"\nmy_comparisons <- list(c(\"setosa\", \"versicolor\"), \n                       c(\"setosa\", \"virginica\"), \n                       c(\"versicolor\", \"virginica\")) \n# comparaciones para pruebas de promedio\n# Editar hasta aquí\n\n# Edit at your own risk\nfor (i in y) {\n  for (j in x) {\n    p <- ggboxplot(dat,\n      x = colnames(dat[j]), y = colnames(dat[i]),\n      color = colnames(dat[j]),\n      legend = \"none\",\n      palette = \"npg\",\n      add = \"jitter\"\n    )\n    print(\n      p + stat_compare_means(aes(\n        label = paste0(..method.., \", p-value = \", ..p.format..)),\n        method = method1, label.y = max(dat[, i], na.rm = TRUE)\n      )\n      + stat_compare_means(comparisons = my_comparisons, \n                           method = method2, label = \"p.format\") \n      # remove if p-value of ANOVA or Kruskal-Wallis test >= alpha\n    )\n  }\n}\n\n\n\n\nOtra opción de gráfica para observar la significancia entre las medias de cada par de especies.\n\n# pairwise comparisons\n\npwc <- fsdata %>%\n  pairwise_t_test(\n    Sepal.Width ~ Species, pool.sd = FALSE,\n    p.adjust.method = \"none\"\n    )\n\n# Visualization: box plots with p-values\npwc <- pwc %>% add_xy_position(x = \"Species\")\nggboxplot(fsdata, x = \"Species\", y = \"Sepal.Width\",\n          color = \"Species\", \n          legend = \"none\", \n          add = \"jitter\") +\n  stat_pvalue_manual(pwc, hide.ns = TRUE)"
  },
  {
    "objectID": "posts/anova/index.html#conclusión",
    "href": "posts/anova/index.html#conclusión",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Conclusión",
    "text": "Conclusión\nLa figura de prueba de promedios es muy buena opción para incluir en la sección de resultados de los reportes de investigación. La figura incluye el resultado de p-valor del análisis de varianza, además, p-valor para la comparación de medias entre especies o tratamientos de la investigación."
  },
  {
    "objectID": "posts/anova/index.html#referencias",
    "href": "posts/anova/index.html#referencias",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Referencias",
    "text": "Referencias\n\nR bloggers 2020. ANOVA in R\nSoetewey A. 2020. How to do a t-test or ANOVA for more than one variable at once in R"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Franklin Santos",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nInteligencia artificial en la agricultura\n\n\n\nAI\n\n\nArtificial Intelligence\n\n\nAgriculture\n\n\n\nLa inteligencia artificial (IA) es una tecnología emergente que ha revolucionado muchas industrias, incluyendo la industria agrícola. Con la aplicación de la IA en la…\n\n\n\nFranklin Santos\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapas de Bolivia\n\n\n\nmaps\n\n\nBolivia\n\n\nbioinformatic\n\n\n\nVamos a generar mapas de Bolivia.\n\n\n\nFranklin Santos\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de datos experimentales agrícolas\n\n\n\nagridata\n\n\ndata science\n\n\nbioinformatic\n\n\n\nUso de datos agrícolas y metodologias básicas para un tesista o investigador despues de haber culminado la etapa de campo.\n\n\n\nFranklin Santos\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Componentes Principales\n\n\n\nBiplot\n\n\nComponentes Principales\n\n\nCorrelation\n\n\n\nEl análisis de componentes principales nos permite resumir y visualizar la información de un conjunto de datos que contiene observaciones descritos por múltiples variables…\n\n\n\nFranklin Santos\n\n\nDec 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Varianza y Prueba de Promedios en R\n\n\n\nANOVA\n\n\nPost hoc\n\n\nInferencial Statistic\n\n\n\nLa figura de prueba de promedios de TukeyHSD es una buena opción, ya que incluye p-valor del ANVA y de la comparación de medias entre tratamientos.\n\n\n\nFranklin Santos\n\n\nNov 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de correspondencia con R\n\n\n\nChi-cuadrado\n\n\nBiplot\n\n\nComponentes Principales\n\n\n\nAnálisis para variables categóricas utilizando la metodología de tablas cruzadas con la prueba de chi-cuadrado. Este análisis es una recopilación del libro Multivariate…\n\n\n\nFranklin Santos\n\n\nNov 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrdenar datos con el paquete Tidyverse\n\n\n\nData Science\n\n\nTidy Data\n\n\nTidyverse\n\n\n\nEs un ejemplo de caso extraido del libro R4DS.\n\n\n\nFranklin Santos\n\n\nOct 31, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Franklin Santos",
    "section": "",
    "text": "Franklin Santos is plant and data scientist. He develops research work and data analysis. His research interests include plant and animal breeding. He directs a group of researchers and he is a undergraduate thesis advisor for the Agronomic Engineering Career at the Public University of El Alto."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Franklin Santos",
    "section": "Education",
    "text": "Education\n\nStatistics with R | 2020\n\nDuke University | Coursera Specialization\n\nAnimal Breeding and Genetics | 2020\n\nWageningen University & Research | edX Professional Certificate\n\nData Science | 2020\n\nJohns Hopkins University | Coursera Specialization\n\nAgronomist Engineer, B.Sc. | 2017\n\nEl Alto Public University"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Franklin Santos",
    "section": "Experience",
    "text": "Experience\n\nINIAF | Research Technician | January 2018 - June 2020\nGAM-Licoma | Director of Agricultural Development | June 2016 - Dec 2017\nINE | Municipal Census Chief | August 2013 - November 2013"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "Franklin Santos",
    "section": "Interests",
    "text": "Interests\n\nPlant and Animal Breeding\nMentoring\nData Science\nR programming"
  },
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "Posts",
    "section": "",
    "text": "Análisis de Componentes Principales\n\n\n\n\n\nEl análisis de componentes principales nos permite resumir y visualizar la información de un conjunto de datos que contiene observaciones descritos por múltiples variables cuantitativas inter-correlacionadas.\n\n\n\n\n\n\nDec 15, 2020\n\n\nFranklin Santos\n\n\n\n\n\n\n  \n\n\n\n\nAnálisis de Varianza y Prueba de Promedios en R\n\n\n\n\n\nLa figura de prueba de promedios de TukeyHSD es una buena opción, ya que incluye p-valor del ANVA y de la comparación de medias entre tratamientos.\n\n\n\n\n\n\nNov 15, 2020\n\n\nFranklin Santos\n\n\n\n\n\n\n  \n\n\n\n\nAnálisis de correspondencia con R\n\n\n\n\n\nAnálisis para variables categóricas utilizando la metodología de tablas cruzadas con la prueba de chi-cuadrado. Este análisis es una recopilación del libro Multivariate Analysis II: Practical Guide To Principal Component Methods in R.\n\n\n\n\n\n\nNov 7, 2020\n\n\nFranklin Santos\n\n\n\n\n\n\n  \n\n\n\n\nAnálisis de datos experimentales agrícolas\n\n\n\n\n\nUso de datos agrícolas y metodologias básicas para un tesista o investigador despues de haber culminado la etapa de campo.\n\n\n\n\n\n\nJan 1, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n  \n\n\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMapas de Bolivia\n\n\n\n\n\nVamos a generar mapas de Bolivia.\n\n\n\n\n\n\nFeb 1, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n  \n\n\n\n\nOrdenar datos con el paquete Tidyverse\n\n\n\n\n\nEs un ejemplo de caso extraido del libro R4DS.\n\n\n\n\n\n\nOct 31, 2020\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Surface Response/index.html",
    "href": "posts/Surface Response/index.html",
    "title": "Superficies de respuesta",
    "section": "",
    "text": "Los diseños experimentales de superficie de respuesta y el análisis de sus resultados están orientados a determinar la combinación óptima de factores que nos permita obtener la mejor respuesta dentro de la región experimental. Como mejor respuesta podemos referirnos a un máximo o a un mínimo dependiendo de nuestros objetivos.\nEs recomendable que los diseños de superficie de respuesta se realicen una vez hayamos determinado previamente qué factores tienen un efecto significativo sobre la respuesta. Esto puede hacerse mediante un diseño fraccionado.\nLas superficies de respuesta son herramientas útiles en ensayos de manejo agronómico. Estas superficies muestran cómo una variable de interés, como el rendimiento de un cultivo, responde a diferentes niveles de una o más variables de manejo, como la cantidad de fertilizante aplicado o el momento de la siembra.\nLas superficies de respuesta se construyen utilizando datos de ensayos de campo, en los que se evalúan diferentes niveles de la variable de manejo de interés. Por ejemplo, se podría evaluar el efecto de diferentes niveles de fertilización en el rendimiento de un cultivo. Para cada nivel de fertilización, se mide el rendimiento del cultivo y se registra en una tabla de datos.\nUna vez que se tienen los datos, se pueden graficar en una superficie de respuesta. En este tipo de gráfico, los niveles de la variable de manejo se representan en los ejes x e y, mientras que el rendimiento del cultivo se representa en el eje z. La superficie resultante muestra cómo el rendimiento del cultivo cambia en función de los niveles de la variable de manejo.\nLas superficies de respuesta son útiles para optimizar el manejo agronómico, ya que permiten identificar los niveles óptimos de las variables de manejo para maximizar el rendimiento del cultivo. Por ejemplo, la superficie de respuesta podría mostrar que el rendimiento del cultivo es máximo cuando se aplica una cantidad específica de fertilizante. Al conocer esta información, los agricultores pueden ajustar su manejo para maximizar el rendimiento y reducir los costos de producción."
  },
  {
    "objectID": "posts/Surface Response/index.html#generación-de-diseños-de-superficie-con-el-paquete-rsm",
    "href": "posts/Surface Response/index.html#generación-de-diseños-de-superficie-con-el-paquete-rsm",
    "title": "Superficies de respuesta",
    "section": "Generación de diseños de superficie con el paquete rsm",
    "text": "Generación de diseños de superficie con el paquete rsm\nLa generación de este tipo de diseños es bastante sencilla con el paquete rsm. Si desea ver la funcionalidad completa de este paquete pulse en el siguiente enlace: paquete rsm. También existe un documento con varios ejemplos en el siguiente enlace: ejemplos con rsm."
  },
  {
    "objectID": "posts/AI/index.html",
    "href": "posts/AI/index.html",
    "title": "Inteligencia artificial en la agricultura",
    "section": "",
    "text": "La inteligencia artificial es la capacidad de una máquina o sistema de software para realizar tareas que normalmente requerirían inteligencia humana, como el aprendizaje, la percepción, el razonamiento y la toma de decisiones. La IA puede ser utilizada en una amplia variedad de aplicaciones, desde el reconocimiento de imágenes hasta la automatización de procesos."
  },
  {
    "objectID": "posts/AI/index.html#el-estado-del-arte-de-la-ia",
    "href": "posts/AI/index.html#el-estado-del-arte-de-la-ia",
    "title": "Inteligencia artificial en la agricultura",
    "section": "El estado del arte de la IA",
    "text": "El estado del arte de la IA\nEn la actualidad, la IA se encuentra en un estado avanzado de desarrollo. Los sistemas de IA están mejorando rápidamente y se están utilizando en una amplia variedad de aplicaciones, incluyendo la medicina, el transporte, la banca y la industria agrícola."
  },
  {
    "objectID": "posts/AI/index.html#desafíos-y-oportunidades-futuras",
    "href": "posts/AI/index.html#desafíos-y-oportunidades-futuras",
    "title": "Inteligencia artificial en la agricultura",
    "section": "Desafíos y oportunidades futuras",
    "text": "Desafíos y oportunidades futuras\nA pesar del progreso significativo, todavía hay desafíos que se deben abordar en la aplicación de la IA en la agricultura. Algunos de estos desafíos incluyen la falta de datos de calidad y la resistencia de los agricultores a adoptar nuevas tecnologías. Sin embargo, la oportunidad de mejorar la producción y la sostenibilidad agrícola mediante la aplicación de la IA sigue siendo significativa."
  },
  {
    "objectID": "posts/AI/index.html#habilidades-en-programación-y-ciencia-de-datos-para-ingenieros-agrónomos",
    "href": "posts/AI/index.html#habilidades-en-programación-y-ciencia-de-datos-para-ingenieros-agrónomos",
    "title": "Inteligencia artificial en la agricultura",
    "section": "Habilidades en programación y ciencia de datos para ingenieros agrónomos",
    "text": "Habilidades en programación y ciencia de datos para ingenieros agrónomos\nLos ingenieros agrónomos pueden mejorar su trabajo en la agronomía aprendiendo habilidades en programación y ciencia de datos. Estas habilidades incluyen aprender los fundamentos de programación, lenguajes de programación populares, realizar cursos en línea o presenciales, trabajar en proyectos prácticos y participar en eventos y comunidades de programación y ciencia de datos."
  },
  {
    "objectID": "posts/AI/index.html#la-aplicación-de-la-ia-en-la-agricultura",
    "href": "posts/AI/index.html#la-aplicación-de-la-ia-en-la-agricultura",
    "title": "Inteligencia artificial en la agricultura",
    "section": "La aplicación de la IA en la agricultura",
    "text": "La aplicación de la IA en la agricultura\nLa aplicación de la IA en la agricultura puede proporcionar soluciones para mejorar la producción, la calidad y la sostenibilidad de los cultivos. Algunas de las aplicaciones de la IA en la agricultura incluyen el análisis de datos agrícolas para la toma de decisiones, la automatización de procesos agrícolas, el monitoreo y la detección de enfermedades en los cultivos, la identificación de plagas y la predicción de la calidad de los cultivos."
  },
  {
    "objectID": "posts/AI/index.html#conclusión",
    "href": "posts/AI/index.html#conclusión",
    "title": "Inteligencia artificial en la agricultura",
    "section": "Conclusión",
    "text": "Conclusión\nLa IA es una tecnología emergente que tiene el potencial de revolucionar la forma en que se aborda la producción agrícola. Los ingenieros agrónomos pueden mejorar su trabajo en la agronomía aprendiendo habilidades en programación y ciencia de datos, lo que les permitirá aprovechar la IA para mejorar la producción, la calidad y la sostenibilidad de los cultivos. A medida que la tecnología continúa avanzando, es probable que veamos aún más aplicaciones de la IA en la industria agrícola."
  }
]