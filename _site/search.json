[
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "Posts",
    "section": "",
    "text": "Analisis de datos climaticos\n\n\n\n\n\nMostramos unas figuras relacionadas a datos climaticos, las cuales pueden ser utilizados para presentar en manuscritos. \n\n\n\n\n\nJun 25, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Componentes Principales\n\n\n\n\n\nEl análisis de componentes principales nos permite resumir y visualizar la información de un conjunto de datos que contiene observaciones descritos por múltiples variables cuantitativas inter-correlacionadas. \n\n\n\n\n\nDec 15, 2020\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Varianza y Prueba de Promedios en R\n\n\n\n\n\nLa figura de prueba de promedios de TukeyHSD es una buena opción, ya que incluye p-valor del ANVA y de la comparación de medias entre tratamientos. \n\n\n\n\n\nNov 15, 2020\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de correspondencia con R\n\n\n\n\n\nAnálisis para variables categóricas utilizando la metodología de tablas cruzadas con la prueba de chi-cuadrado. Este análisis es una recopilación del libro Multivariate Analysis II: Practical Guide To Principal Component Methods in R. \n\n\n\n\n\nNov 7, 2020\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de datos experimentales agrícolas\n\n\n\n\n\nUso de datos agrícolas y metodologias básicas para un tesista o investigador despues de haber culminado la etapa de campo. \n\n\n\n\n\nJan 1, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nDumbbell plot: visualizar cambio en ggplot2\n\n\n\n\n\nUna manera de comparar los cambios en una figura. \n\n\n\n\n\nJul 20, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing Plant Breeding Precision with Row-Column Design\n\n\n\n\n\nRow-column design involves arranging experimental units into a grid of rows and columns. \n\n\n\n\n\nMay 26, 2024\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nExplorando el Mundo de la Estadística: Frecuentista vs. Bayesiana\n\n\n\n\n\nLa estadística es el arte y la ciencia de extraer conocimiento a partir de datos, y dos enfoques principales han moldeado el campo a lo largo del tiempo: el frecuentista y el bayesiano. \n\n\n\n\n\nDec 25, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Frontier: Artificial Intelligence in Molecular Plant Breeding Data Science\n\n\n\n\n\nAI in molecular plant breeding data science is not just a technological advancement; it’s a paradigm shift. \n\n\n\n\n\nJan 25, 2024\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nFAMD - Factor Analysis of Mixed Data in R\n\n\n\n\n\nFactor Analysis of Mixed Data (FAMD) is a powerful statistical technique used to analyze datasets that contain both numerical and categorical variables. \n\n\n\n\n\nNov 29, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteligencia artificial en la agricultura\n\n\n\n\n\nLa inteligencia artificial (IA) es una tecnología emergente que ha revolucionado muchas industrias, incluyendo la industria agrícola. Con la aplicación de la IA en la agronomía, se pueden obtener beneficios significativos, como una producción más eficiente y sostenible. En este artículo, se explorará cómo la IA está cambiando la forma en que se aborda la producción agrícola. \n\n\n\n\n\nMar 15, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nJULIA: Lenguaje de programación del futuro\n\n\n\n\n\nJulia es un lenguaje de programación de alto nivel, diseñado específicamente para el cómputo científico y la estadística. \n\n\n\n\n\nApr 25, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nLa inteligencia artificial revoluciona la agricultura\n\n\n\n\n\nLa agricultura es un sector que se enfrenta a una serie de desafíos, como el cambio climático, la escasez de recursos y la creciente demanda de alimentos. La inteligencia artificial (IA) ofrece una serie de soluciones prometedoras para estos desafíos. \n\n\n\n\n\nAug 30, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nMapas de Bolivia\n\n\n\n\n\nVamos a generar mapas de Bolivia. \n\n\n\n\n\nFeb 1, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nOrdenar datos con el paquete Tidyverse\n\n\n\n\n\nEs un ejemplo de caso extraido del libro R4DS. \n\n\n\n\n\nOct 31, 2020\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nThe corrr package in R: A powerful tool for exploring correlations\n\n\n\n\n\nIt is a tool for exploring correlations, and it makes it possible to easily perform routine tasks when exploring correlation matrices. \n\n\n\n\n\nSep 20, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nUn Viaje de Sueños: Camino a la Universidad Austral de Chile\n\n\n\n\n\nRecuerden: donde hay voluntad, hay camino, y con la guía divina, los límites son solo el comienzo de nuevas posibilidades. \n\n\n\n\n\nApr 4, 2024\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments\n\n\n\n\n\nCRD is a statistical method that serves as a robust tool for comparing the effects of different treatments on a single factor within crop experiments. \n\n\n\n\n\nOct 24, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2: facet_wrap, facet_grid\n\n\n\n\n\nfacet_grid y facet_wrap son funciones poderosas de ggplot2 que permiten dividir un gráfico en múltiples paneles según variables categóricas. Estas funciones son útiles para explorar la relación entre variables en diferentes subconjuntos de datos y ayudan a visualizar patrones y tendencias de manera más efectiva. \n\n\n\n\n\nMay 15, 2023\n\n\nFranklin Santos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Franklin Santos",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBack-transformations with emmeans() in R\n\n\n\nR\n\n\nNormalidad\n\n\nTransformation\n\n\n\nTransformaciones inversas: Regresando a la escala original en el análisis de datos.\n\n\n\nFranklin Santos\n\n\nAug 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPotato Diversity Analysis:A Step-by-Step Guide\n\n\n\nDiversity\n\n\nmultivariate\n\n\nPlant Breeding\n\n\n\nComprehensive approach to analyzing the genetic diversity of potato using various statistical and multivariate analysis methods in R.\n\n\n\nFranklin Santos\n\n\nJul 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiseño Completamente al Azar\n\n\n\nRCD\n\n\nExperimental Design\n\n\nPlant Breeding\n\n\n\nEl diseño más sencillo desde el punto de vista de la asignación de unidades experimentales a los tratamientos.\n\n\n\nFranklin Santos\n\n\nJun 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing Plant Breeding Precision with Row-Column Design\n\n\n\nRow-Column\n\n\nExperimental Design\n\n\nPlant Breeding\n\n\n\nRow-column design involves arranging experimental units into a grid of rows and columns.\n\n\n\nFranklin Santos\n\n\nMay 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUn Viaje de Sueños: Camino a la Universidad Austral de Chile\n\n\n\nScholarship\n\n\nMaster Science\n\n\nAGCID Chile\n\n\n\nRecuerden: donde hay voluntad, hay camino, y con la guía divina, los límites son solo el comienzo de nuevas posibilidades.\n\n\n\nFranklin Santos\n\n\nApr 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Frontier: Artificial Intelligence in Molecular Plant Breeding Data Science\n\n\n\nAI\n\n\nPlant Breeding\n\n\nGenomic Selection\n\n\n\nAI in molecular plant breeding data science is not just a technological advancement; it’s a paradigm shift.\n\n\n\nFranklin Santos\n\n\nJan 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplorando el Mundo de la Estadística: Frecuentista vs. Bayesiana\n\n\n\nBayesian\n\n\nStatistic\n\n\nBayes\n\n\n\nLa estadística es el arte y la ciencia de extraer conocimiento a partir de datos, y dos enfoques principales han moldeado el campo a lo largo del tiempo: el frecuentista y…\n\n\n\nFranklin Santos\n\n\nDec 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFAMD - Factor Analysis of Mixed Data in R\n\n\n\nFAMD\n\n\nFactoMineR\n\n\nfactoextra\n\n\n\nFactor Analysis of Mixed Data (FAMD) is a powerful statistical technique used to analyze datasets that contain both numerical and categorical variables.\n\n\n\nFranklin Santos\n\n\nNov 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments\n\n\n\ncrd\n\n\nagroR package\n\n\nExperimental Design\n\n\n\nCRD is a statistical method that serves as a robust tool for comparing the effects of different treatments on a single factor within crop experiments.\n\n\n\nFranklin Santos\n\n\nOct 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe corrr package in R: A powerful tool for exploring correlations\n\n\n\ncorrelation\n\n\ncorrr package\n\n\ncorrelation plot\n\n\n\nIt is a tool for exploring correlations, and it makes it possible to easily perform routine tasks when exploring correlation matrices.\n\n\n\nFranklin Santos\n\n\nSep 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa inteligencia artificial revoluciona la agricultura\n\n\n\nAI\n\n\nagriculture\n\n\nartificial intelligence\n\n\n\nLa agricultura es un sector que se enfrenta a una serie de desafíos, como el cambio climático, la escasez de recursos y la creciente demanda de alimentos. La inteligencia…\n\n\n\nFranklin Santos\n\n\nAug 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDumbbell plot: visualizar cambio en ggplot2\n\n\n\ndata science\n\n\nggalt\n\n\ndumbbell plot\n\n\n\nUna manera de comparar los cambios en una figura.\n\n\n\nFranklin Santos\n\n\nJul 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalisis de datos climaticos\n\n\n\ndata science\n\n\nweather analysis\n\n\nplot\n\n\n\nMostramos unas figuras relacionadas a datos climaticos, las cuales pueden ser utilizados para presentar en manuscritos.\n\n\n\nFranklin Santos\n\n\nJun 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2: facet_wrap, facet_grid\n\n\n\nggplot2\n\n\nfacet_wrap\n\n\nfacet_grid\n\n\n\nfacet_grid y facet_wrap son funciones poderosas de ggplot2 que permiten dividir un gráfico en múltiples paneles según variables categóricas. Estas funciones son útiles para…\n\n\n\nFranklin Santos\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJULIA: Lenguaje de programación del futuro\n\n\n\njulia\n\n\nArtificial Intelligence\n\n\nData Science\n\n\n\nJulia es un lenguaje de programación de alto nivel, diseñado específicamente para el cómputo científico y la estadística.\n\n\n\nFranklin Santos\n\n\nApr 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteligencia artificial en la agricultura\n\n\n\nAI\n\n\nArtificial Intelligence\n\n\nAgriculture\n\n\n\nLa inteligencia artificial (IA) es una tecnología emergente que ha revolucionado muchas industrias, incluyendo la industria agrícola. Con la aplicación de la IA en la…\n\n\n\nFranklin Santos\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapas de Bolivia\n\n\n\nmaps\n\n\nBolivia\n\n\nbioinformatic\n\n\n\nVamos a generar mapas de Bolivia.\n\n\n\nFranklin Santos\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de datos experimentales agrícolas\n\n\n\nagridata\n\n\ndata science\n\n\nbioinformatic\n\n\n\nUso de datos agrícolas y metodologias básicas para un tesista o investigador despues de haber culminado la etapa de campo.\n\n\n\nFranklin Santos\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Componentes Principales\n\n\n\nBiplot\n\n\nComponentes Principales\n\n\nCorrelation\n\n\n\nEl análisis de componentes principales nos permite resumir y visualizar la información de un conjunto de datos que contiene observaciones descritos por múltiples variables…\n\n\n\nFranklin Santos\n\n\nDec 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de Varianza y Prueba de Promedios en R\n\n\n\nANOVA\n\n\nPost hoc\n\n\nInferencial Statistic\n\n\n\nLa figura de prueba de promedios de TukeyHSD es una buena opción, ya que incluye p-valor del ANVA y de la comparación de medias entre tratamientos.\n\n\n\nFranklin Santos\n\n\nNov 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de correspondencia con R\n\n\n\nChi-cuadrado\n\n\nBiplot\n\n\nComponentes Principales\n\n\n\nAnálisis para variables categóricas utilizando la metodología de tablas cruzadas con la prueba de chi-cuadrado. Este análisis es una recopilación del libro Multivariate…\n\n\n\nFranklin Santos\n\n\nNov 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrdenar datos con el paquete Tidyverse\n\n\n\nData Science\n\n\nTidy Data\n\n\nTidyverse\n\n\n\nEs un ejemplo de caso extraido del libro R4DS.\n\n\n\nFranklin Santos\n\n\nOct 31, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/anova/index.html",
    "href": "posts/anova/index.html",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "",
    "text": "El análisis de varianza es una prueba estadística para determinar si dos o más medias poblacionales son diferentes entre si. En otras palabras, se usa para comparar dos o más grupos para ver si son significativamente diferentes.\nEn el resto del post lo comentaremos desde un punto de vista más práctico y en particular abordaremos los siguientes puntos:\n\nel objetivo del análisis de varianza y cuándo debe usarse\ncómo realizar el ANVA en R\ncómo interpretar los resultados del ANVA\ncomprender la noción de prueba de promedios e interpretar los resultados\ncómo visualizar los resultados de ANVA y pruebas de promedio"
  },
  {
    "objectID": "posts/anova/index.html#introducción",
    "href": "posts/anova/index.html#introducción",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "",
    "text": "El análisis de varianza es una prueba estadística para determinar si dos o más medias poblacionales son diferentes entre si. En otras palabras, se usa para comparar dos o más grupos para ver si son significativamente diferentes.\nEn el resto del post lo comentaremos desde un punto de vista más práctico y en particular abordaremos los siguientes puntos:\n\nel objetivo del análisis de varianza y cuándo debe usarse\ncómo realizar el ANVA en R\ncómo interpretar los resultados del ANVA\ncomprender la noción de prueba de promedios e interpretar los resultados\ncómo visualizar los resultados de ANVA y pruebas de promedio"
  },
  {
    "objectID": "posts/anova/index.html#datos",
    "href": "posts/anova/index.html#datos",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Datos",
    "text": "Datos\nEl dato que se utilizará es iris, que se encuentra en la base de datos de R. Estos datos como tratamientos tienen tres especies (setosa, versicolor y virginica) y cuatro variables (Sepal.Length, Sepal.Width, Petal.Length y Petal.Width) cuantitativas\n\n#paquetes R a utilizar\nlibrary(tidyverse)\nlibrary(easyanova)\nlibrary(car)\nlibrary(lattice)\nlibrary(multcomp)\nlibrary(ggpubr)\nlibrary(rstatix)\n\nLa librería de easyanova es un paquete para realizar análisis de experimentos agrícolas y animales. Las funciones de esta librería son fáciles de usar. Realiza análisis en varios diseños, con datos balanceados y no balanceados.\nSalida de datos a utilizar:\n\n#datos\ntibble(iris)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 140 more rows\n\n\n\n#inspección de datos\np &lt;- ggplot(iris) +\n  aes(x = Species, y = Sepal.Width, color = Species) +\n  geom_jitter() +\n  theme(legend.position = \"none\")\n\nlibrary(plotly)\nfig &lt;- ggplotly(p)\n\nfig"
  },
  {
    "objectID": "posts/anova/index.html#objetivo-del-anva",
    "href": "posts/anova/index.html#objetivo-del-anva",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Objetivo del ANVA",
    "text": "Objetivo del ANVA\nComo se mencionó en la introducción, el ANVA se usa para comparar grupos (en la práctica, 3 o más grupos). De manera más general, se utiliza para:\n\nestudiar si las mediciones son similares en diferentes modalidades (también llamadas niveles o tratamientos en el contexto de ANVA) de una variable categórica\ncomparar el impacto de los diferentes niveles de una variable categórica sobre una variable cuantitativa\nexplicar una variable cuantitativa basada en una variable cualitativa"
  },
  {
    "objectID": "posts/anova/index.html#supuestos-subyacentes-de-anva",
    "href": "posts/anova/index.html#supuestos-subyacentes-de-anva",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Supuestos subyacentes de ANVA",
    "text": "Supuestos subyacentes de ANVA\nComo ocurre con muchas pruebas estadísticas, hay algunas suposiciones que deben cumplirse para poder interpretar los resultados. Cuando no se cumplen uno o varios supuestos, aunque técnicamente es posible realizar estas pruebas, sería incorrecto interpretar los resultados y confiar en las conclusiones.\n\n1. Tipo de variable\nLas variables dependientes Sepal.Length, Sepal.Width, Petal.Length y Petal.Width es una variable cuantitativa y la variable independiente Species es cualitativa (con 3 niveles correspondientes a las 3 especies). Así que tenemos una combinación de los dos tipos de variables y se cumple este supuesto.\n\n\n2. Independencia\nSe asume la independencia de las observaciones ya que los datos se han recopilado de una parte de la población seleccionada al azar y las mediciones dentro y entre las 3 muestras no están relacionadas.\nEl supuesto de independencia se verifica con mayor frecuencia con base en el diseño del experimento y en el buen control de las condiciones experimentales, como es el caso aquí. Sin embargo, si realmente desea probarlo de manera más formal, puede probarlo mediante una prueba estadística: la prueba de Durbin-Watson (en R: durbinWatsonTest(res_lm) donde res_lm es un modelo lineal). La hipótesis nula de esta prueba especifica un coeficiente de autocorrelación = 0, mientras que la hipótesis alternativa especifica un coeficiente de autocorrelación ≠ 0.\n\n\n3. Normalidad\nRecuerde que la normalidad de los residuos se puede probar visualmente mediante un histograma y un gráfico QQ, y/o formalmente mediante una prueba de normalidad (prueba de Shapiro-Wilk, por ejemplo).\nAntes de verificar el supuesto de normalidad, primero debemos calcular el ANVA. Luego guardamos los resultados en res_aov:\n\nres_aov &lt;- aov(Sepal.Width ~ Species,\n  data = iris\n)\n\nAhora podemos comprobar la normalidad visualmente:\n\npar(mfrow = c(1, 2)) # combine plots\n\n# histogram\nhist(res_aov$residuals)\n\n# QQ-plot\nqqPlot(res_aov$residuals,\n  id = FALSE # id = FALSE to remove point identification\n)\n\n\n\n\n\n\n\n\nA partir del histograma y el gráfico QQ anteriores, ya podemos ver que el supuesto de normalidad parece cumplirse. De hecho, el histograma forma aproximadamente una curva de campana, lo que indica que los residuos siguen una distribución normal. Además, los puntos en las gráficas QQ siguen aproximadamente la línea recta y la mayoría de ellos están dentro de las bandas de confianza, lo que también indica que los residuos siguen aproximadamente una distribución normal.\nAlgunos investigadores se detienen aquí y asumen que se cumple la normalidad, mientras que otros también prueban la suposición a través de una prueba estadística formal. Es su elección probarlo (i) solo visualmente, (ii) solo a través de una prueba de normalidad, o (iii) tanto visualmente como a través de una prueba de normalidad. Sin embargo, tenga en cuenta los dos puntos siguientes:\n\n\nANVA es bastante robusto a pequeñas desviaciones de la normalidad. Esto significa que no es un problema (desde la perspectiva de la interpretación de los resultados de ANVA) si un pequeño número de puntos se desvía ligeramente de la normalidad,\nLas pruebas de normalidad son a veces bastante conservadoras, lo que significa que la hipótesis nula de normalidad puede rechazarse debido a una desviación limitada de la normalidad. Este es especialmente el caso con muestras grandes, ya que la potencia de la prueba aumenta con el tamaño de la muestra.\n\n\nEn la práctica, se tiende a preferir el (i) enfoque visual solamente, pero nuevamente, esto es una cuestión de elección personal y también depende del contexto del análisis. Tambien, puede utilizar la prueba de Shapiro-Wilk o la prueba de Kolmogorov-Smirnov, entre otras.\n\n\n4. Igualdad de varianzas - homogeneidad\nSuponiendo que los residuos siguen una distribución normal, ahora es el momento de comprobar si las varianzas son iguales entre especies o no. El resultado tendrá un impacto en si usamos el ANVA o la prueba de Welch.\nEsto se puede verificar nuevamente visualmente, a través de una gráfica de caja o gráfica de puntos, o más formalmente a través de una prueba estadística (la prueba de Levene, entre otras).\nVisualmente tenemos:\n\n# Boxplot\nboxplot(Sepal.Width ~ Species,\n  data = iris\n)\n\n\n\n\n\n\n\n\n\n# Dotplot\n\ndotplot(Sepal.Width ~ Species,\n  data = iris\n)\n\n\n\n\n\n\n\n\nTanto la gráfica de boxplot como la gráfica de puntos muestran una variación similar para las diferentes especies. En el boxplot, esto se puede ver por el hecho de que las cajas y los bigotes tienen un tamaño comparable para todas las especies. Hay un par de valores atípicos como lo muestran los puntos fuera de los bigotes, pero esto no cambia el hecho de que la dispersión es más o menos la misma entre las diferentes especies.\nEn la gráfica de puntos, esto se puede ver por el hecho de que los puntos para las 3 especies tienen más o menos el mismo rango, un signo de la dispersión y, por lo tanto, la varianza es similar.\nAl igual que el supuesto de normalidad, si cree que el enfoque visual no es suficiente, puede probar formalmente la igualdad de las varianzas con una prueba de Levene o de Bartlett. Observe que la prueba de Levene es menos sensible a las desviaciones de la distribución normal que la prueba de Bartlett.\nLas hipótesis nula y alternativa para ambas pruebas son:\n\nH0: las variaciones son iguales\nH1: al menos una varianza es diferente\n\nEn R, la prueba de Levene se puede realizar gracias a la función leveneTest() del paquete {car}:\n\n# Levene's test\n\nleveneTest(Sepal.Width ~ Species,\n  data = iris\n)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   2  0.5902 0.5555\n      147               \n\n\nSiendo el p-valor mayor que el nivel de significancia de 0.05, no rechazamos la hipótesis nula, por lo que no podemos rechazar la hipótesis de que las varianzas son iguales entre especies (p-valor = 0.556).\nEste resultado también está en línea con el enfoque visual, por lo que la homogeneidad de las variaciones se cumple tanto visual como formalmente."
  },
  {
    "objectID": "posts/anova/index.html#análisis-de-varianza-en-r",
    "href": "posts/anova/index.html#análisis-de-varianza-en-r",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Análisis de varianza en R",
    "text": "Análisis de varianza en R\nEl ANVA puede ayudarnos a hacer inferencias sobre la población dada la muestra en cuestión y ayudarnos a responder la pregunta de investigación “¿Existe diferencia en ancho de sépalo para las 3 especies?”.\nEl ANVA en R se puede realizar de varias formas, de las cuales tres se presentan a continuación:\na). Con la función oneway.test():\n\n# primer metodo:\noneway.test(Sepal.Width ~ Species,\n  data = iris,\n  var.equal = TRUE # asumiendo varianzas iguales\n)\n\n\n    One-way analysis of means\n\ndata:  Sepal.Width and Species\nF = 49.16, num df = 2, denom df = 147, p-value &lt; 2.2e-16\n\n\nb). Con las funciones de summary() y aov():\n\n# 2nd method:\nres_aov &lt;- aov(Sepal.Width ~ Species,\n  data = iris\n)\n\nsummary(res_aov)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  11.35   5.672   49.16 &lt;2e-16 ***\nResiduals   147  16.96   0.115                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nComo puede ver en los dos resultados anteriores, la estadística de prueba (F = en el primer método y el valor F en el segundo) y el p-valor (p-valor en el primer método y Pr (&gt; F) en el segundo) son exactamente iguales para ambos métodos, lo que significa que en caso de variaciones iguales, los resultados y las conclusiones no cambiarán.\nLa ventaja del primer método es que es fácil cambiar del ANVA (utilizado cuando las variaciones son iguales) a la prueba de Welch (utilizado cuando las variaciones son desiguales). Esto se puede hacer reemplazando nvar.equal = TRUE por var.equal = FALSE, como se presenta a continuación:\n\noneway.test(Sepal.Width ~ Species,\n  data = iris,\n  var.equal = FALSE # asumiendo variaciones desiguales\n)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  Sepal.Width and Species\nF = 45.012, num df = 2.000, denom df = 97.402, p-value = 1.433e-14\n\n\nSin embargo, la ventaja del segundo método es que:\n\nSe imprime la tabla ANVA completa (con grados de libertad, cuadrados medios, etc.), lo que puede ser de interés en algunos casos (teóricos).\nlos resultados del ANVA (res_aov) se pueden guardar para su uso posterior (especialmente útil para pruebas de promedio)\n\n\nInterpretaciones de los resultados del ANVA\nDado que el p-valor es menor que 0.05, rechazamos la hipótesis nula, por lo que rechazamos la hipótesis de que todas las medias son iguales. Por tanto, podemos concluir que al menos una especie es diferente a las otras en términos del ancho de sépalo (p-valor &lt;2.2e-16).\n\n\n¿Que sigue?\nSi no se rechaza la hipótesis nula (p-valor ≥ 0,05), significa que no rechazamos la hipótesis de que todos los grupos son iguales. El ANVA más o menos se detiene aquí. Por supuesto, se pueden realizar otros tipos de análisis, pero, dados los datos disponibles, no pudimos probar que al menos un grupo fuera diferente, por lo que generalmente no avanzamos más con el ANVA.\nPor el contrario, si y solo si se rechaza la hipótesis nula (como es nuestro caso ya que el p-valor &lt; 0.05), probamos que al menos un grupo es diferente. Podemos decidir detenernos aquí si solo estamos interesados en probar si todas las especies son iguales en términos de ancho de sépalo.\nPero la mayoría de las veces, cuando demostramos gracias a un ANVA que al menos un grupo es diferente, también nos interesa saber cuál es diferente. Para probar esto, necesitamos usar otros tipos de prueba, denominados pruebas de promedio o pruebas de comparación múltiple por pares. Esta familia de pruebas estadísticas es el tema de las siguientes secciones."
  },
  {
    "objectID": "posts/anova/index.html#pruebas-de-promedio-en-r-y-su-interpretación",
    "href": "posts/anova/index.html#pruebas-de-promedio-en-r-y-su-interpretación",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Pruebas de promedio en R y su interpretación",
    "text": "Pruebas de promedio en R y su interpretación\nLas pruebas de promedio son una familia de pruebas estadísticas, por lo que hay varias. Las más utilizadas son las pruebas Tukey HSD y Dunnett:\n\nTukey HSD se utiliza para comparar todos los grupos entre sí (por lo que todas las posibles comparaciones de 2 grupos).\nDunnett se utiliza para hacer comparaciones con un grupo de referencia. Por ejemplo, considere 2 grupos de tratamiento y un grupo de control. Si solo desea comparar los 2 grupos de tratamiento con respecto al grupo de control y no desea comparar los 2 grupos de tratamiento entre sí, se prefiere la prueba de Dunnett.\n\nAmbas pruebas se presentan en las siguientes secciones.\n\nPrueba de Tukey HSD\nEn nuestro caso, dado que no existe una especie de “referencia” y nos interesa comparar todas las especies, vamos a utilizar la prueba de Tukey HSD.\nEn R, la prueba de Tukey HSD se realiza de la siguiente manera. Aquí es donde el segundo método para realizar el ANVA resulta útil porque los resultados (res_aov) se reutilizan para la prueba de promedios:\n\n# Prueba de Tukey HSD:\npost_test &lt;- glht(res_aov,\n  linfct = mcp(Species = \"Tukey\")\n)\nsummary(post_test)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = Sepal.Width ~ Species, data = iris)\n\nLinear Hypotheses:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \nversicolor - setosa == 0    -0.65800    0.06794  -9.685  &lt; 1e-04 ***\nvirginica - setosa == 0     -0.45400    0.06794  -6.683  &lt; 1e-04 ***\nvirginica - versicolor == 0  0.20400    0.06794   3.003  0.00871 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nEn el resultado de la prueba Tukey HSD, nos interesa la tabla que se muestra después de las Hipótesis lineales: más precisamente, en la primera y última columna de la tabla. La primera columna muestra las comparaciones que se han realizado; la última columna (Pr(&gt;|t|)) muestra los p-valores ajustados para cada comparación (con la hipótesis nula siendo los dos grupos iguales y la hipótesis alternativa siendo los dos grupos diferentes).\nSon estos p-valores ajustados los que se utilizan para probar si dos grupos son significativamente diferentes o no. En nuestro ejemplo, probamos:\n\nversicolor vs setosa (línea versicolor - setosa == 0)\nvirginica vs setosa (línea virginica - setosa == 0)\nvirginica vs versicolor (línea virginica - versicolor == 0)\n\nLos tres p-valores son menores que 0.05, por lo que rechazamos la hipótesis nula para todas las comparaciones, lo que significa que todas las especies son significativamente diferentes en términos de ancho de sépalo.\nTenga en cuenta que la prueba Tukey HSD también se puede realizar en R con la función TukeyHSD():\n\nTukeyHSD(res_aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Width ~ Species, data = iris)\n\n$Species\n                       diff         lwr        upr     p adj\nversicolor-setosa    -0.658 -0.81885528 -0.4971447 0.0000000\nvirginica-setosa     -0.454 -0.61485528 -0.2931447 0.0000000\nvirginica-versicolor  0.204  0.04314472  0.3648553 0.0087802\n\n\nCon este código, es la columna p adj (también la última columna) la que interesa. Tenga en cuenta que las conclusiones son las mismas que las anteriores: todas las especies son significativamente diferentes en términos de ancho de sépalo."
  },
  {
    "objectID": "posts/anova/index.html#visualización-de-anva-y-pruebas-de-promedio",
    "href": "posts/anova/index.html#visualización-de-anva-y-pruebas-de-promedio",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Visualización de ANVA y pruebas de promedio",
    "text": "Visualización de ANVA y pruebas de promedio\nPara realizar de forma más fácil un análisis de varianza, se puede usar la librería easyanova para analizar diferentes diseños experimentales.\n\nAnálisis de varianza con easyanova\nPara proceder con ANVA los datos de iris se selecciona y ordena para dar uso con el paquete easyanova.\n\nfsdata &lt;- iris %&gt;%\n  dplyr::select(Species, Sepal.Width)\ntibble(fsdata)\n\n# A tibble: 150 × 2\n   Species Sepal.Width\n   &lt;fct&gt;         &lt;dbl&gt;\n 1 setosa          3.5\n 2 setosa          3  \n 3 setosa          3.2\n 4 setosa          3.1\n 5 setosa          3.6\n 6 setosa          3.9\n 7 setosa          3.4\n 8 setosa          3.4\n 9 setosa          2.9\n10 setosa          3.1\n# … with 140 more rows\n\n\n\n# Análisis de varianza para DCA\n\nr1 &lt;- ea1(data = fsdata, # Base de datos\n          design = 1, # Diseño experimental: 1=DCA, 2=DBCA, etc.\n          alpha = 0.05) # Probabilidad estadística\n\n\n\n\n\n\n\nr1\n\n$`Analysis of variance`\n            df type I SS mean square F value    p&gt;F\ntreatments   2   11.3449      5.6725   49.16 &lt;0.001\nResiduals  147   16.9620      0.1154       -      -\n\n$Means\n   treatment  mean standard.error tukey snk duncan t scott_knott\n1     setosa 3.428          0.048     a   a      a a           a\n2  virginica 2.974          0.048     b   b      b b           b\n3 versicolor 2.770          0.048     c   c      c c           c\n\n$`Multiple comparison test`\n                    pair contrast p(tukey) p(snk) p(duncan)   p(t)\n1     setosa - virginica    0.454   0.0000 0.0000    0.0000 0.0000\n2    setosa - versicolor    0.658   0.0000 0.0000    0.0000 0.0000\n3 virginica - versicolor    0.204   0.0087 0.0031    0.0031 0.0031\n\n$`Residual analysis`\n$`Residual analysis`$`residual analysis`\n                               values\np.value Shapiro-Wilk test      0.3230\np.value Bartlett test          0.3515\ncoefficient of variation (%)  11.1100\nfirst value most discrepant   42.0000\nsecond value most discrepant  16.0000\nthird value most discrepant  118.0000\n\n$`Residual analysis`$residuals\n     1      2      3      4      5      6      7      8      9     10     11 \n 0.072 -0.428 -0.228 -0.328  0.172  0.472 -0.028 -0.028 -0.528 -0.328  0.272 \n    12     13     14     15     16     17     18     19     20     21     22 \n-0.028 -0.428 -0.428  0.572  0.972  0.472  0.072  0.372  0.372 -0.028  0.272 \n    23     24     25     26     27     28     29     30     31     32     33 \n 0.172 -0.128 -0.028 -0.428 -0.028  0.072 -0.028 -0.228 -0.328 -0.028  0.672 \n    34     35     36     37     38     39     40     41     42     43     44 \n 0.772 -0.328 -0.228  0.072  0.172 -0.428 -0.028  0.072 -1.128 -0.228  0.072 \n    45     46     47     48     49     50     51     52     53     54     55 \n 0.372 -0.428  0.372 -0.228  0.272 -0.128  0.430  0.430  0.330 -0.470  0.030 \n    56     57     58     59     60     61     62     63     64     65     66 \n 0.030  0.530 -0.370  0.130 -0.070 -0.770  0.230 -0.570  0.130  0.130  0.330 \n    67     68     69     70     71     72     73     74     75     76     77 \n 0.230 -0.070 -0.570 -0.270  0.430  0.030 -0.270  0.030  0.130  0.230  0.030 \n    78     79     80     81     82     83     84     85     86     87     88 \n 0.230  0.130 -0.170 -0.370 -0.370 -0.070 -0.070  0.230  0.630  0.330 -0.470 \n    89     90     91     92     93     94     95     96     97     98     99 \n 0.230 -0.270 -0.170  0.230 -0.170 -0.470 -0.070  0.230  0.130  0.130 -0.270 \n   100    101    102    103    104    105    106    107    108    109    110 \n 0.030  0.326 -0.274  0.026 -0.074  0.026  0.026 -0.474 -0.074 -0.474  0.626 \n   111    112    113    114    115    116    117    118    119    120    121 \n 0.226 -0.274  0.026 -0.474 -0.174  0.226  0.026  0.826 -0.374 -0.774  0.226 \n   122    123    124    125    126    127    128    129    130    131    132 \n-0.174 -0.174 -0.274  0.326  0.226 -0.174  0.026 -0.174  0.026 -0.174  0.826 \n   133    134    135    136    137    138    139    140    141    142    143 \n-0.174 -0.174 -0.374  0.026  0.426  0.126  0.026  0.126  0.126  0.126 -0.274 \n   144    145    146    147    148    149    150 \n 0.226  0.326  0.026 -0.474  0.026  0.426  0.026 \n\n$`Residual analysis`$`standardized residuals`\n          1           2           3           4           5           6 \n 0.21339641 -1.26852308 -0.67575529 -0.97213918  0.50978030  1.39893200 \n          7           8           9          10          11          12 \n-0.08298749 -0.08298749 -1.56490698 -0.97213918  0.80616420 -0.08298749 \n         13          14          15          16          17          18 \n-1.26852308 -1.26852308  1.69531589  2.88085148  1.39893200  0.21339641 \n         19          20          21          22          23          24 \n 1.10254810  1.10254810 -0.08298749  0.80616420  0.50978030 -0.37937139 \n         25          26          27          28          29          30 \n-0.08298749 -1.26852308 -0.08298749  0.21339641 -0.08298749 -0.67575529 \n         31          32          33          34          35          36 \n-0.97213918 -0.08298749  1.99169979  2.28808369 -0.97213918 -0.67575529 \n         37          38          39          40          41          42 \n 0.21339641  0.50978030 -1.26852308 -0.08298749  0.21339641 -3.34321036 \n         43          44          45          46          47          48 \n-0.67575529  0.21339641  1.10254810 -1.26852308  1.10254810 -0.67575529 \n         49          50          51          52          53          54 \n 0.80616420 -0.37937139  1.27445076  1.27445076  0.97806686 -1.39300432 \n         55          56          57          58          59          60 \n 0.08891517  0.08891517  1.57083466 -1.09662042  0.38529907 -0.20746873 \n         61          62          63          64          65          66 \n-2.28215601  0.68168296 -1.68938822  0.38529907  0.38529907  0.97806686 \n         67          68          69          70          71          72 \n 0.68168296 -0.20746873 -1.68938822 -0.80023652  1.27445076  0.08891517 \n         73          74          75          76          77          78 \n-0.80023652  0.08891517  0.38529907  0.68168296  0.08891517  0.68168296 \n         79          80          81          82          83          84 \n 0.38529907 -0.50385263 -1.09662042 -1.09662042 -0.20746873 -0.20746873 \n         85          86          87          88          89          90 \n 0.68168296  1.86721855  0.97806686 -1.39300432  0.68168296 -0.80023652 \n         91          92          93          94          95          96 \n-0.50385263  0.68168296 -0.50385263 -1.39300432 -0.20746873  0.68168296 \n         97          98          99         100         101         102 \n 0.38529907  0.38529907 -0.80023652  0.08891517  0.96621151 -0.81209188 \n        103         104         105         106         107         108 \n 0.07705981 -0.21932408  0.07705981  0.07705981 -1.40485967 -0.21932408 \n        109         110         111         112         113         114 \n-1.40485967  1.85536320  0.66982761 -0.81209188  0.07705981 -1.40485967 \n        115         116         117         118         119         120 \n-0.51570798  0.66982761  0.07705981  2.44813099 -1.10847578 -2.29401137 \n        121         122         123         124         125         126 \n 0.66982761 -0.51570798 -0.51570798 -0.81209188  0.96621151  0.66982761 \n        127         128         129         130         131         132 \n-0.51570798  0.07705981 -0.51570798  0.07705981 -0.51570798  2.44813099 \n        133         134         135         136         137         138 \n-0.51570798 -0.51570798 -1.10847578  0.07705981  1.26259540  0.37344371 \n        139         140         141         142         143         144 \n 0.07705981  0.37344371  0.37344371  0.37344371 -0.81209188  0.66982761 \n        145         146         147         148         149         150 \n 0.96621151  0.07705981 -1.40485967  0.07705981  1.26259540  0.07705981 \n\n\nEn la salida se puede observar el resultado de análisis de varianza, prueba de promedios y comparación múltiple de medias. Estas salidas son muy fáciles de obtener y poder interpretar las mismas. Asimismo, se puede verificar la normalidad y coeficiente de variación de los datos.\n\n\nVisualización de la prueba de promedios\nSi está interesado en incluir resultados de ANVA y pruebas de promedio directamente en los boxplot, aquí hay un fragmento de código que puede ser de su interés:\n\n#paquete para p-valor en la visualización de prueba de promedios\n\ndat &lt;- iris\n# Editar desde aquí\nx &lt;- which(names(dat) == \"Species\") #variable de agrupación\ny &lt;- which(names(dat) == \"Sepal.Width\") \n#variables para la prueba de promedios\n          #| names(dat) == \"Sepal.Length\"\n          #| names(dat) == \"Petal.Length\"\n          #| names(dat) == \"Petal.Width\")\nmethod1 &lt;- \"anova\" # Una de \"anova\" o \"kruskal.test\"\nmethod2 &lt;- \"t.test\" # Una de \"wilcox.test\" o \"t.test\"\nmy_comparisons &lt;- list(c(\"setosa\", \"versicolor\"), \n                       c(\"setosa\", \"virginica\"), \n                       c(\"versicolor\", \"virginica\")) \n# comparaciones para pruebas de promedio\n# Editar hasta aquí\n\n# Edit at your own risk\nfor (i in y) {\n  for (j in x) {\n    p &lt;- ggboxplot(dat,\n      x = colnames(dat[j]), y = colnames(dat[i]),\n      color = colnames(dat[j]),\n      legend = \"none\",\n      palette = \"npg\",\n      add = \"jitter\"\n    )\n    print(\n      p + stat_compare_means(aes(\n        label = paste0(..method.., \", p-value = \", ..p.format..)),\n        method = method1, label.y = max(dat[, i], na.rm = TRUE)\n      )\n      + stat_compare_means(comparisons = my_comparisons, \n                           method = method2, label = \"p.format\") \n      # remove if p-value of ANOVA or Kruskal-Wallis test &gt;= alpha\n    )\n  }\n}\n\n\n\n\n\n\n\n\nOtra opción de gráfica para observar la significancia entre las medias de cada par de especies.\n\n# pairwise comparisons\n\npwc &lt;- fsdata %&gt;%\n  pairwise_t_test(\n    Sepal.Width ~ Species, pool.sd = FALSE,\n    p.adjust.method = \"none\"\n    )\n\n# Visualization: box plots with p-values\npwc &lt;- pwc %&gt;% add_xy_position(x = \"Species\")\nggboxplot(fsdata, x = \"Species\", y = \"Sepal.Width\",\n          color = \"Species\", \n          legend = \"none\", \n          add = \"jitter\") +\n  stat_pvalue_manual(pwc, hide.ns = TRUE)"
  },
  {
    "objectID": "posts/anova/index.html#conclusión",
    "href": "posts/anova/index.html#conclusión",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Conclusión",
    "text": "Conclusión\nLa figura de prueba de promedios es muy buena opción para incluir en la sección de resultados de los reportes de investigación. La figura incluye el resultado de p-valor del análisis de varianza, además, p-valor para la comparación de medias entre especies o tratamientos de la investigación."
  },
  {
    "objectID": "posts/anova/index.html#referencias",
    "href": "posts/anova/index.html#referencias",
    "title": "Análisis de Varianza y Prueba de Promedios en R",
    "section": "Referencias",
    "text": "Referencias\n\nR bloggers 2020. ANOVA in R\nSoetewey A. 2020. How to do a t-test or ANOVA for more than one variable at once in R"
  },
  {
    "objectID": "posts/row_column_design/index.html",
    "href": "posts/row_column_design/index.html",
    "title": "Enhancing Plant Breeding Precision with Row-Column Design",
    "section": "",
    "text": "In the realm of agricultural research, particularly plant breeding, precision is paramount. The quest to identify superior plant varieties requires experiments that can accurately attribute observed differences to genetic factors rather than environmental noise. This is where the row-column design comes into play, a sophisticated statistical methodology designed to enhance the reliability of field trials by controlling spatial variation."
  },
  {
    "objectID": "posts/row_column_design/index.html#introduction",
    "href": "posts/row_column_design/index.html#introduction",
    "title": "Enhancing Plant Breeding Precision with Row-Column Design",
    "section": "",
    "text": "In the realm of agricultural research, particularly plant breeding, precision is paramount. The quest to identify superior plant varieties requires experiments that can accurately attribute observed differences to genetic factors rather than environmental noise. This is where the row-column design comes into play, a sophisticated statistical methodology designed to enhance the reliability of field trials by controlling spatial variation."
  },
  {
    "objectID": "posts/row_column_design/index.html#what-is-row-column-design",
    "href": "posts/row_column_design/index.html#what-is-row-column-design",
    "title": "Enhancing Plant Breeding Precision with Row-Column Design",
    "section": "What is Row-Column Design?",
    "text": "What is Row-Column Design?\nAt its core, a row-column design involves arranging experimental units into a grid of rows and columns. This organization allows researchers to control two sources of spatial variation simultaneously. Each cell in the grid, or plot, hosts a specific genotype (or treatment), which is randomly assigned to ensure unbiased results."
  },
  {
    "objectID": "posts/row_column_design/index.html#why-is-it-important",
    "href": "posts/row_column_design/index.html#why-is-it-important",
    "title": "Enhancing Plant Breeding Precision with Row-Column Design",
    "section": "Why is it Important?",
    "text": "Why is it Important?\nField trials often face the challenge of spatial heterogeneity—variations in soil fertility, moisture, sunlight, and other environmental conditions. These variations can introduce significant noise into the data, potentially obscuring true genetic differences. By accounting for variations along both rows and columns, the row-column design mitigates these effects, leading to more precise and reliable experimental outcomes."
  },
  {
    "objectID": "posts/row_column_design/index.html#key-features",
    "href": "posts/row_column_design/index.html#key-features",
    "title": "Enhancing Plant Breeding Precision with Row-Column Design",
    "section": "Key Features",
    "text": "Key Features\n\nGrid Layout: The field is divided into a systematic grid, creating rows and columns that structure the experimental units.\nRandomization: Treatments are randomly assigned to each plot within the grid, minimizing bias.\nControl of Variation:\n\nRow Effects: Horizontal variations such as soil fertility gradients or irrigation patterns.\nColumn Effects: Vertical variations such as different drainage conditions or sunlight exposure."
  },
  {
    "objectID": "posts/row_column_design/index.html#advantages-of-row-column-design",
    "href": "posts/row_column_design/index.html#advantages-of-row-column-design",
    "title": "Enhancing Plant Breeding Precision with Row-Column Design",
    "section": "Advantages of Row-Column Design",
    "text": "Advantages of Row-Column Design\n\nIncreased Precision: By controlling for two dimensions of variability, this design reduces experimental error, enhancing the accuracy of genotype evaluations.\nFlexibility: Adaptable to fields with irregular shapes or non-uniform conditions, making it versatile for various agricultural scenarios.\nImproved Accuracy: Better control of environmental variation ensures that performance differences are more accurately attributed to genetic factors."
  },
  {
    "objectID": "posts/row_column_design/index.html#row-column-design-with-fieldhub-r-package",
    "href": "posts/row_column_design/index.html#row-column-design-with-fieldhub-r-package",
    "title": "Enhancing Plant Breeding Precision with Row-Column Design",
    "section": "Row-Column Design with FielDHub R Package",
    "text": "Row-Column Design with FielDHub R Package\nTo facilitate the implementation and analysis of row-column designs, researchers can utilize the FieldHUB R package. FieldHUB is a powerful tool specifically developed for managing and analyzing field trial data. It provides a user-friendly interface and a suite of functions tailored to the unique needs of agricultural experiments.\n\nDesign Creation: FieldHUB allows researchers to easily set up row-column designs, specifying the number of rows and columns, and randomizing the assignment of treatments.\nData Management: The package streamlines data entry and management, ensuring that all experimental data are accurately recorded and organized.\nVisualization: FieldHUB offers various visualization tools to help researchers understand spatial patterns and variations within their trials, aiding in the interpretation of results.\nStatistical Analysis: Integrated with R’s robust statistical capabilities, FieldHUB can perform complex analyses, including mixed models and ANOVA, tailored to row-column designs.\n\nBy leveraging the FieldHUB R package, researchers can enhance the efficiency and accuracy of their field trials, making the row-column design more accessible and practical for large-scale agricultural research.\n\nUsing the FielDHub function: row_column()\nFirst, you need to load the FielDHub package by typing\n\n#Install and load FielDHub package\nlibrary(FielDHub)\n\nThen, you can enter the information describing the above design like this:\n\nrcd &lt;- row_column(\n  t = 45,\n  nrows = 5,\n  r = 3,\n  l = 1, \n  plotNumber = 101, \n  locationNames = \"FARGO\",\n  seed = 1244\n)\n\nThe description for the inputs that we used to generate the design,\n\nt = 45 is the number of treatments.\nnrows = 5 is the number of rows.\nr=3 is the number of reps\nl = 1 is the number of locations.\nplotNumber = 101 is the starting plot number.\nlocationNames = \"FARGO\" is an optional name for each location.\nseed = 1244 is the random seed to replicate identical randomizations.\n\nTo print a summary of the information that is in the object rcd, we can use the generic function print().\n\nprint(rcd)\n\nResolvable Row-Column Design (Two-Step Optimization) \n\nEfficiency of design: \n   Level Blocks D-Efficiency A-Efficiency   A-Bound\n1    Rep      3    1.0000000    1.0000000 1.0000000\n2    Row     15    0.8940599    0.8767433 0.8842892\n3 Column     27    0.7912130    0.7623893 0.7674419\n\nInformation on the design parameters: \nList of 7\n $ rows          : num 5\n $ columns       : num 9\n $ reps          : num 3\n $ treatments    : num 45\n $ locations     : num 1\n $ location_names: chr \"FARGO\"\n $ seed          : num 1244\n\n 10 First observations of the data frame with the row_column field book: \n   ID LOCATION PLOT REP ROW COLUMN ENTRY TREATMENT\n1   1    FARGO  101   1   1      1    30      G-30\n6   2    FARGO  102   1   1      2    41      G-41\n11  3    FARGO  103   1   1      3     5       G-5\n16  4    FARGO  104   1   1      4    39      G-39\n21  5    FARGO  105   1   1      5    22      G-22\n26  6    FARGO  106   1   1      6    44      G-44\n31  7    FARGO  107   1   1      7     9       G-9\n36  8    FARGO  108   1   1      8    36      G-36\n41  9    FARGO  109   1   1      9    11      G-11\n2  10    FARGO  110   1   2      1     6       G-6\n\n\nThe row_column() function returns a list consisting of all the information displayed in the output tabs in the FielDHub app: design information, plot layout, plot numbering, entries list, and field book. These are accessible by the $ operator, i.e. rcd$layoutRandom or rcd$fieldBook.\nrcd$fieldBook is a list containing information about every plot in the field, with information about the location of the plot and the treatment in each plot. As seen in the output below, the field book has columns for ID, LOCATION, PLOT, REP, ROW, COLUMN, ENTRY, and TREATMENT.\n\nfield_book &lt;- rcd$fieldBook\nhead(rcd$fieldBook, 10)\n\n   ID LOCATION PLOT REP ROW COLUMN ENTRY TREATMENT\n1   1    FARGO  101   1   1      1    30      G-30\n6   2    FARGO  102   1   1      2    41      G-41\n11  3    FARGO  103   1   1      3     5       G-5\n16  4    FARGO  104   1   1      4    39      G-39\n21  5    FARGO  105   1   1      5    22      G-22\n26  6    FARGO  106   1   1      6    44      G-44\n31  7    FARGO  107   1   1      7     9       G-9\n36  8    FARGO  108   1   1      8    36      G-36\n41  9    FARGO  109   1   1      9    11      G-11\n2  10    FARGO  110   1   2      1     6       G-6\n\n\nFor plotting the layout in function of the coordinates ROW and COLUMN, you can use the the generic function plot() as follows,\n\nplot(rcd)"
  },
  {
    "objectID": "posts/row_column_design/index.html#analytical-techniques",
    "href": "posts/row_column_design/index.html#analytical-techniques",
    "title": "Enhancing Plant Breeding Precision with Row-Column Design",
    "section": "Analytical Techniques",
    "text": "Analytical Techniques\nThe data from row-column designs are typically analyzed using mixed models or ANOVA, with row and column effects included as factors. Advanced statistical software like R (using packages such as lme4, asreml, SpAtS, and statgenSTA) and SAS provides robust tools for handling the complex data structures generated by these designs."
  },
  {
    "objectID": "posts/row_column_design/index.html#conclusion",
    "href": "posts/row_column_design/index.html#conclusion",
    "title": "Enhancing Plant Breeding Precision with Row-Column Design",
    "section": "Conclusion",
    "text": "Conclusion\nRow-column design stands as a critical tool for plant breeders, ensuring that the evaluation of new plant genotypes is as accurate and reliable as possible. By effectively controlling spatial variation in field trials, this design not only improves the precision of experimental results but also aids in the selection and development of superior plant varieties. As agriculture continues to evolve, methodologies like the row-column design will remain essential in advancing our understanding and cultivation of crops."
  },
  {
    "objectID": "posts/facet_ggplot2/index.html",
    "href": "posts/facet_ggplot2/index.html",
    "title": "ggplot2: facet_wrap, facet_grid",
    "section": "",
    "text": "ggplot2 es un paquete de visualización de datos en R que utiliza la gramática de gráficos para crear gráficos flexibles y de alta calidad. Con su sintaxis intuitiva, amplias opciones de personalización y variedad de gráficos predefinidos, ggplot2 se ha convertido en una herramienta popular y poderosa para la visualización de datos en la comunidad de R.\nfacet_grid y facet_wrap son dos funciones en el paquete ggplot2 de R que permiten dividir un gráfico en múltiples paneles según las variables de una o varias variables categóricas. Estas funciones son especialmente útiles cuando se desea explorar la relación entre variables en diferentes subconjuntos de datos."
  },
  {
    "objectID": "posts/facet_ggplot2/index.html#introducción",
    "href": "posts/facet_ggplot2/index.html#introducción",
    "title": "ggplot2: facet_wrap, facet_grid",
    "section": "",
    "text": "ggplot2 es un paquete de visualización de datos en R que utiliza la gramática de gráficos para crear gráficos flexibles y de alta calidad. Con su sintaxis intuitiva, amplias opciones de personalización y variedad de gráficos predefinidos, ggplot2 se ha convertido en una herramienta popular y poderosa para la visualización de datos en la comunidad de R.\nfacet_grid y facet_wrap son dos funciones en el paquete ggplot2 de R que permiten dividir un gráfico en múltiples paneles según las variables de una o varias variables categóricas. Estas funciones son especialmente útiles cuando se desea explorar la relación entre variables en diferentes subconjuntos de datos."
  },
  {
    "objectID": "posts/facet_ggplot2/index.html#facet-wrap",
    "href": "posts/facet_ggplot2/index.html#facet-wrap",
    "title": "ggplot2: facet_wrap, facet_grid",
    "section": "Facet wrap",
    "text": "Facet wrap\n\nlibrary(ggplot2)\nmpg2 &lt;- subset(mpg, cyl != 5 & drv %in% c(\"4\", \"f\") & class != \"2seater\")\nbase &lt;- ggplot(mpg2, aes(displ, hwy)) + \n  geom_blank() + \n  xlab(NULL) + \n  ylab(NULL)\n\nbase + facet_wrap(~class, ncol = 3)\n\n\n\n\n\n\n\nbase + facet_wrap(~class, ncol = 3, as.table = FALSE)\n\n\n\n\n\n\n\n\n\nbase + facet_wrap(~class, nrow = 3)\n\n\n\n\n\n\n\nbase + facet_wrap(~class, nrow = 3, dir = \"v\")"
  },
  {
    "objectID": "posts/facet_ggplot2/index.html#facet-grid",
    "href": "posts/facet_ggplot2/index.html#facet-grid",
    "title": "ggplot2: facet_wrap, facet_grid",
    "section": "Facet grid",
    "text": "Facet grid\n\nbase + facet_grid(. ~ cyl)\n\n\n\n\n\n\n\n\n\nfacet_grid() dispone los gráficos en una cuadrícula 2D, definida por una fórmula:\n\n. ~ a distribuye los valores de a a través de las columnas. Esta dirección facilita las comparaciones de la posición y, porque las escalas verticales están alineadas.\n\nbase + facet_grid(. ~ cyl)\n\n\n\n\n\n\n\n\n\nb ~ . reparte los valores de b por las filas. Esta dirección facilita la comparación de la posición x porque las escalas horizontales están alineadas. Esto la hace especialmente útil para comparar distribuciones.\n\n\nbase + facet_grid(drv ~ .)\n\n\n\n\n\n\n\n\n\nb ~ a distribuye a a lo largo de las columnas y b a lo largo de las filas. Normalmente querrás poner la variable con el mayor número de niveles en las columnas, para aprovechar la relación de aspecto de tu pantalla.\n\n\nbase + facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n\nPuede utilizar múltiples variables en las filas o columnas, “sumándolas”, por ejemplo a + b ~ c + d. Las variables que aparecen juntas en las filas o columnas están anidadas en el sentido de que sólo las combinaciones que aparecen en los datos aparecerán en el gráfico. Las variables que se especifican en filas y columnas se cruzarán: se mostrarán todas las combinaciones, incluidas las que no aparecían en el conjunto de datos original: esto puede dar lugar a paneles vacíos.\n\n# Simulación de datos\nset.seed(4)\nx &lt;- runif(500)\ny &lt;- 4 * x ^ 2 + rnorm(length(x), sd = 5)\ngrupo1 &lt;- ifelse(x &lt; 0.4, \"G1\", ifelse(x &lt; 0.6, \"G2\", ifelse(x &lt; 0.8, \"G3\", \"G4\")))\ngrupo2 &lt;- ifelse(y &lt; 0.5, \"A\", \"B\")\nx &lt;- x + runif(length(x), -0.5, 0.5)\n\n# Data frame\ndf &lt;- data.frame(x = x, y = y, grupo1 = grupo1, grupo2 = grupo2)\n\n# Matriz de gráficos\nggplot(df, aes(x = x, y = y, color = grupo1)) +\n  geom_point(show.legend = FALSE) +\n  facet_grid(grupo1 ~ grupo2, scales = \"free\")"
  },
  {
    "objectID": "posts/facet_ggplot2/index.html#controlando-escalas",
    "href": "posts/facet_ggplot2/index.html#controlando-escalas",
    "title": "ggplot2: facet_wrap, facet_grid",
    "section": "Controlando escalas",
    "text": "Controlando escalas\n\neconomics_long\n\n# A tibble: 2,870 × 4\n   date       variable value  value01\n   &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 1967-07-01 pce       507. 0       \n 2 1967-08-01 pce       510. 0.000265\n 3 1967-09-01 pce       516. 0.000762\n 4 1967-10-01 pce       512. 0.000471\n 5 1967-11-01 pce       517. 0.000916\n 6 1967-12-01 pce       525. 0.00157 \n 7 1968-01-01 pce       531. 0.00207 \n 8 1968-02-01 pce       534. 0.00230 \n 9 1968-03-01 pce       544. 0.00322 \n10 1968-04-01 pce       544  0.00319 \n# ℹ 2,860 more rows\n\nggplot(economics_long, aes(date, value)) + \n  geom_line() + \n  facet_wrap(~variable, scales = \"free_y\", ncol = 1)\n\n\n\n\n\n\n\n\n\nmpg2$model &lt;- reorder(mpg2$model, mpg2$cty)\nmpg2$manufacturer &lt;- reorder(mpg2$manufacturer, -mpg2$cty)\n\nggplot(mpg2, aes(cty, model)) + \n  geom_point() + \n  facet_grid(manufacturer ~ ., scales = \"free\", space = \"free\") +\n  theme(strip.text.y = element_text(angle = 0))"
  },
  {
    "objectID": "posts/weather_mizque/index.html",
    "href": "posts/weather_mizque/index.html",
    "title": "Analisis de datos climaticos",
    "section": "",
    "text": "library(readxl)\ndf &lt;- read_excel(\"/Users/franklin/Documents/R/myblog/posts/weather_mizque/dato_diarios.xlsx\")"
  },
  {
    "objectID": "posts/weather_mizque/index.html#data",
    "href": "posts/weather_mizque/index.html#data",
    "title": "Analisis de datos climaticos",
    "section": "",
    "text": "library(readxl)\ndf &lt;- read_excel(\"/Users/franklin/Documents/R/myblog/posts/weather_mizque/dato_diarios.xlsx\")"
  },
  {
    "objectID": "posts/weather_mizque/index.html#analisis-de-temperatura",
    "href": "posts/weather_mizque/index.html#analisis-de-temperatura",
    "title": "Analisis de datos climaticos",
    "section": "analisis de temperatura",
    "text": "analisis de temperatura\n\n#Real data Mizque\nlibrary(metan)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n|=========================================================|\n\n\n| Multi-Environment Trial Analysis (metan) v1.18.0        |\n\n\n| Author: Tiago Olivoto                                   |\n\n\n| Type 'citation('metan')' to know how to cite metan      |\n\n\n| Type 'vignette('metan_start')' for a short tutorial     |\n\n\n| Visit 'https://bit.ly/pkgmetan' for a complete tutorial |\n\n\n|=========================================================|\n\ninspect(df, 2:14, plot = F)\n\n# A tibble: 13 × 10\n   Variable      Class Missing Levels Valid_n    Min Median    Max Outlier Text \n   &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1 \"mes\"         nume… No      -          485    1      5     12         0  &lt;NA&gt;\n 2 \"dia\"         nume… No      -          485    1     16     31         0  &lt;NA&gt;\n 3 \"estacion\"    char… No      0          485   NA     NA     NA        NA \"Lin…\n 4 \"longitud\"    nume… No      -          485  -65.4  -65.4  -65.4       0  &lt;NA&gt;\n 5 \"latitud\"     nume… No      -          485  -18.0  -18.0  -18.0       0  &lt;NA&gt;\n 6 \"altura\"      nume… No      -          485 2055   2055   2055         0  &lt;NA&gt;\n 7 \"Precipitaci… char… No      0          485   NA     NA     NA        NA \"Lin…\n 8 \"\\\"Temperatu… char… No      0          485   NA     NA     NA        NA \"Lin…\n 9 \"\\\"Temperatu… char… No      0          485   NA     NA     NA        NA \"Lin…\n10 \"\\\"Temperatu… char… No      0          485   NA     NA     NA        NA \"Lin…\n11 \"\\\"Humedad R… char… No      0          485   NA     NA     NA        NA \"Lin…\n12 \"\\\"Humedad R… char… No      0          485   NA     NA     NA        NA \"Lin…\n13 \"\\\"Humedad R… char… No      0          485   NA     NA     NA        NA \"Lin…\n\n\nWarning: Considering the levels of factors, .data should have 1 rows, but it\nhas 485. Use 'as_factor()' for coercing a variable to a factor.\n\n\nWarning: Expected three or more factor variables. The data has only 0.\n\n\nWarning: Possible text fragments in variable(s) estacion, Precipitación,\n\"Temperatura Máxima\", \"Temperatura Mínima\", \"Temperatura Media\", \"Humedad\nRelativa Máxima\", \"Humedad Relativa Mínima\", \"Humedad Relativa Media\".\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ forcats::as_factor()         masks metan::as_factor()\n✖ tibble::column_to_rownames() masks metan::column_to_rownames()\n✖ dplyr::filter()              masks stats::filter()\n✖ dplyr::lag()                 masks stats::lag()\n✖ dplyr::recode_factor()       masks metan::recode_factor()\n✖ tibble::remove_rownames()    masks metan::remove_rownames()\n✖ tidyr::replace_na()          masks metan::replace_na()\n✖ tibble::rownames_to_column() masks metan::rownames_to_column()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnames(df)\n\n [1] \"gestion\"                     \"mes\"                        \n [3] \"dia\"                         \"estacion\"                   \n [5] \"longitud\"                    \"latitud\"                    \n [7] \"altura\"                      \"Precipitación\"              \n [9] \"\\\"Temperatura Máxima\\\"\"      \"\\\"Temperatura Mínima\\\"\"     \n[11] \"\\\"Temperatura Media\\\"\"       \"\\\"Humedad Relativa Máxima\\\"\"\n[13] \"\\\"Humedad Relativa Mínima\\\"\" \"\\\"Humedad Relativa Media\\\"\""
  },
  {
    "objectID": "posts/weather_mizque/index.html#rename-variables",
    "href": "posts/weather_mizque/index.html#rename-variables",
    "title": "Analisis de datos climaticos",
    "section": "Rename variables",
    "text": "Rename variables\n\nnewvar = c(PP = \"Precipitación\", \n           Tmax = \"\\\"Temperatura Máxima\\\"\",\n           Tmin = \"\\\"Temperatura Mínima\\\"\",\n           Tmean = \"\\\"Temperatura Media\\\"\",\n           HRmax = \"\\\"Humedad Relativa Máxima\\\"\",\n           HRmin = \"\\\"Humedad Relativa Mínima\\\"\",\n           HRmean = \"\\\"Humedad Relativa Media\\\"\")\n\n\ndb = df%&gt;%\n  rename(any_of(newvar))%&gt;%\n  select(c(1,2,3, 8:14))%&gt;%\n  mutate_if(is.character, as.numeric)%&gt;%\n  mutate(mes = as.factor(mes), \n         gestion = as.factor(gestion))%&gt;%\n  mutate(mes=recode(mes, \n                    '1'='Ene', \n                    '2'='Feb',\n                    '3'='Mar',\n                    '4'='Abr',\n                    '5'='May',\n                    '6'='Jun',\n                    '7'='Jul',\n                    '8'='Ago',\n                    '9'='Sep',\n                    '10'='Oct',\n                    '11'='Nov',\n                    '12'='Dic'))\ndb  \n\n# A tibble: 485 × 10\n   gestion mes     dia    PP  Tmax  Tmin Tmean HRmax HRmin HRmean\n   &lt;fct&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 2022    Ene       1   0.4  31.4  9.92  20.6  93.2  19.3   53.4\n 2 2022    Ene       2   0    33.3 11.7   19.7  96.6  18.0   69.6\n 3 2022    Ene       3   5.1  29.6 14.4   21.0  88.7  27.2   58.7\n 4 2022    Ene       4   0    30.1 14.1   20.1  90.8  23.6   66.3\n 5 2022    Ene       5   1.5  21.9 13.4   16.7  94.6  47.8   81.0\n 6 2022    Ene       6   4.5  25.5 14.4   19.8  94.3  35.9   65.8\n 7 2022    Ene       7   0    27.3 14.7   20.4  95.6  39.9   67.3\n 8 2022    Ene       8  24.2  25.3 15.3   19.0  91.3  49.6   74.5\n 9 2022    Ene       9   0    26.9 15.5   20.2  92.7  38.5   70.6\n10 2022    Ene      10   0    24   14.5   18.5  91.9  47.2   73.0\n# ℹ 475 more rows"
  },
  {
    "objectID": "posts/weather_mizque/index.html#plot-temperature",
    "href": "posts/weather_mizque/index.html#plot-temperature",
    "title": "Analisis de datos climaticos",
    "section": "plot temperature",
    "text": "plot temperature\n\nlibrary(ggridges)\n\nmy_theme &lt;- \n  theme_bw() +\n  theme(legend.title = element_blank(),\n        axis.title = element_text(color = \"black\"),\n        axis.text = element_text(color = \"black\"),\n        panel.grid.minor = element_blank())\n\np1 = ggplot(\n  data = db,\n  aes(x = `Tmean`,\n      y = `mes`,\n      fill = after_stat(x))\n) +\n  geom_density_ridges_gradient(scale = 1.5, size = 0.1, \n                               rel_min_height = 0.001) +\n  scale_fill_viridis_c(name = \"Temp (ºC)\", \n                       direction = 1, option = \"H\") +\n  my_theme +\n  theme(legend.position = c(0.95, 0.4))+\n  labs(x=\"Temperatura promedio (ºC)\", y = \"Meses\")\n\np2 = ggplot(\n  data = db,\n  aes(x = `Tmin`,\n      y = `mes`,\n      fill = after_stat(x))\n) +\n  geom_density_ridges_gradient(scale = 1.5, size = 0.1, \n                               rel_min_height = 0.001) +\n  scale_fill_viridis_c(name = \"Temp (ºC)\", \n                       direction = 1, option = \"H\")+\n  my_theme+\n  theme(legend.position = c(0.9, 0.77))+\n  labs(x=\"Temperatura mínima (ºC)\", y = \"Meses\")\n\n\np3 = ggplot(\n  data = db,\n  aes(x = `Tmax`,\n      y = `mes`,\n      fill = after_stat(x))\n) +\n  geom_density_ridges_gradient(scale = 1.5, size = 0.1, \n                               rel_min_height = 0.001) +\n  scale_fill_viridis_c(name = \"Temp (ºC)\", \n                       direction = 1, option = \"H\") +\n  my_theme+\n  theme(legend.position = \"\")+\n  labs(x=\"Temperatura maxima (ºC)\", y = \"Meses\")\n\narrange_ggplot((p1),\n               (p2+p3),\n               nrow = 2,\n               tag_levels = \"a\")\n\nPicking joint bandwidth of 0.801\n\n\nPicking joint bandwidth of 1.11\n\n\nPicking joint bandwidth of 1.2"
  },
  {
    "objectID": "posts/weather_mizque/index.html#plot-precipitation",
    "href": "posts/weather_mizque/index.html#plot-precipitation",
    "title": "Analisis de datos climaticos",
    "section": "plot precipitation",
    "text": "plot precipitation\n\npp =db%&gt;%\n  mutate(PP = na_if(PP, 0.0))%&gt;%\n  ggplot(aes(x = mes,y = PP,\n             colour = mes\n             )) +\n  geom_boxplot(outlier.shape = NA)+\n  geom_jitter()+\n  my_theme+\n  theme(legend.position = \"\")+\n  labs(x=\"Meses\", y = \"Precipitacion (mm)\")"
  },
  {
    "objectID": "posts/weather_mizque/index.html#humedad-relativa-plot",
    "href": "posts/weather_mizque/index.html#humedad-relativa-plot",
    "title": "Analisis de datos climaticos",
    "section": "Humedad relativa plot",
    "text": "Humedad relativa plot\n\nfechas = df%&gt;%\n  rename(any_of(newvar))%&gt;%\n  select(c(1,2,3, 8:14))%&gt;%\n  mutate_if(is.character, as.numeric)%&gt;%\n  mutate(fecha = make_datetime(gestion,mes,dia))%&gt;%\n  select(fecha,HRmin,HRmax, HRmean)\nfechas\n\n# A tibble: 485 × 4\n   fecha               HRmin HRmax HRmean\n   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 2022-01-01 00:00:00  19.3  93.2   53.4\n 2 2022-01-02 00:00:00  18.0  96.6   69.6\n 3 2022-01-03 00:00:00  27.2  88.7   58.7\n 4 2022-01-04 00:00:00  23.6  90.8   66.3\n 5 2022-01-05 00:00:00  47.8  94.6   81.0\n 6 2022-01-06 00:00:00  35.9  94.3   65.8\n 7 2022-01-07 00:00:00  39.9  95.6   67.3\n 8 2022-01-08 00:00:00  49.6  91.3   74.5\n 9 2022-01-09 00:00:00  38.5  92.7   70.6\n10 2022-01-10 00:00:00  47.2  91.9   73.0\n# ℹ 475 more rows\n\n\n\nHR = fechas%&gt;%\n  ggplot(aes(x=fecha, y=HRmean))+\n  geom_line()+\n  my_theme+\n  labs(x=\"Meses\", y = \"Humedad relativa promedio (%)\")\nHR\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\np = fechas%&gt;%\nplot_ly(x=~fecha, y=~HRmean)%&gt;%\n  add_lines()\np\n\n\narrange_ggplot((pp),\n               (p1+HR),\n               nrow = 2,\n               tag_levels = \"a\")\n\nWarning: Removed 342 rows containing non-finite values (`stat_boxplot()`).\n\n\nWarning: Removed 342 rows containing missing values (`geom_point()`).\n\n\nPicking joint bandwidth of 0.801"
  },
  {
    "objectID": "posts/agri_AI/index.html",
    "href": "posts/agri_AI/index.html",
    "title": "La inteligencia artificial revoluciona la agricultura",
    "section": "",
    "text": "La agricultura es un sector que se enfrenta a una serie de desafíos, como el cambio climático, la escasez de recursos y la creciente demanda de alimentos. La inteligencia artificial (IA) ofrece una serie de soluciones prometedoras para estos desafíos.\nLa IA se puede utilizar para analizar datos y tomar decisiones. Esto puede ayudar a los agricultores a:\n\nMejorar la detección de enfermedades y plagas: La IA se puede utilizar para analizar imágenes de cultivos para detectar signos de enfermedades o plagas. Esto permite a los agricultores tomar medidas para controlar estas amenazas antes de que causen daños significativos.\nOptimizar el riego: La IA se puede utilizar para analizar datos sobre el clima, el suelo y los cultivos para optimizar el riego. Esto puede ayudar a los agricultores a ahorrar agua y mejorar la eficiencia del riego.\nMejorar la gestión de los nutrientes: La IA se puede utilizar para analizar datos sobre el suelo y los cultivos para optimizar la aplicación de fertilizantes. Esto puede ayudar a los agricultores a ahorrar fertilizantes y mejorar la salud de los cultivos.\nAutomatizar las tareas agrícolas: La IA se puede utilizar para automatizar tareas agrícolas, como la siembra, la cosecha y el control de plagas. Esto puede ayudar a los agricultores a reducir los costos de mano de obra y mejorar la eficiencia.\n\nLa IA aún está en sus primeras etapas de desarrollo, pero tiene el potencial de revolucionar la agricultura. A medida que la IA continúe desarrollándose, se espera que tenga un impacto aún mayor en el sector agropecuario."
  },
  {
    "objectID": "posts/agri_AI/index.html#introducción",
    "href": "posts/agri_AI/index.html#introducción",
    "title": "La inteligencia artificial revoluciona la agricultura",
    "section": "",
    "text": "La agricultura es un sector que se enfrenta a una serie de desafíos, como el cambio climático, la escasez de recursos y la creciente demanda de alimentos. La inteligencia artificial (IA) ofrece una serie de soluciones prometedoras para estos desafíos.\nLa IA se puede utilizar para analizar datos y tomar decisiones. Esto puede ayudar a los agricultores a:\n\nMejorar la detección de enfermedades y plagas: La IA se puede utilizar para analizar imágenes de cultivos para detectar signos de enfermedades o plagas. Esto permite a los agricultores tomar medidas para controlar estas amenazas antes de que causen daños significativos.\nOptimizar el riego: La IA se puede utilizar para analizar datos sobre el clima, el suelo y los cultivos para optimizar el riego. Esto puede ayudar a los agricultores a ahorrar agua y mejorar la eficiencia del riego.\nMejorar la gestión de los nutrientes: La IA se puede utilizar para analizar datos sobre el suelo y los cultivos para optimizar la aplicación de fertilizantes. Esto puede ayudar a los agricultores a ahorrar fertilizantes y mejorar la salud de los cultivos.\nAutomatizar las tareas agrícolas: La IA se puede utilizar para automatizar tareas agrícolas, como la siembra, la cosecha y el control de plagas. Esto puede ayudar a los agricultores a reducir los costos de mano de obra y mejorar la eficiencia.\n\nLa IA aún está en sus primeras etapas de desarrollo, pero tiene el potencial de revolucionar la agricultura. A medida que la IA continúe desarrollándose, se espera que tenga un impacto aún mayor en el sector agropecuario."
  },
  {
    "objectID": "posts/agri_AI/index.html#ejemplos-de-aplicaciones-de-la-ia-en-la-agricultura",
    "href": "posts/agri_AI/index.html#ejemplos-de-aplicaciones-de-la-ia-en-la-agricultura",
    "title": "La inteligencia artificial revoluciona la agricultura",
    "section": "Ejemplos de aplicaciones de la IA en la agricultura",
    "text": "Ejemplos de aplicaciones de la IA en la agricultura\nAquí hay algunos ejemplos de cómo la IA se está utilizando actualmente en la agricultura:\n\nLa empresa estadounidense John Deere utiliza la IA para desarrollar tractores autónomos que pueden realizar tareas agrícolas sin la intervención humana.\nLa empresa israelí CropX utiliza la IA para analizar datos de imágenes satelitales para detectar signos de estrés en los cultivos.\nLa empresa estadounidense Blue River Technology utiliza la IA para desarrollar sistemas de riego que pueden ajustar la cantidad de agua aplicada a cada planta."
  },
  {
    "objectID": "posts/agri_AI/index.html#beneficios-de-la-ia-en-la-agricultura",
    "href": "posts/agri_AI/index.html#beneficios-de-la-ia-en-la-agricultura",
    "title": "La inteligencia artificial revoluciona la agricultura",
    "section": "Beneficios de la IA en la agricultura",
    "text": "Beneficios de la IA en la agricultura\nLa IA ofrece una serie de beneficios para la agricultura, incluyendo:\n\nMejora la productividad: La IA puede ayudar a los agricultores a obtener mayores rendimientos de sus cultivos.\nReduce los costos: La IA puede ayudar a los agricultores a reducir los costos de mano de obra, insumos y otros gastos.\nMejora la sostenibilidad: La IA puede ayudar a los agricultores a utilizar los recursos de manera más eficiente y reducir su impacto ambiental."
  },
  {
    "objectID": "posts/agri_AI/index.html#retos-de-la-ia-en-la-agricultura",
    "href": "posts/agri_AI/index.html#retos-de-la-ia-en-la-agricultura",
    "title": "La inteligencia artificial revoluciona la agricultura",
    "section": "Retos de la IA en la agricultura",
    "text": "Retos de la IA en la agricultura\nLa IA aún enfrenta algunos retos en la agricultura, incluyendo:\n\nCosto: La tecnología de IA puede ser costosa, lo que puede limitar su adopción por parte de los agricultores.\nAcceso a datos: La IA requiere grandes cantidades de datos para funcionar de manera efectiva. Los agricultores pueden tener dificultades para acceder a estos datos.\nAceptación por parte de los agricultores: Algunos agricultores pueden ser reacios a adoptar nuevas tecnologías, como la IA."
  },
  {
    "objectID": "posts/agri_AI/index.html#conclusión",
    "href": "posts/agri_AI/index.html#conclusión",
    "title": "La inteligencia artificial revoluciona la agricultura",
    "section": "Conclusión",
    "text": "Conclusión\nLa IA tiene el potencial de transformar la agricultura. A medida que la IA continúe desarrollándose y el costo de la tecnología disminuya, es probable que se adopte de manera más generalizada por los agricultores. Esto podría conducir a una agricultura más productiva, sostenible y rentable."
  },
  {
    "objectID": "posts/corrr/index.html",
    "href": "posts/corrr/index.html",
    "title": "The corrr package in R: A powerful tool for exploring correlations",
    "section": "",
    "text": "Correlations are a fundamental tool for data analysis, and they can be used to measure the strength and direction of the relationship between two variables. The corrr package in R is a powerful tool for exploring correlations, and it makes it possible to easily perform routine tasks when exploring correlation matrices, such as:\n\nIgnoring the diagonal\nFocusing on the correlations of certain variables against others\nRearranging and visualizing the matrix in terms of the strength of the correlations\n\nThe corrr package also provides a number of functions for working with correlation data frames, including:\n\nFiltering and sorting\nComputing descriptive statistics\nCreating plots"
  },
  {
    "objectID": "posts/corrr/index.html#introduction",
    "href": "posts/corrr/index.html#introduction",
    "title": "The corrr package in R: A powerful tool for exploring correlations",
    "section": "",
    "text": "Correlations are a fundamental tool for data analysis, and they can be used to measure the strength and direction of the relationship between two variables. The corrr package in R is a powerful tool for exploring correlations, and it makes it possible to easily perform routine tasks when exploring correlation matrices, such as:\n\nIgnoring the diagonal\nFocusing on the correlations of certain variables against others\nRearranging and visualizing the matrix in terms of the strength of the correlations\n\nThe corrr package also provides a number of functions for working with correlation data frames, including:\n\nFiltering and sorting\nComputing descriptive statistics\nCreating plots"
  },
  {
    "objectID": "posts/corrr/index.html#body",
    "href": "posts/corrr/index.html#body",
    "title": "The corrr package in R: A powerful tool for exploring correlations",
    "section": "Body",
    "text": "Body\nThis blog post will provide an overview of how to use the corrr package in R to explore correlations. We will cover the following topics:\n\nInstalling and loading the corrr package\nCreating a correlation data frame\nFiltering and sorting the correlation data frame\nComputing descriptive statistics for the correlation data frame\nCreating plots of the correlation data frame"
  },
  {
    "objectID": "posts/corrr/index.html#conclusion",
    "href": "posts/corrr/index.html#conclusion",
    "title": "The corrr package in R: A powerful tool for exploring correlations",
    "section": "Conclusion",
    "text": "Conclusion\nThe corrr package is a powerful tool for exploring correlations in R. It is highly recommended for anyone who works with correlation data."
  },
  {
    "objectID": "posts/corrr/index.html#example",
    "href": "posts/corrr/index.html#example",
    "title": "The corrr package in R: A powerful tool for exploring correlations",
    "section": "Example:",
    "text": "Example:\nHere is an example of how to use the corrr package to create a heatmap of the correlation matrix for the mtcars dataset:\n\n# Install the corrr package\ninstall.packages(\"corrr\")\n\n\n# Load the corrr package\nlibrary(tidyverse)\nlibrary(corrr)\n\n\nData\n\ndata(\"airquality\")\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\n\n\nCompute correlation matrix\n\nres.cor = correlate(airquality)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\nres.cor\n\n# A tibble: 6 × 7\n  term      Ozone Solar.R    Wind   Temp    Month      Day\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Ozone   NA       0.348  -0.602   0.698  0.165   -0.0132 \n2 Solar.R  0.348  NA      -0.0568  0.276 -0.0753  -0.150  \n3 Wind    -0.602  -0.0568 NA      -0.458 -0.178    0.0272 \n4 Temp     0.698   0.276  -0.458  NA      0.421   -0.131  \n5 Month    0.165  -0.0753 -0.178   0.421 NA       -0.00796\n6 Day     -0.0132 -0.150   0.0272 -0.131 -0.00796 NA      \n\n\nfashion(), this function is for pleasant correlation viewing:\n\nres.cor %&gt;%\n  fashion()\n\n     term Ozone Solar.R Wind Temp Month  Day\n1   Ozone           .35 -.60  .70   .16 -.01\n2 Solar.R   .35         -.06  .28  -.08 -.15\n3    Wind  -.60    -.06      -.46  -.18  .03\n4    Temp   .70     .28 -.46        .42 -.13\n5   Month   .16    -.08 -.18  .42       -.01\n6     Day  -.01    -.15  .03 -.13  -.01     \n\n\n\n\nCreate a correlation network\nThe R function network_plot() can be used to visualize and explore correlations.\n\nairquality %&gt;%\n  correlate() %&gt;%\n  network_plot(min_cor = 0.3)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\n\n\nThe option min_cor indicates the required minimum correlation value for a correlation to be plotted.\nEach point reprents a variable. Variable that are highly correlated are clustered together. The positioning of variables is handled by multidimensional scaling of the absolute values of the correlations.\nFor example, it can be seen from the above plot that the variables Ozone, Wind and Temp are clustering together (which makes sense).\nEach path represents a correlation between the two variables that it joins. Blue color represents a positive correlation, and a red color corresponds to a negative correlation.\nThe width and transparency of the path represent the strength of the correlation (wider and less transparent = stronger correlation).\nFor example, it can be seen that the positive correlation between Ozone and Temp is stronger than the positive correlation between Ozone and Solar.R.\n\n\nCleaning up the correlation network\nWe can clean this up by increasing the min_cor, thus plotting fewer correlation paths:\n\nmtcars %&gt;%\n  correlate() %&gt;%\n  network_plot(min_cor = .7)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'"
  },
  {
    "objectID": "posts/componentes_principales/index.html",
    "href": "posts/componentes_principales/index.html",
    "title": "Análisis de Componentes Principales",
    "section": "",
    "text": "El análisis de componentes principales (PCA) nos permite resumir y visualizar la información en un conjunto de datos que contiene individuos/observaciones descritos por múltiples variables cuantitativas inter-correlacionadas. Cada variable podría considerarse como una dimensión diferente. Si tiene más de 3 variables en sus conjuntos de datos, podría ser muy difícil visualizar un hiperespacio multidimensional.\nEl análisis de componentes principales se utiliza para extraer la información importante de una tabla de datos multivariados y para expresar esta información como un conjunto de algunas variables nuevas llamadas componentes principales. Estas nuevas variables corresponden a una combinación lineal de las originales. El número de componentes principales es menor o igual al número de variables originales.\nLa información de un conjunto de datos dado corresponde a la variación total que contiene. El objetivo de PCA es identificar direcciones (o componentes principales) a lo largo de los cuales la variación en los datos es máxima.\nEn otras palabras, PCA reduce la dimensionalidad de un dato multivariado a dos o tres componentes principales, que se pueden visualizar gráficamente, con una mínima pérdida de información.\n\nEn este capítulo, describimos la idea básica de PCA y demostramos cómo calcular y visualizar PCA usando el software R. Además, mostraremos cómo identificar las variables más importantes que explican las variaciones en un conjunto de datos.\n\nEn conjunto, el objetivo principal del análisis de componentes principales es:\n\n\nidentificar patrones ocultos en un conjunto de datos,\nreducir la dimensionalidad de los datos eliminando el ruido y la redundancia en los datos,\nidentificar variables correlacionadas"
  },
  {
    "objectID": "posts/componentes_principales/index.html#introducción",
    "href": "posts/componentes_principales/index.html#introducción",
    "title": "Análisis de Componentes Principales",
    "section": "",
    "text": "El análisis de componentes principales (PCA) nos permite resumir y visualizar la información en un conjunto de datos que contiene individuos/observaciones descritos por múltiples variables cuantitativas inter-correlacionadas. Cada variable podría considerarse como una dimensión diferente. Si tiene más de 3 variables en sus conjuntos de datos, podría ser muy difícil visualizar un hiperespacio multidimensional.\nEl análisis de componentes principales se utiliza para extraer la información importante de una tabla de datos multivariados y para expresar esta información como un conjunto de algunas variables nuevas llamadas componentes principales. Estas nuevas variables corresponden a una combinación lineal de las originales. El número de componentes principales es menor o igual al número de variables originales.\nLa información de un conjunto de datos dado corresponde a la variación total que contiene. El objetivo de PCA es identificar direcciones (o componentes principales) a lo largo de los cuales la variación en los datos es máxima.\nEn otras palabras, PCA reduce la dimensionalidad de un dato multivariado a dos o tres componentes principales, que se pueden visualizar gráficamente, con una mínima pérdida de información.\n\nEn este capítulo, describimos la idea básica de PCA y demostramos cómo calcular y visualizar PCA usando el software R. Además, mostraremos cómo identificar las variables más importantes que explican las variaciones en un conjunto de datos.\n\nEn conjunto, el objetivo principal del análisis de componentes principales es:\n\n\nidentificar patrones ocultos en un conjunto de datos,\nreducir la dimensionalidad de los datos eliminando el ruido y la redundancia en los datos,\nidentificar variables correlacionadas"
  },
  {
    "objectID": "posts/componentes_principales/index.html#metodología-de-análisis",
    "href": "posts/componentes_principales/index.html#metodología-de-análisis",
    "title": "Análisis de Componentes Principales",
    "section": "Metodología de análisis",
    "text": "Metodología de análisis\nInstale los dos paquetes de la siguiente manera:\n\ninstall.packages(c(\"FactoMineR\", \"factoextra\"))\n\nCargar paquetes en R:\n\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\n\nCargando datos\nUsaremos los conjuntos de datos de demostración decathlon2 del paquete factoextra:\n\ndata(decathlon2)\n#head(decathlon2)\n\nDe esta base de datos seleccionamos variables cuantitativas para el análisis de componentes principales:\n\n#seleccionamos 27 filas y las primeras 10 variables\ndecathlon2.active &lt;- decathlon2[1:23, 1:10]\nhead(decathlon2.active[, 1:7], 4)\n\n        X100m Long.jump Shot.put High.jump X400m X110m.hurdle Discus\nSEBRLE  11.04      7.58    14.83      2.07 49.81        14.69  43.75\nCLAY    10.76      7.40    14.26      1.86 49.37        14.05  50.72\nBERNARD 11.02      7.23    14.25      1.92 48.93        14.99  40.87\nYURKOV  11.34      7.09    15.19      2.10 50.42        15.31  46.26\n\n\n\n\nCódigo R\nSe puede utilizar la función PCA() [paquete FactoMineR]. Un formato simplificado es:\n\nPCA(X, scale.unit = TRUE, ncp = 5, graph = TRUE)\n\n\n\nX: un marco de datos. Las filas son individuos y las columnas son variables numéricas\nscale.unit: un valor lógico. Si es TRUE, los datos se escalan a la varianza de la unidad antes del análisis. Esta estandarización a la misma escala evita que algunas variables se conviertan en dominantes solo por sus grandes unidades de medida. Hace que la variable sea comparable.\nncp: número de dimensiones mantenidas en los resultados finales.\ngraph: un valor lógico. Si es TRUE, se muestra un gráfico.\n\n\nEl código R a continuación, calcula el análisis de componentes principales de las variables seleccionadas o activas:\n\nlibrary(\"FactoMineR\")\nres.pca &lt;- PCA(decathlon2.active, graph = FALSE)\n\n\nEl objeto que se crea usando la función PCA() contiene mucha información que se encuentra en muchas listas y matrices diferentes. Estos valores se describen en la siguiente sección.\n\n\n\nExtraiga y visualice valores propios/varianzas:\nLos valores propios o varianzas miden la cantidad de variación retenida por cada componente principal. Las varianzas son grandes para los primeros componentes y pequeños para las siguientes. Es decir, las primeras PC corresponden a las direcciones con la cantidad máxima de variación en el conjunto de datos.\n\n# Extraer valores propios/varianzas\nget_eig(res.pca)\n\n       eigenvalue variance.percent cumulative.variance.percent\nDim.1   4.1242133        41.242133                    41.24213\nDim.2   1.8385309        18.385309                    59.62744\nDim.3   1.2391403        12.391403                    72.01885\nDim.4   0.8194402         8.194402                    80.21325\nDim.5   0.7015528         7.015528                    87.22878\nDim.6   0.4228828         4.228828                    91.45760\nDim.7   0.3025817         3.025817                    94.48342\nDim.8   0.2744700         2.744700                    97.22812\nDim.9   0.1552169         1.552169                    98.78029\nDim.10  0.1219710         1.219710                   100.00000\n\n# Visualizar valores propios/variaciones\nfviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))\n\n\n\n\n\n\n\n\nLa suma de todos los valores propios da una varianza total de 10.\nLa proporción de variación explicada por cada valor propio se da en la segunda columna. Por ejemplo, 4,124 dividido por 10 es igual a 0,4124, o aproximadamente el 41,24% de la variación se explica por este primer valor propio. El porcentaje acumulado explicado se obtiene sumando las sucesivas proporciones de variación explicadas para obtener el total acumulado. Por ejemplo, 41,242% más 18,385% es igual a 59,627%, y así sucesivamente. Por lo tanto, aproximadamente el 59,627% de la variación se explica por los dos primeros valores propios juntos.\nLos valores propios se pueden utilizar para determinar la cantidad de componentes principales que se deben retener después de la PCA (Kaiser 1961):\n\n\nUn valor propio &gt; 1 indica que los PCs representan más varianza que la contabilizada por una de las variables originales en los datos estandarizados. Esto se usa comúnmente como un punto de corte para el cual se retienen las PCs. Esto es válido solo cuando los datos están estandarizados.\nTambién puede limitar el número de componentes a ese número que representa una cierta fracción de la varianza total. Por ejemplo, si está satisfecho con el 70% de la varianza total explicada, utilice el número de componentes para lograrlo.\n\n\nDesafortunadamente, no existe una forma objetiva bien aceptada de decidir cuántos componentes principales son suficientes. Esto dependerá del campo de aplicación específico y del conjunto de datos específico. En la práctica, tendemos a mirar los primeros componentes principales para encontrar patrones interesantes en los datos.\nEn nuestro análisis, los tres primeros componentes principales explican el 72% de la variación. Este es un porcentaje aceptablemente grande.\n\n\nGráfica círculo de correlación\nLa correlación entre una variable y un componente principal (PC) se utiliza como coordenadas de la variable en la PC. La representación de las variables difiere del gráfico de las observaciones: las observaciones están representadas por sus proyecciones, pero las variables están representadas por sus correlaciones (Abdi y Williams 2010).\nEs posible controlar los colores de las variables usando sus contribuciones (“contrib”) a los ejes principales:\n\n# Control variable colors using their contributions\nfviz_pca_var(res.pca, col.var=\"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE # Avoid text overlapping\n             )\n\n\n\n\n\n\n\n\nLa gráfica anterior también se conoce como gráficas de correlación variable. Muestra las relaciones entre todas las variables. Se puede interpretar de la siguiente manera:\n\nLas variables correlacionadas positivamente se agrupan.\nLas variables correlacionadas negativamente se colocan en lados opuestos del origen de la gráfica (cuadrantes opuestos).\nLa distancia entre las variables y el origen mide la calidad de las variables en el mapa de factores. Las variables que están lejos del origen están bien representadas en el mapa de factores.\n\n\n\nContribucion de las variables a los ejes principales:\n\n# Contributions of variables to PC1\nfviz_contrib(res.pca, choice = \"var\", axes = 1, top = 10)\n\n\n\n\n\n\n\n# Contributions of variables to PC2\nfviz_contrib(res.pca, choice = \"var\", axes = 2, top = 10)\n\n\n\n\n\n\n\n\nLa línea discontinua roja en el gráfico anterior indica la contribución promedio esperada. Si la contribución de las variables fuera uniforme, el valor esperado sería 1/longitud(variables) = 1/10 = 10%. Para un componente dado, una variable con una contribución mayor que este límite podría considerarse importante para contribuir al componente.\n\nSe puede observar que las variables X100m, Long.jump y Pole.vault contribuyen más a las dimensiones 1 y 2.\n\n\n\nBiplot\nPara hacer un biplot simple de individuos y variables, escriba esto:\n\n# Biplot of individuals and variables\nfviz_pca_biplot(res.pca, repel = TRUE)\n\n\n\n\n\n\n\n\nEn biplot, debe centrarse principalmente en la dirección de las variables pero no en sus posiciones absolutas en la gráfica.\nEn términos generales, un biplot se puede interpretar de la siguiente manera:\n\nun individuo que está en el mismo lado de una variable dada tiene un valor alto para esta variable;\nun individuo que está en el lado opuesto de una variable dada tiene un valor bajo para esta variable.\n\nNota: Este post fue extraido del libro “Multivariate Analisis II”"
  },
  {
    "objectID": "posts/componentes_principales/index.html#referencias",
    "href": "posts/componentes_principales/index.html#referencias",
    "title": "Análisis de Componentes Principales",
    "section": "Referencias",
    "text": "Referencias\nKassambara A. 2017. Multivariate Analysis: Practical Guide to Principal Component Methods in R."
  },
  {
    "objectID": "posts/change_plot/index.html",
    "href": "posts/change_plot/index.html",
    "title": "Dumbbell plot: visualizar cambio en ggplot2",
    "section": "",
    "text": "El gráfico de mancuernas es una visualización que muestra el cambio entre dos puntos de nuestros datos. Su nombre se debe a la forma de la mancuerna. Es una gran manera de mostrar los datos cambiantes entre dos puntos (piensa en el inicio y el final). Aquí podemos ver la mejora en el ahorro de combustible de los vehículos a lo largo del tiempo (entre 1999 y 2008). La mancuerna muestra el punto inicial (MPG en 1999) y el punto final (MPG en 2008)."
  },
  {
    "objectID": "posts/change_plot/index.html#introducción",
    "href": "posts/change_plot/index.html#introducción",
    "title": "Dumbbell plot: visualizar cambio en ggplot2",
    "section": "",
    "text": "El gráfico de mancuernas es una visualización que muestra el cambio entre dos puntos de nuestros datos. Su nombre se debe a la forma de la mancuerna. Es una gran manera de mostrar los datos cambiantes entre dos puntos (piensa en el inicio y el final). Aquí podemos ver la mejora en el ahorro de combustible de los vehículos a lo largo del tiempo (entre 1999 y 2008). La mancuerna muestra el punto inicial (MPG en 1999) y el punto final (MPG en 2008)."
  },
  {
    "objectID": "posts/change_plot/index.html#cargar-librerias-y-datos",
    "href": "posts/change_plot/index.html#cargar-librerias-y-datos",
    "title": "Dumbbell plot: visualizar cambio en ggplot2",
    "section": "Cargar librerias y datos",
    "text": "Cargar librerias y datos\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(ggalt)\n\n# Data\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows"
  },
  {
    "objectID": "posts/change_plot/index.html#paso-1-preparacion-de-datos",
    "href": "posts/change_plot/index.html#paso-1-preparacion-de-datos",
    "title": "Dumbbell plot: visualizar cambio en ggplot2",
    "section": "Paso 1: Preparacion de datos",
    "text": "Paso 1: Preparacion de datos\n\nmpg_by_year_tbl = mpg %&gt;%\n  select(hwy, year, model, class)%&gt;%\n  pivot_wider(\n    names_from = year,\n    values_from = hwy,\n    id_cols = c(class, model),\n    values_fn = function(x) mean(x, na.rm = TRUE),\n    names_prefix = \"year_\"\n  )%&gt;%\n  mutate(model = fct_reorder(model, year_2008))%&gt;%\n  drop_na()"
  },
  {
    "objectID": "posts/change_plot/index.html#paso-2-dumbbell-plot-base",
    "href": "posts/change_plot/index.html#paso-2-dumbbell-plot-base",
    "title": "Dumbbell plot: visualizar cambio en ggplot2",
    "section": "Paso 2: Dumbbell plot base",
    "text": "Paso 2: Dumbbell plot base\n\n# basic plot with ggalt\ng1 = mpg_by_year_tbl%&gt;%\n  ggplot(aes(x = year_1999, xend = year_2008,\n             y = model, group = model))+\n  geom_dumbbell(\n    colour=\"#a3c4dc\",\n    colour_xend = \"#0e668b\",\n    size=2.5,\n    dot_guide = TRUE,\n    dot_guide_size = 0.19,\n    dot_guide_colour = \"grey60\"\n  )\n\ng1"
  },
  {
    "objectID": "posts/change_plot/index.html#paso-3-configurar-con-ggplot",
    "href": "posts/change_plot/index.html#paso-3-configurar-con-ggplot",
    "title": "Dumbbell plot: visualizar cambio en ggplot2",
    "section": "Paso 3: Configurar con ggplot",
    "text": "Paso 3: Configurar con ggplot\n\ng2 = g1 +\n  labs(\n    title = \"Cambio de vehiculos entre 1999 y 2008\",\n    x=\"Fuel Economy (MPG)\", y=\"Vehicle Model\"\n)+\n  theme_tq()+\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.major.x = element_line(),\n    axis.ticks = element_blank(),\n    panel.border = element_blank()\n  )\n\ng2"
  },
  {
    "objectID": "posts/PB_AI/index.html",
    "href": "posts/PB_AI/index.html",
    "title": "Exploring the Frontier: Artificial Intelligence in Molecular Plant Breeding Data Science",
    "section": "",
    "text": "In the realm of molecular plant breeding, where the intricate dance of genes and environment shapes the future of agriculture, the integration of Artificial Intelligence (AI) has emerged as a transformative force. As we stand at the intersection of biology and technology, the marriage of advanced methodologies with AI presents unprecedented opportunities to revolutionize how we understand, manipulate, and optimize plant genomes for improved traits and yield."
  },
  {
    "objectID": "posts/PB_AI/index.html#introduction",
    "href": "posts/PB_AI/index.html#introduction",
    "title": "Exploring the Frontier: Artificial Intelligence in Molecular Plant Breeding Data Science",
    "section": "",
    "text": "In the realm of molecular plant breeding, where the intricate dance of genes and environment shapes the future of agriculture, the integration of Artificial Intelligence (AI) has emerged as a transformative force. As we stand at the intersection of biology and technology, the marriage of advanced methodologies with AI presents unprecedented opportunities to revolutionize how we understand, manipulate, and optimize plant genomes for improved traits and yield."
  },
  {
    "objectID": "posts/PB_AI/index.html#advanced-methodologies",
    "href": "posts/PB_AI/index.html#advanced-methodologies",
    "title": "Exploring the Frontier: Artificial Intelligence in Molecular Plant Breeding Data Science",
    "section": "Advanced Methodologies",
    "text": "Advanced Methodologies\nIn recent years, advanced methodologies fueled by AI have catalyzed groundbreaking progress in molecular plant breeding:\n\nGenomic Selection (GS): AI algorithms have revolutionized GS by enabling the prediction of plant traits based on genomic information. Through machine learning models trained on vast genomic datasets coupled with phenotypic data, breeders can now identify promising plant varieties much earlier in the breeding cycle, accelerating the development of high-yielding, stress-tolerant crops.\nPhenomics and High-Throughput Phenotyping: The integration of AI-powered image analysis and sensor technologies has empowered researchers to capture detailed phenotypic data on an unprecedented scale. From leaf morphology to root architecture, AI algorithms can extract valuable insights from complex phenotypic datasets, facilitating the identification of key traits for crop improvement with remarkable precision and efficiency.\nGenome Editing and CRISPR-Cas9: AI-driven algorithms play a pivotal role in guiding the design and optimization of CRISPR-Cas9 gene editing systems. Through computational modeling and machine learning techniques, researchers can predict the outcomes of gene edits with greater accuracy, enabling targeted modifications of plant genomes to enhance traits such as disease resistance, nutritional content, and environmental adaptability.\nData Integration and Multidimensional Analysis: With the proliferation of omics technologies, molecular plant breeding generates vast amounts of heterogeneous data encompassing genomics, transcriptomics, metabolomics, and beyond. AI-driven approaches, including deep learning and network analysis, enable the integration of diverse omics datasets, unveiling intricate molecular networks and uncovering novel gene interactions underlying complex traits."
  },
  {
    "objectID": "posts/PB_AI/index.html#future-perspectives",
    "href": "posts/PB_AI/index.html#future-perspectives",
    "title": "Exploring the Frontier: Artificial Intelligence in Molecular Plant Breeding Data Science",
    "section": "Future Perspectives",
    "text": "Future Perspectives\nLooking ahead, the future of AI in molecular plant breeding data science holds immense promise and potential:\n\nPrecision Breeding and Trait Stacking: AI algorithms will continue to refine our ability to precisely manipulate plant genomes, facilitating the targeted introduction and stacking of desirable traits with unprecedented accuracy and efficiency. From drought tolerance to nutrient efficiency, AI-driven breeding strategies will enable the development of tailored crop varieties optimized for diverse environmental conditions and agricultural practices.\nHarnessing Genetic Diversity: AI-powered analytics will unlock the full potential of genetic diversity harbored within plant germplasm collections, empowering breeders to explore and exploit novel alleles and genetic variants for crop improvement. By leveraging machine learning algorithms to dissect complex genotype-phenotype relationships, researchers can uncover hidden genetic treasures that hold the key to unlocking yield gains and resilience in the face of emerging biotic and abiotic stresses.\nSustainable Agriculture and Climate Resilience: AI-driven approaches will play a pivotal role in addressing the urgent challenges posed by climate change and global food security. By deciphering the complex interactions between genotype, phenotype, and environment, AI models can guide the development of climate-resilient crop varieties tailored to thrive in changing climatic conditions while minimizing environmental impact and resource inputs.\n\nIn conclusion, the integration of Artificial Intelligence into molecular plant breeding data science heralds a new era of innovation and possibility. By harnessing the power of AI-driven methodologies, we are poised to unlock the full potential of plant genomes, ushering in a greener, more sustainable agricultural future for generations to come. As we embark on this transformative journey, let us embrace the boundless opportunities that AI affords us to shape a world where crops are not just plants, but living solutions to the grand challenges of our time."
  },
  {
    "objectID": "posts/maps/index.html",
    "href": "posts/maps/index.html",
    "title": "Mapas de Bolivia",
    "section": "",
    "text": "El paquete rnaturalearth es una herramienta excelente para mantener y facilitar la interacción con los datos cartográficos de Natural Earth. Para producir mapas con ggplot2, se necesitan los siguientes paquetes.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rnaturalearth)\n\n\n# America del sur\nsam = ne_countries(continent = \"south america\",\n                   returnclass = \"sf\",\n                   scale = 50)\np1 = ggplot() +\n  geom_sf(data = sam, fill = \"white\") +\n  theme_light() +\n  xlim(c(-90, -35))\n\n# Imagen de Bolivia y destacar La Paz\n\nbolivia = ne_states(country = \"bolivia\", returnclass = \"sf\") %&gt;%\n  mutate(scat = ifelse(postal == \"LP\", \"La Paz\", \"Otros\"))\n\np2 = \n  p1 +\n  geom_sf(data = bolivia, aes(fill = scat))\np2"
  },
  {
    "objectID": "posts/maps/index.html#introduccion",
    "href": "posts/maps/index.html#introduccion",
    "title": "Mapas de Bolivia",
    "section": "",
    "text": "El paquete rnaturalearth es una herramienta excelente para mantener y facilitar la interacción con los datos cartográficos de Natural Earth. Para producir mapas con ggplot2, se necesitan los siguientes paquetes.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rnaturalearth)\n\n\n# America del sur\nsam = ne_countries(continent = \"south america\",\n                   returnclass = \"sf\",\n                   scale = 50)\np1 = ggplot() +\n  geom_sf(data = sam, fill = \"white\") +\n  theme_light() +\n  xlim(c(-90, -35))\n\n# Imagen de Bolivia y destacar La Paz\n\nbolivia = ne_states(country = \"bolivia\", returnclass = \"sf\") %&gt;%\n  mutate(scat = ifelse(postal == \"LP\", \"La Paz\", \"Otros\"))\n\np2 = \n  p1 +\n  geom_sf(data = bolivia, aes(fill = scat))\np2"
  },
  {
    "objectID": "posts/introbayes/index.html",
    "href": "posts/introbayes/index.html",
    "title": "Explorando el Mundo de la Estadística: Frecuentista vs. Bayesiana",
    "section": "",
    "text": "La estadística es el arte y la ciencia de extraer conocimiento a partir de datos, y dos enfoques principales han moldeado el campo a lo largo del tiempo: el frecuentista y el bayesiano. En este viaje, exploraremos las profundidades de la estadística bayesiana, comparándola con su contraparte frecuentista y destacando las características distintivas de cada enfoque."
  },
  {
    "objectID": "posts/introbayes/index.html#introducción",
    "href": "posts/introbayes/index.html#introducción",
    "title": "Explorando el Mundo de la Estadística: Frecuentista vs. Bayesiana",
    "section": "",
    "text": "La estadística es el arte y la ciencia de extraer conocimiento a partir de datos, y dos enfoques principales han moldeado el campo a lo largo del tiempo: el frecuentista y el bayesiano. En este viaje, exploraremos las profundidades de la estadística bayesiana, comparándola con su contraparte frecuentista y destacando las características distintivas de cada enfoque."
  },
  {
    "objectID": "posts/introbayes/index.html#estadística-bayesiana-más-que-probabilidades-creencias-en-acción",
    "href": "posts/introbayes/index.html#estadística-bayesiana-más-que-probabilidades-creencias-en-acción",
    "title": "Explorando el Mundo de la Estadística: Frecuentista vs. Bayesiana",
    "section": "Estadística Bayesiana: Más que Probabilidades, Creencias en Acción",
    "text": "Estadística Bayesiana: Más que Probabilidades, Creencias en Acción\nLa estadística bayesiana se basa en el teorema de Bayes, una herramienta poderosa que nos permite actualizar nuestras creencias a medida que nuevos datos emergen. En este enfoque, la probabilidad se interpreta como una medida subjetiva de creencia, lo que significa que incorporamos nuestro conocimiento previo (a priori) al análisis. Esta flexibilidad es clave, ya que nos permite abordar problemas con muestras pequeñas y aprovechar la información previa de manera efectiva.\nEl Teorema de Bayes en Acción:\n\\[\nP(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n\\] donde:\n\n\\(P(H∣E)\\) es la probabilidad posterior de la hipótesis \\(H\\) dado el conjunto de evidencia \\(E\\).\n\\(P(E∣H)\\) es la verosimilitud de la evidencia \\(E\\) bajo la hipótesis \\(H\\).\n\\(P(H)\\) es la probabilidad a priori de la hipótesis \\(H\\).\n\\(P(E)\\) es la probabilidad marginal de la evidencia \\(E\\).\n\nEste simple pero poderoso teorema nos guía en la actualización de nuestras creencias sobre una hipótesis \\(H\\) dado un conjunto de evidencia \\(E\\). Ahora, comparemos esto con el enfoque frecuentista."
  },
  {
    "objectID": "posts/introbayes/index.html#estadística-frecuentista-frente-a-la-incertidumbre-con-la-variabilidad-de-los-datos",
    "href": "posts/introbayes/index.html#estadística-frecuentista-frente-a-la-incertidumbre-con-la-variabilidad-de-los-datos",
    "title": "Explorando el Mundo de la Estadística: Frecuentista vs. Bayesiana",
    "section": "Estadística Frecuentista: Frente a la Incertidumbre con la Variabilidad de los Datos",
    "text": "Estadística Frecuentista: Frente a la Incertidumbre con la Variabilidad de los Datos\nLos estadísticos frecuentistas se centran en la frecuencia relativa de los eventos observados a lo largo del tiempo. La incertidumbre se maneja reconociendo la variabilidad inherente de los datos. Este enfoque es robusto y ha sido la columna vertebral de la estadística tradicional durante décadas.\nEstimaciones Puntuales y Pruebas de Hipótesis:\nLos frecuentistas tienden a buscar estimaciones puntuales y realizar pruebas de hipótesis para tomar decisiones. Sin embargo, este enfoque puede volverse limitado cuando se trata de manejar la incertidumbre de manera más integral."
  },
  {
    "objectID": "posts/introbayes/index.html#dónde-reside-la-diferencia",
    "href": "posts/introbayes/index.html#dónde-reside-la-diferencia",
    "title": "Explorando el Mundo de la Estadística: Frecuentista vs. Bayesiana",
    "section": "¿Dónde Reside la Diferencia?",
    "text": "¿Dónde Reside la Diferencia?\n\n1. Incorporación de Información Previa\nBayesiana: La información a priori se considera esencial. Antes de analizar los datos, se incorpora el conocimiento existente, lo que es especialmente útil en escenarios con muestras pequeñas.\nFrecuentista: No se utiliza información a priori. La inferencia se basa únicamente en los datos observados sin tener en cuenta conocimientos previos.\n\n\n2. Tamaño de la Muestra y Flexibilidad\nBayesiana: Adaptable a tamaños de muestra variables, lo que permite realizar inferencias incluso con conjuntos de datos más pequeños.\nFrecuentista: A menudo requiere grandes muestras para producir estimaciones precisas y realizar pruebas de hipótesis robustas.\n\n\n3. Enfoque para la Toma de Decisiones\nBayesiana: Ofrece distribuciones de probabilidad completas, permitiendo una toma de decisiones más informada bajo diversas condiciones de incertidumbre.\nFrecuentista: Se centra en estimaciones puntuales y pruebas de hipótesis, lo que podría no proporcionar una imagen completa de la incertidumbre."
  },
  {
    "objectID": "posts/introbayes/index.html#conclusion",
    "href": "posts/introbayes/index.html#conclusion",
    "title": "Explorando el Mundo de la Estadística: Frecuentista vs. Bayesiana",
    "section": "Conclusion",
    "text": "Conclusion\nLa estadística bayesiana ofrece una perspectiva moderna y flexible para el análisis de datos, permitiendo a los investigadores incorporar información previa de manera sistemática y actualizar sus creencias a medida que se obtiene nueva evidencia. Si bien tanto los métodos frecuentistas como los bayesianos tienen sus propias fortalezas y limitaciones, la elección entre ellos a menudo depende de la naturaleza de los datos y las preguntas de investigación específicas.\nEn resumen, la estadística bayesiana nos invita a pensar en la probabilidad y la incertidumbre de una manera diferente, abriendo nuevas posibilidades para la exploración y el descubrimiento en el mundo de la estadística y la ciencia de datos."
  },
  {
    "objectID": "posts/Analisis_correspondencia/index.html",
    "href": "posts/Analisis_correspondencia/index.html",
    "title": "Análisis de correspondencia con R",
    "section": "",
    "text": "El análisis de correspondencia (CA) es una extensión del análisis de componentes principales (Capítulo 4) adecuado para explorar relaciones entre variables cualitativas (o datos categóricos). Al igual que el análisis de componentes principales, proporciona una solución para resumir y visualizar conjuntos de datos en gráficos de dos dimensiones.\nAquí, describimos el análisis de correspondencia simple, que se utiliza para analizar frecuencias formadas por dos datos categóricos, una tabla de datos conocida como tabla de contingencia, tambien se conoce como tablas cruzadas. Proporciona puntuaciones de factores (coordenadas) para los puntos de fila y columna de la tabla de contingencia. Estas coordenadas se utilizan para visualizar gráficamente la asociación entre elementos de fila y columna en la tabla de contingencia.\nAl analizar una tabla de contingencia bidireccional, una pregunta típica es si ciertos elementos de fila están asociados con algunos elementos de elementos de columna. El análisis de correspondencia es un enfoque geométrico para visualizar las filas y columnas de una tabla de contingencia bidireccional como puntos en un espacio de baja dimensión, de modo que las posiciones de los puntos de fila y columna sean consistentes con sus asociaciones en la tabla. El objetivo es tener una visión global de los datos que sea útil para la interpretación.\nEn el capítulo actual, mostraremos cómo calcular e interpretar el análisis de correspondencia usando dos paquetes R: i) FactoMineR para el análisis y ii) factoextra para la visualización de datos. Además, mostraremos cómo revelar las variables más importantes que explican las variaciones en un conjunto de datos."
  },
  {
    "objectID": "posts/Analisis_correspondencia/index.html#introducción",
    "href": "posts/Analisis_correspondencia/index.html#introducción",
    "title": "Análisis de correspondencia con R",
    "section": "",
    "text": "El análisis de correspondencia (CA) es una extensión del análisis de componentes principales (Capítulo 4) adecuado para explorar relaciones entre variables cualitativas (o datos categóricos). Al igual que el análisis de componentes principales, proporciona una solución para resumir y visualizar conjuntos de datos en gráficos de dos dimensiones.\nAquí, describimos el análisis de correspondencia simple, que se utiliza para analizar frecuencias formadas por dos datos categóricos, una tabla de datos conocida como tabla de contingencia, tambien se conoce como tablas cruzadas. Proporciona puntuaciones de factores (coordenadas) para los puntos de fila y columna de la tabla de contingencia. Estas coordenadas se utilizan para visualizar gráficamente la asociación entre elementos de fila y columna en la tabla de contingencia.\nAl analizar una tabla de contingencia bidireccional, una pregunta típica es si ciertos elementos de fila están asociados con algunos elementos de elementos de columna. El análisis de correspondencia es un enfoque geométrico para visualizar las filas y columnas de una tabla de contingencia bidireccional como puntos en un espacio de baja dimensión, de modo que las posiciones de los puntos de fila y columna sean consistentes con sus asociaciones en la tabla. El objetivo es tener una visión global de los datos que sea útil para la interpretación.\nEn el capítulo actual, mostraremos cómo calcular e interpretar el análisis de correspondencia usando dos paquetes R: i) FactoMineR para el análisis y ii) factoextra para la visualización de datos. Además, mostraremos cómo revelar las variables más importantes que explican las variaciones en un conjunto de datos."
  },
  {
    "objectID": "posts/Analisis_correspondencia/index.html#procedimiento-computacional-en-r",
    "href": "posts/Analisis_correspondencia/index.html#procedimiento-computacional-en-r",
    "title": "Análisis de correspondencia con R",
    "section": "Procedimiento computacional en R",
    "text": "Procedimiento computacional en R\nCargar paquetes\n\nlibrary(FactoMineR)\nlibrary(factoextra)\n\n\nFormato de datos\nLos datos deben ser una tabla de contingencia (resultados de tablas cruzadas). Usaremos los conjuntos de datos de demostración de tareas domésticas disponibles en el paquete factoextra R.\n\ndata(housetasks)\n\nLos datos son una tabla de contingencia que contiene 13 tareas del hogar y su reparto en la pareja:\n\nlas filas son las diferentes tareas\nLos valores son las frecuencias de las tareas realizadas:\n\nsolo por la esposa “wife only”\nalternativamente “Alternating”\nsolo por el marido “husband only”\no conjuntamente “jointly\n\n\nLos datos se ilustran en la siguiente salida:\n\n\n           Wife Alternating Husband Jointly\nLaundry     156          14       2       4\nMain_meal   124          20       5       4\nDinner       77          11       7      13\nBreakfeast   82          36      15       7\nTidying      53          11       1      57\nDishes       32          24       4      53\nShopping     33          23       9      55\nOfficial     12          46      23      15\nDriving      10          51      75       3\nFinances     13          13      21      66\nInsurance     8           1      53      77\nRepairs       0           3     160       2\nHolidays      0           1       6     153\n\n\n\n\nGráfica de tablas de contingencia y prueba de chi-cuadrado\nLa tabla de contingencia anterior no es muy grande. Por lo tanto, es fácil inspeccionar e interpretar visualmente los perfiles de filas y columnas:\n\nEs evidente que las tareas de la casa (lavandería, comida principal y cena) las realiza con más frecuencia la “esposa”.\nLas reparaciones y la conducción las realiza predominantemente el marido.\nLos días festivos se asocian con frecuencia con la columna “conjuntamente”\n\nEl análisis de datos exploratorios y la visualización de tablas de contingencia se cubrieron en nuestro artículo anterior: Prueba de independencia de chi-cuadrado en R. Brevemente, la tabla de contingencia se puede visualizar utilizando las funciones balloonplot() [paquete gplots] y mosaicplot() [paquete graphics]:\n\nlibrary(gplots)\n#1. Convierte el dato como una tabla\ndt &lt;- as.table(as.matrix(housetasks))\n# 2. Graph\nballoonplot(t(dt), main =\"housetasks\", xlab =\"\", ylab=\"\",\n            label = FALSE, show.margins = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTenga en cuenta que las sumas de filas y columnas se imprimen de forma predeterminada en los márgenes inferior y derecho, respectivamente. Estos valores están ocultos, en el gráfico anterior, utilizando el argumento show.margins = FALSE.\n\n\nPara una tabla de contingencia pequeña, puede utilizar la prueba de chi-cuadrado para evaluar si existe una dependencia significativa entre las categorías de filas y columnas:\n\nchisq &lt;- chisq.test(housetasks)\nchisq\n\n\n    Pearson's Chi-squared test\n\ndata:  housetasks\nX-squared = 1944.5, df = 36, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\nTip\n\n\n\nEn nuestro ejemplo, las variables de fila y columna están asociadas de manera estadísticamente significativa (valor p = r chisq$p.value).\n\n\n\n\nCódigo R para calcular CA (Analisis de Correspondencia)\nSe puede utilizar la función CA() [paquete FactoMiner]. Un formato simplificado es:\n\nCA(X, ncp = 5, graph = TRUE)\n\n\n\nX: es un marco de datos (tabla de contingencia)\nncp: número de dimensiones mantenidas en los resultados finales.\ngraph: un valor lógico. Si es TRUE, se muestra un gráfico.\n\n\nPara calcular el análisis de correspondencia, escriba esto:\n\nlibrary(\"FactoMineR\")\n#housetasks es nuestro marco de datos\nres.ca &lt;- CA(housetasks, graph = FALSE)\n\nLa salida de la función CA() es una lista que incluye:\n\nprint(res.ca)\n\n**Results of the Correspondence Analysis (CA)**\nThe row variable has  13  categories; the column variable has 4 categories\nThe chi square of independence between the two variables is equal to 1944.456 (p-value =  0 ).\n*The results are available in the following objects:\n\n   name              description                   \n1  \"$eig\"            \"eigenvalues\"                 \n2  \"$col\"            \"results for the columns\"     \n3  \"$col$coord\"      \"coord. for the columns\"      \n4  \"$col$cos2\"       \"cos2 for the columns\"        \n5  \"$col$contrib\"    \"contributions of the columns\"\n6  \"$row\"            \"results for the rows\"        \n7  \"$row$coord\"      \"coord. for the rows\"         \n8  \"$row$cos2\"       \"cos2 for the rows\"           \n9  \"$row$contrib\"    \"contributions of the rows\"   \n10 \"$call\"           \"summary called parameters\"   \n11 \"$call$marge.col\" \"weights of the columns\"      \n12 \"$call$marge.row\" \"weights of the rows\"         \n\n\n\nEl objeto que se crea usando la función CA() contiene mucha información que se encuentra en muchas listas y matrices diferentes. Estos valores se describen en la siguiente sección."
  },
  {
    "objectID": "posts/Analisis_correspondencia/index.html#visualización-e-interpretación",
    "href": "posts/Analisis_correspondencia/index.html#visualización-e-interpretación",
    "title": "Análisis de correspondencia con R",
    "section": "Visualización e interpretación",
    "text": "Visualización e interpretación\nUsaremos las siguientes funciones en factoextra para ayudar en la interpretación y visualización del análisis de correspondencia:\n\nget_eigenvalue(res.ca): Extraiga los autovalores/varianzas retenidos por cada dimensión (eje)\nfviz_eig(res.ca): Visualiza los valores propios\nget_ca_row(res.ca), get_ca_col(res.ca): Extrae los resultados para filas y columnas, respectivamente.\nfviz_ca_row(res.ca), fviz_ca_col(res.ca): Visualiza los resultados para filas y columnas, respectivamente.\nfviz_ca_biplot(res.ca): Crea un biplot de filas y columnas.\n\nEn las siguientes secciones, ilustraremos cada una de estas funciones.\n\nSignificación estadística\nPara interpretar el análisis de correspondencia, el primer paso es evaluar si existe una dependencia significativa entre las filas y las columnas.\nUn método riguroso consiste en utilizar la estadística de chi-cuadrado para examinar la asociación entre las variables de fila y columna. Esto aparece en la parte superior y es generado por la función summary(res.ca) o print(res.ca), ver sección “Gráfica de tablas de contingencia y prueba de chi-cuadrado”. Una estadística de chi-cuadrado alta significa un vínculo fuerte entre las variables de fila y columna o sea, la probabilidad de chi cuadrado deberá ser inferior a 0.05.\n\nEn nuestro ejemplo, la asociación es muy significativa (chi-cuadrado: 1944,456, p = 0).\n\n\n# Chi-square statistics\nchi2 &lt;- 1944.456\nchi2\n\n[1] 1944.456\n\n# Degree of freedom\ndf &lt;- (nrow(housetasks) - 1) * (ncol(housetasks) - 1)\ndf\n\n[1] 36\n\n# P-value\npval &lt;- pchisq(chi2, df = df, lower.tail = FALSE)\npval\n\n[1] 0\n\n\n\n\nValores propios/Varianzas\nRecuerde que examinamos los valores propios para determinar el número de ejes a considerar. Los valores propios y la proporción de varianzas retenidas por los diferentes ejes se pueden extraer utilizando la función get_eigenvalue() [paquete factoextra]. Los valores propios son grandes para el primer eje y pequeños para el eje siguiente.\n\nlibrary(\"factoextra\")\neig.val &lt;- get_eigenvalue(res.ca)\neig.val\n\n      eigenvalue variance.percent cumulative.variance.percent\nDim.1  0.5428893         48.69222                    48.69222\nDim.2  0.4450028         39.91269                    88.60491\nDim.3  0.1270484         11.39509                   100.00000\n\n\nLos valores propios corresponden a la cantidad de información retenida por cada eje. Las dimensiones se ordenan de forma decreciente y se enumeran de acuerdo con la cantidad de variación explicada en la solución. La dimensión 1 explica la mayor variación en la solución, seguida de la dimensión 2 y así sucesivamente.\nEl porcentaje acumulado explicado se obtiene sumando las sucesivas proporciones de variación explicadas para obtener el total acumulado. Por ejemplo, 48,69% más 39,91% es igual a 88,6% y así sucesivamente. Por lo tanto, aproximadamente el 88,6% de la variación se explica por las dos primeras dimensiones.\nLos valores propios se pueden utilizar para determinar el número de ejes que se deben retener. No existe una “regla general” para elegir el número de dimensiones que se deben mantener para la interpretación de los datos. Depende de la pregunta de investigación y la necesidad del investigador. Por ejemplo, si está satisfecho con el 80% de las variaciones totales explicadas, utilice la cantidad de dimensiones necesarias para lograrlo.\n\n\n\n\n\n\nTenga en cuenta\n\n\n\nSe logra una buena reducción de dimensión cuando las primeras dimensiones representan una gran proporción de la variabilidad.\n\n\nEn nuestro análisis, los dos primeros ejes explican el 88,6% de la variación. Este es un porcentaje aceptablemente grande.\nUn método alternativo para determinar el número de dimensiones es mirar una gráfica de Scree Plot, que es el diagrama de valores propios/varianzas ordenados de mayor a menor. El número de componentes se determina en el punto, más allá del cual los valores propios restantes son todos relativamente pequeños y de tamaño comparable.\nEl gráfico Scree plot se puede realizar usando la función fviz_eig() o fviz_screeplot() del paquete factoextra.\n\nfviz_screeplot(res.ca, addlabels = TRUE, ylim = c(0, 50))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsideración\n\n\n\nSe puede considerar que el punto en el que el gráfico de pedregal muestra una curvatura (denominado “codo”) indica una dimensionalidad óptima.\n\n\nTambién es posible calcular un valor propio promedio por encima del cual el eje debe mantenerse en la solución.\n\n\nNuestros datos contienen 13 filas y 4 columnas.\nSi los datos fueran aleatorios, el valor esperado del valor propio para cada eje sería 1/(nrow(housetasks)-1) = 1/12 = 8,33% en términos de filas.\nAsimismo, el eje promedio debe representar 1/(ncol(housetasks)-1) = 1/3 = 33,33% en términos de las 4 columnas.\n\n\n\n\n\n\n\n\nSegún (M. T. Bendixen 1995):\n\n\n\nCualquier eje con una contribución mayor que el máximo de estos dos porcentajes debe ser considerado como importante e incluido en la solución para la interpretación de los datos.\n\n\nEl código R a continuación, dibuja el gráfico de pantalla con una línea discontinua roja que especifica el valor propio promedio:\n\nfviz_screeplot(res.ca) +\n  geom_hline(yintercept=33.33, linetype=2, color=\"red\")\n\n\n\n\n\n\n\n\nSegún el gráfico anterior, solo las dimensiones 1 y 2 deben usarse en la solución. La dimensión 3 explica sólo el 11,4% de la inercia total, que está por debajo del valor propio medio (33,33%) y es demasiado poco para guardarlo para un análisis más detallado.\n\nTenga en cuenta que puede utilizar más de 2 dimensiones. Sin embargo, es poco probable que las dimensiones complementarias contribuyan de manera significativa a la interpretación de la naturaleza de la asociación entre filas y columnas.\n\nLas dimensiones 1 y 2 explican aproximadamente el 48,7% y el 39,9% de la inercia total, respectivamente. Esto corresponde a un total acumulado del 88,6% de la inercia total retenida por las 2 dimensiones. Cuanto mayor sea la retención, más sutileza en los datos originales se retiene en la solución de baja dimensión (M. Bendixen 2003).\n\n\nBiplot\nLa función fviz_ca_biplot() del paquete factoextra se puede utilizar para dibujar el biplot de las variables de filas y columnas.\n\n# repel= TRUE para evitar la superposición de texto (lenta si tiene muchos puntos)\nfviz_ca_biplot(res.ca, repel = TRUE)\n\n\n\n\n\n\n\n\nEl gráfico anterior se llama gráfico simétrico y muestra un patrón global dentro de los datos. Las filas están representadas por puntos azules y las columnas por triángulos rojos.\nLa distancia entre cualquier punto de fila o columna da una medida de su similitud (o disimilitud). Los puntos de fila con un perfil similar se cierran en el mapa de factores. Lo mismo es válido para los puntos de columna.\n\n\n\n\n\n\nEste gráfico muestra que:\n\n\n\n\nLas tareas de la casa, como la cena, el desayuno y la ropa, las hace la esposa con más frecuencia.\nLa conducción y las reparaciones las realiza el marido.\nEl gráfico simétrico representa los perfiles de fila y columna simultáneamente en un espacio común. En este caso, solo se puede interpretar realmente la distancia entre puntos de fila o la distancia entre puntos de columna.\n¡La distancia entre cualquier elemento de fila y columna no es significativa! Solo puede hacer declaraciones generales sobre el patrón observado.\nPara interpretar la distancia entre los puntos de columna y fila, los perfiles de columna deben presentarse en el espacio de fila o viceversa. Este tipo de mapa se llama biplot asimétrico y se analiza al final de este artículo.\n\n\n\nEl siguiente paso para la interpretación es determinar qué variables de fila y columna contribuyen más en la definición de las diferentes dimensiones retenidas en el modelo.\n\n\nOpciones de biplot\nBiplot es una visualización gráfica de filas y columnas en 2 o 3 dimensiones. Ya hemos descrito cómo crear CA biplots en la sección anterior. Aquí, describiremos diferentes tipos de biplots de CA.\n\nBiplot simétrico\nComo se mencionó anteriormente, el gráfico estándar del análisis de correspondencia es un biplot simétrico en el que tanto las filas (puntos azules) como las columnas (triángulos rojos) se representan en el mismo espacio utilizando las coordenadas principales. Estas coordenadas representan los perfiles de fila y columna. En este caso, solo se puede interpretar realmente la distancia entre puntos de fila o la distancia entre puntos de columna.\n\n\n\n\n\n\nTip\n\n\n\nCon la gráfica simétrica, la distancia entre filas y columnas no se puede interpretar. Solo se pueden hacer declaraciones generales sobre el patrón.\n\n\n\nfviz_ca_biplot(res.ca, repel = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTenga en cuenta que, para interpretar la distancia entre los puntos de la columna y los puntos de la fila, la forma más sencilla es hacer una gráfica asimétrica. Esto significa que los perfiles de columna deben presentarse en el espacio de la fila o viceversa.\n\n\n\n\nBiplot asimétrico\nPara hacer un biplot asimétrico, los puntos de las filas (o columnas) se trazan a partir de las coordenadas estándar (S) y los perfiles de las columnas (o las filas) se trazan a partir de las coordenadas principales (P) (M. Bendixen 2003).\nPara un eje dado, las coordenadas estándar y principal se relacionan de la siguiente manera:\nP = sqrt(valor propio) X S\n\nP : la coordenada principal de una fila (o una columna) en el eje\nvalor propio : el valor propio del eje\n\nDependiendo de la situación, se pueden configurar otros tipos de visualización utilizando el mapa de argumentos (Nenadic y Greenacre 2007) en la función fviz_ca_biplot() [in factoextra].\nLas opciones permitidas para el mapa de argumentos son:\n\n“rowprincipal” o “colprincipal”: estos son los llamados biplots asimétricos, con filas en coordenadas principales y columnas en coordenadas estándar, o viceversa (también conocido como preservar métricas de filas o preservar métricas de columnas, respectivamente).\n\n“rowprincipal”: las columnas se representan en el espacio de las filas\n“colprincipal”: las filas se representan en el espacio de la columna\n\n“symbiplot”: tanto las filas como las columnas se escalan para tener varianzas iguales a los valores singulares (raíces cuadradas de los valores propios), lo que da un biplot simétrico pero no conserva las métricas de filas o columnas.\n“rowgab” o “colgab”: mapas asimétricos propuestos por Gabriel y Odoroff (Gabriel y Odoroff 1990):\n\n“rowgab”: filas en coordenadas principales y columnas en coordenadas estándar multiplicadas por la masa.\n“colgab”: columnas en coordenadas principales y filas en coordenadas estándar multiplicadas por la masa.\n\n“rowgreen” o “colgreen”: Los llamados biplots de contribución que muestran visualmente los puntos que más contribuyen (Greenacre 2006b).\n\n“rowgreen”: filas en coordenadas principales y columnas en coordenadas estándar multiplicadas por la raíz cuadrada de la masa.\n“colgreen”: columnas en coordenadas principales y filas en coordenadas estándar multiplicadas por la raíz cuadrada de la masa.\n\n\nEl siguiente código R dibuja un biplot asimétrico estándar:\n\nfviz_ca_biplot(res.ca,\n               map =\"rowprincipal\", arrow = c(TRUE, TRUE),\n               repel = TRUE)\n\n\n\n\n\n\n\n\nUsamos el argumento flechas, que es un vector de dos lógicas que especifican si la gráfica debe contener puntos (FALSE, predeterminado) o flechas (TRUE). El primer valor establece las filas y el segundo valor establece el columnas.\nSi el ángulo entre dos flechas es agudo, entonces existe una fuerte asociación entre la fila y la columna correspondientes.\nPara interpretar la distancia entre filas y una columna, debe proyectar perpendicularmente puntos de fila en la flecha de la columna.\n\n\nBiplot de contribución\nEn el biplot simétrico estándar (mencionado en la sección anterior), es difícil conocer los puntos que más contribuyen a la solución de la CA.\nMichael Greenacre propuso una nueva escala mostrada (llamada biplot de contribución) que incorpora la contribución de puntos (M. Greenacre 2013). En esta visualización, los puntos que contribuyen muy poco a la solución, están cerca del centro de la biplot y son relativamente poco importantes para la interpretación.\n\nSe puede dibujar un biplot de contribución usando el argumento map = \"rowgreen\" o map = \"colgreen\".\n\nEn primer lugar, hay que decidir si analizar las contribuciones de filas o columnas a la definición de los ejes.\nEn nuestro ejemplo interpretaremos la contribución de las filas a los ejes. Se utiliza el argumento map = \"colgreen\". En este caso, recuerde que las columnas están en coordenadas principales y las filas en coordenadas estándar multiplicadas por la raíz cuadrada de la masa. Para una fila dada, el cuadrado de la nueva coordenada en un eje i es exactamente la contribución de esta fila a la inercia del eje i.\n\nfviz_ca_biplot(res.ca, map =\"colgreen\", arrow = c(TRUE, FALSE),\n               repel = TRUE)\n\n\n\n\n\n\n\n\nEn el gráfico anterior, la posición de los puntos del perfil de la columna no cambia con respecto a la del biplot convencional. Sin embargo, las distancias de los puntos de fila desde el origen de la gráfica están relacionadas con sus contribuciones al mapa de factores bidimensionales.\nCuanto más cerca esté una flecha (en términos de distancia angular) de un eje, mayor será la contribución de la categoría de fila en ese eje en relación con el otro eje. Si la flecha está a medio camino entre los dos, su categoría de fila contribuye a los dos ejes en la misma medida.\n\n\nEs evidente que la categoría de fila Reparaciones (Repairs) tiene una contribución importante al polo positivo de la primera dimensión, mientras que las categorías Lavandería (Laundry) y Comida principal (Main_meal) tienen una contribución importante al polo negativo de la primera dimensión;\nLa dimensión 2 se define principalmente por la categoría de fila Vacaciones (Holidays).\nLa categoría de fila Conducción (Driving) contribuye a los dos ejes en la misma medida.\n\n\n\n\n\nDescripción de las dimensiones\nPara identificar fácilmente los puntos de fila y columna que están más asociados con las dimensiones principales, puede usar la función dimdesc() [en FactoMineR]. Las variables de fila/columna se ordenan por sus coordenadas en la salida dimdesc().\n\n# Descripción de la dimensión\nres.desc &lt;- dimdesc(res.ca, axes = c(1,2))\n\nDescripción de la dimensión 1:\n\n# Descripción de la dimensión 1 por puntos de fila\nhead(res.desc[[1]]$row, 4)\n\n                coord\nLaundry    -0.9918368\nMain_meal  -0.8755855\nDinner     -0.6925740\nBreakfeast -0.5086002\n\n# Descripción de la dimensión 1 por puntos de columna\nhead(res.desc [[1]]$col, 4)\n\n                  coord\nWife        -0.83762154\nAlternating -0.06218462\nJointly      0.14942609\nHusband      1.16091847\n\n\nDescripción de la dimensión 2:\n\n# Descripción de la dimensión 2 por puntos de fila\nres.desc[[2]]$fila\n\nNULL\n\n# Descripción de la dimensión 1 por puntos de columna\nres.desc[[2]]$col\n\n                 coord\nJointly     -1.0265791\nAlternating  0.2915938\nWife         0.3652207\nHusband      0.6019199"
  },
  {
    "objectID": "posts/crd/index.html",
    "href": "posts/crd/index.html",
    "title": "Unveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments",
    "section": "",
    "text": "In the vast expanse of crop science, where precision and reliability are paramount, the choice of experimental design plays a pivotal role in unlocking the secrets of optimal crop growth. Among the arsenal of experimental methodologies, the Completely Randomized Design (CRD) emerges as a beacon of simplicity and statistical robustness. In this blog, we embark on a journey into the realm of CRD, unraveling its principles, applications, and the advantages it brings to the forefront of crop science research.\n\n\nWhat is Completely Randomized Design (CRD)?\nCRD is a statistical method that serves as a robust tool for comparing the effects of different treatments on a single factor within crop experiments. It operates on the principle of randomness, ensuring that each experimental unit has an equal and unbiased chance of receiving any treatment. This randomization process is pivotal, eradicating potential biases and attributing observed crop responses solely to the applied treatments.\nWhen to Use CRD:\nCRD finds its niche in situations where experimental units are relatively homogenous, and there are no discernible patterns of variability within the experimental area. It is particularly valuable in small-scale or preliminary studies, where the primary objective is to gather initial data, identify trends, and pave the way for more intricate experimental designs.\n\n\n\n\nDefine the Research Question and Identify Treatments:\n\nBegin by articulating the research question you aim to answer. Identify the treatments relevant to the research question, laying the groundwork for a focused and purposeful experiment.\n\nPrepare the Experimental Area:\n\nSelect a uniform experimental site, ensuring consistency in soil conditions, topography, and other relevant factors. Divide the area into individual experimental units of equal size to maintain comparability.\n\nLabel the Experimental Units:\n\nAssign unique identifiers to each experimental unit. This step facilitates tracking and systematic data recording throughout the experiment.\n\nRandomly Assign Treatments:\n\nUtilize a random number generator or a randomizing device to assign treatments to experimental units. This step ensures an unbiased distribution of treatments, a key feature of CRD.\n\nApply Treatments:\n\nImplement the assigned treatments to the respective experimental units. Keep accurate records of application dates, quantities, and methods to maintain precision in data collection.\n\nCollect Data:\n\nRegularly monitor the experimental units, collecting pertinent data on crop growth, yield, and any other relevant parameters. Consistent and systematic data collection is crucial for the success of the experiment.\n\nAnalyze the Data:\n\nEmploy appropriate statistical methods to analyze the collected data. Determine whether there are significant differences in crop responses among the different treatment groups, providing valuable insights into the effectiveness of each treatment.\n\n\n\n\n\nSimplicity: CRD’s conceptual straightforwardness and ease of implementation make it accessible to researchers of varying experience levels. Its simplicity is a virtue that expedites the experimental process without compromising reliability.\nFlexibility: CRD’s adaptability shines through, accommodating a wide range of experimental setups. This flexibility allows researchers to explore various factors and treatment combinations, offering versatility in experimental design.\nStatistical Efficiency: By providing a statistically sound basis for comparing treatments, CRD ensures that conclusions drawn from the experiment are valid and reliable. This statistical efficiency is critical for making informed decisions based on experimental outcomes.\nWide Applicability: CRD’s versatility extends across different crop types and experimental environments, making it a valuable tool for crop scientists working in diverse agricultural landscapes.\n\n\n\nCRD has found widespread application in various facets of crop science research, including:\n\nEvaluating Fertilizer Effectiveness:\n\nCRD is instrumental in assessing the impact of different fertilizer rates and application methods on crop yield and nutrient utilization efficiency.\n\nComparing Crop Varieties:\n\nResearchers leverage CRD to compare the performance of various crop varieties under different environmental conditions, aiding in the selection of optimal cultivars.\n\nOptimizing Irrigation Practices:\n\nCRD facilitates investigations into the effects of different irrigation regimes on crop yield and water use efficiency, contributing to water conservation efforts.\n\nExploring Tillage Practices:\n\nResearchers delve into the effects of different tillage practices on soil health and crop productivity, using CRD as a systematic approach to experimentation.\n\n\nIn essence, CRD serves as a linchpin for crop scientists, offering a systematic and statistically sound approach to experimentation. Through its application, researchers generate reliable data that not only informs agricultural practices but also contributes to the broader understanding of crop dynamics, fostering innovation and sustainability in the realm of crop science.\nThe fields of green are not just a canvas for growth; they are a stage for scientific inquiry, and CRD takes center stage in unraveling the intricacies of crop science."
  },
  {
    "objectID": "posts/crd/index.html#understanding-crd-a-statistical-marvel",
    "href": "posts/crd/index.html#understanding-crd-a-statistical-marvel",
    "title": "Unveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments",
    "section": "",
    "text": "What is Completely Randomized Design (CRD)?\nCRD is a statistical method that serves as a robust tool for comparing the effects of different treatments on a single factor within crop experiments. It operates on the principle of randomness, ensuring that each experimental unit has an equal and unbiased chance of receiving any treatment. This randomization process is pivotal, eradicating potential biases and attributing observed crop responses solely to the applied treatments.\nWhen to Use CRD:\nCRD finds its niche in situations where experimental units are relatively homogenous, and there are no discernible patterns of variability within the experimental area. It is particularly valuable in small-scale or preliminary studies, where the primary objective is to gather initial data, identify trends, and pave the way for more intricate experimental designs."
  },
  {
    "objectID": "posts/crd/index.html#conducting-a-crd-experiment-steps-unveiled",
    "href": "posts/crd/index.html#conducting-a-crd-experiment-steps-unveiled",
    "title": "Unveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments",
    "section": "",
    "text": "Define the Research Question and Identify Treatments:\n\nBegin by articulating the research question you aim to answer. Identify the treatments relevant to the research question, laying the groundwork for a focused and purposeful experiment.\n\nPrepare the Experimental Area:\n\nSelect a uniform experimental site, ensuring consistency in soil conditions, topography, and other relevant factors. Divide the area into individual experimental units of equal size to maintain comparability.\n\nLabel the Experimental Units:\n\nAssign unique identifiers to each experimental unit. This step facilitates tracking and systematic data recording throughout the experiment.\n\nRandomly Assign Treatments:\n\nUtilize a random number generator or a randomizing device to assign treatments to experimental units. This step ensures an unbiased distribution of treatments, a key feature of CRD.\n\nApply Treatments:\n\nImplement the assigned treatments to the respective experimental units. Keep accurate records of application dates, quantities, and methods to maintain precision in data collection.\n\nCollect Data:\n\nRegularly monitor the experimental units, collecting pertinent data on crop growth, yield, and any other relevant parameters. Consistent and systematic data collection is crucial for the success of the experiment.\n\nAnalyze the Data:\n\nEmploy appropriate statistical methods to analyze the collected data. Determine whether there are significant differences in crop responses among the different treatment groups, providing valuable insights into the effectiveness of each treatment."
  },
  {
    "objectID": "posts/crd/index.html#advantages-of-crd-unraveling-simplicity-and-robustness",
    "href": "posts/crd/index.html#advantages-of-crd-unraveling-simplicity-and-robustness",
    "title": "Unveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments",
    "section": "",
    "text": "Simplicity: CRD’s conceptual straightforwardness and ease of implementation make it accessible to researchers of varying experience levels. Its simplicity is a virtue that expedites the experimental process without compromising reliability.\nFlexibility: CRD’s adaptability shines through, accommodating a wide range of experimental setups. This flexibility allows researchers to explore various factors and treatment combinations, offering versatility in experimental design.\nStatistical Efficiency: By providing a statistically sound basis for comparing treatments, CRD ensures that conclusions drawn from the experiment are valid and reliable. This statistical efficiency is critical for making informed decisions based on experimental outcomes.\nWide Applicability: CRD’s versatility extends across different crop types and experimental environments, making it a valuable tool for crop scientists working in diverse agricultural landscapes."
  },
  {
    "objectID": "posts/crd/index.html#applications-of-crd-in-crop-science-navigating-agricultural-frontiers",
    "href": "posts/crd/index.html#applications-of-crd-in-crop-science-navigating-agricultural-frontiers",
    "title": "Unveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments",
    "section": "",
    "text": "CRD has found widespread application in various facets of crop science research, including:\n\nEvaluating Fertilizer Effectiveness:\n\nCRD is instrumental in assessing the impact of different fertilizer rates and application methods on crop yield and nutrient utilization efficiency.\n\nComparing Crop Varieties:\n\nResearchers leverage CRD to compare the performance of various crop varieties under different environmental conditions, aiding in the selection of optimal cultivars.\n\nOptimizing Irrigation Practices:\n\nCRD facilitates investigations into the effects of different irrigation regimes on crop yield and water use efficiency, contributing to water conservation efforts.\n\nExploring Tillage Practices:\n\nResearchers delve into the effects of different tillage practices on soil health and crop productivity, using CRD as a systematic approach to experimentation.\n\n\nIn essence, CRD serves as a linchpin for crop scientists, offering a systematic and statistically sound approach to experimentation. Through its application, researchers generate reliable data that not only informs agricultural practices but also contributes to the broader understanding of crop dynamics, fostering innovation and sustainability in the realm of crop science.\nThe fields of green are not just a canvas for growth; they are a stage for scientific inquiry, and CRD takes center stage in unraveling the intricacies of crop science."
  },
  {
    "objectID": "posts/crd/index.html#step-1-set-the-stage-with-agror",
    "href": "posts/crd/index.html#step-1-set-the-stage-with-agror",
    "title": "Unveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments",
    "section": "Step 1: Set the Stage with AgroR",
    "text": "Step 1: Set the Stage with AgroR\n\nInstall and Load the AgroR Package\nBegin by installing the AgroR package using the following R command:\n\ninstall.packages(\"AgroR\")\n\nAfter installation, load the package into your R session with:\n\nlibrary(AgroR)"
  },
  {
    "objectID": "posts/crd/index.html#step-2-prepare-your-data-for-analysis",
    "href": "posts/crd/index.html#step-2-prepare-your-data-for-analysis",
    "title": "Unveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments",
    "section": "Step 2: Prepare Your Data for Analysis",
    "text": "Step 2: Prepare Your Data for Analysis\nEnsure your CRD data is well-organized in a data frame with the following structure:\n\nEach row represents an experimental unit.\nThe first column contains treatment assignments (e.g., “Treatment 1”, “Treatment 2”, etc.).\nSubsequent columns hold measured values for each experimental unit and response variable (e.g., yield, plant height)."
  },
  {
    "objectID": "posts/crd/index.html#step-3-unveil-patterns-with-analysis-of-variance-anova",
    "href": "posts/crd/index.html#step-3-unveil-patterns-with-analysis-of-variance-anova",
    "title": "Unveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments",
    "section": "Step 3: Unveil Patterns with Analysis of Variance (ANOVA)",
    "text": "Step 3: Unveil Patterns with Analysis of Variance (ANOVA)\nUtilize the DIC function to perform ANOVA on your CRD data:\n\ndata(pomegranate)\n\nwith(pomegranate, DIC(trat, WL, ylab = \"Weight loss (%)\")) # tukey\n\n\n-----------------------------------------------------------------\nNormality of errors\n-----------------------------------------------------------------\n                         Method Statistic   p.value\n Shapiro-Wilk normality test(W) 0.9448293 0.2087967\n\n\nAs the calculated p-value is greater than the 5% significance level, hypothesis H0 is not rejected. Therefore, errors can be considered normal\n\n\n\n-----------------------------------------------------------------\nHomogeneity of Variances\n-----------------------------------------------------------------\n                              Method Statistic   p.value\n Bartlett test(Bartlett's K-squared)  8.568274 0.1275737\n\n\nAs the calculated p-value is greater than the 5% significance level,hypothesis H0 is not rejected. Therefore, the variances can be considered homogeneous\n\n\n\n-----------------------------------------------------------------\nIndependence from errors\n-----------------------------------------------------------------\n                 Method Statistic   p.value\n Durbin-Watson test(DW)  2.104821 0.1924474\n\n\nAs the calculated p-value is greater than the 5% significance level, hypothesis H0 is not rejected. Therefore, errors can be considered independent\n\n\n\n-----------------------------------------------------------------\nAdditional Information\n-----------------------------------------------------------------\n\nCV (%) =  10.84\nMStrat/MST =  0.92\nMean =  2.2596\nMedian =  2.225\nPossible outliers =  No discrepant point\n\n-----------------------------------------------------------------\nAnalysis of Variance\n-----------------------------------------------------------------\n          Df   Sum Sq    Mean.Sq  F value        Pr(F)\nTreatment  5 3.692121 0.73842417 12.31191 2.723541e-05\nResiduals 18 1.079575 0.05997639                      \n\n\nAs the calculated p-value, it is less than the 5% significance level.The hypothesis H0 of equality of means is rejected. Therefore, at least two treatments differ\n\n\n\n\n-----------------------------------------------------------------\nMultiple Comparison Test: Tukey HSD\n-----------------------------------------------------------------\n     resp groups\nT5 2.6375      a\nT4 2.6200      a\nT3 2.6175      a\nT6 2.1625     ab\nT1 1.8425      b\nT2 1.6775      b\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuse mcomp to multiple comparison test (Tukey (default), LSD, Scott-Knott and Duncan)\n\nwith(pomegranate, DIC(trat, WL, mcomp = \"duncan\", ylab = \"Weight loss (%)\"))\n\n\n-----------------------------------------------------------------\nNormality of errors\n-----------------------------------------------------------------\n                         Method Statistic   p.value\n Shapiro-Wilk normality test(W) 0.9448293 0.2087967\n\n\nAs the calculated p-value is greater than the 5% significance level, hypothesis H0 is not rejected. Therefore, errors can be considered normal\n\n\n\n-----------------------------------------------------------------\nHomogeneity of Variances\n-----------------------------------------------------------------\n                              Method Statistic   p.value\n Bartlett test(Bartlett's K-squared)  8.568274 0.1275737\n\n\nAs the calculated p-value is greater than the 5% significance level,hypothesis H0 is not rejected. Therefore, the variances can be considered homogeneous\n\n\n\n-----------------------------------------------------------------\nIndependence from errors\n-----------------------------------------------------------------\n                 Method Statistic   p.value\n Durbin-Watson test(DW)  2.104821 0.1924474\n\n\nAs the calculated p-value is greater than the 5% significance level, hypothesis H0 is not rejected. Therefore, errors can be considered independent\n\n\n\n-----------------------------------------------------------------\nAdditional Information\n-----------------------------------------------------------------\n\nCV (%) =  10.84\nMStrat/MST =  0.92\nMean =  2.2596\nMedian =  2.225\nPossible outliers =  No discrepant point\n\n-----------------------------------------------------------------\nAnalysis of Variance\n-----------------------------------------------------------------\n          Df   Sum Sq    Mean.Sq  F value        Pr(F)\nTreatment  5 3.692121 0.73842417 12.31191 2.723541e-05\nResiduals 18 1.079575 0.05997639                      \n\n\nAs the calculated p-value, it is less than the 5% significance level.The hypothesis H0 of equality of means is rejected. Therefore, at least two treatments differ\n\n\n\n\n-----------------------------------------------------------------\nMultiple Comparison Test: Duncan\n-----------------------------------------------------------------\n     resp groups\nT5 2.6375      a\nT4 2.6200      a\nT3 2.6175      a\nT6 2.1625      b\nT1 1.8425     bc\nT2 1.6775      c\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchart type: Graph type (columns, boxes or segments)\n\nwith(pomegranate, DIC(trat, WL, geom=\"point\", ylab = \"Weight loss (%)\"))\n\n\n-----------------------------------------------------------------\nNormality of errors\n-----------------------------------------------------------------\n                         Method Statistic   p.value\n Shapiro-Wilk normality test(W) 0.9448293 0.2087967\n\n\nAs the calculated p-value is greater than the 5% significance level, hypothesis H0 is not rejected. Therefore, errors can be considered normal\n\n\n\n-----------------------------------------------------------------\nHomogeneity of Variances\n-----------------------------------------------------------------\n                              Method Statistic   p.value\n Bartlett test(Bartlett's K-squared)  8.568274 0.1275737\n\n\nAs the calculated p-value is greater than the 5% significance level,hypothesis H0 is not rejected. Therefore, the variances can be considered homogeneous\n\n\n\n-----------------------------------------------------------------\nIndependence from errors\n-----------------------------------------------------------------\n                 Method Statistic   p.value\n Durbin-Watson test(DW)  2.104821 0.1924474\n\n\nAs the calculated p-value is greater than the 5% significance level, hypothesis H0 is not rejected. Therefore, errors can be considered independent\n\n\n\n-----------------------------------------------------------------\nAdditional Information\n-----------------------------------------------------------------\n\nCV (%) =  10.84\nMStrat/MST =  0.92\nMean =  2.2596\nMedian =  2.225\nPossible outliers =  No discrepant point\n\n-----------------------------------------------------------------\nAnalysis of Variance\n-----------------------------------------------------------------\n          Df   Sum Sq    Mean.Sq  F value        Pr(F)\nTreatment  5 3.692121 0.73842417 12.31191 2.723541e-05\nResiduals 18 1.079575 0.05997639                      \n\n\nAs the calculated p-value, it is less than the 5% significance level.The hypothesis H0 of equality of means is rejected. Therefore, at least two treatments differ\n\n\n\n\n-----------------------------------------------------------------\nMultiple Comparison Test: Tukey HSD\n-----------------------------------------------------------------\n     resp groups\nT5 2.6375      a\nT4 2.6200      a\nT3 2.6175      a\nT6 2.1625     ab\nT1 1.8425      b\nT2 1.6775      b"
  },
  {
    "objectID": "posts/crd/index.html#step-4-decode-and-conclude",
    "href": "posts/crd/index.html#step-4-decode-and-conclude",
    "title": "Unveiling the Power of Completely Randomized Design (CRD) in Crop Science Experiments",
    "section": "Step 4: Decode and Conclude",
    "text": "Step 4: Decode and Conclude\n\nInterpret the CRD Results\nBased on ANOVA p-values and multiple comparisons, identify significant differences in the response variable among treatments.\nIf differences are detected, pinpoint the treatment combinations responsible for those variations.\nConsider the biological implications of observed differences and draw conclusions about treatment effects on the response variable.\nBy systematically following these steps, you’ll harness the analytical prowess of AgroR and CRD, unraveling the intricacies of your crop science experiments with precision and confidence."
  },
  {
    "objectID": "posts/agridata/index.html",
    "href": "posts/agridata/index.html",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "",
    "text": "El análisis de datos es una etapa esencial en el proceso de investigación y consiste en examinar y analizar los datos recopilados de manera detallada para extraer información y obtener conclusiones relevantes en función a los objetivos de la investigación. Para realizar el análisis de datos, se pueden utilizar diferentes herramientas y técnicas, generalmente se construyen tablas y figuras. Estos resultados son usados para tomar decisiones y formular conclusiones sobre los datos, y también pueden ser utilizados para mejorar y optimizar procesos y sistemas. El análisis de datos es importante ya que permite a los investigadores obtener una visión más profunda y detallada de los datos y tomar decisiones más informadas y precisas.\nLa etapa de análisis de datos puede resultar un reto difícil para un recien egresado o tesista. Esto debido a la falta de experiencia o conocimiento en el uso de herramientas y técnicas de análisis de datos. Debido a ello, queremos facilitar algunas herramientas de análisis, al mismo tiempo, se pretende que los interesados puedan entrenarse en el uso de R. Ya que esta herramienta estadística ofrece un sin fin de paquetes que nos ayudan a realizar los análisis de manera sencilla pero de mucha categoría científica."
  },
  {
    "objectID": "posts/agridata/index.html#librerias",
    "href": "posts/agridata/index.html#librerias",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "Librerias",
    "text": "Librerias"
  },
  {
    "objectID": "posts/agridata/index.html#base-de-datos",
    "href": "posts/agridata/index.html#base-de-datos",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "Base de datos",
    "text": "Base de datos\nPara esta oportunidad daremos uso de la base de datos de iris. Estos datos estarán disponibles al momento de instalar R."
  },
  {
    "objectID": "posts/agridata/index.html#análisis-descriptivo",
    "href": "posts/agridata/index.html#análisis-descriptivo",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "Análisis descriptivo",
    "text": "Análisis descriptivo\nGeneralmente se puede presentar en una tabla los resultados descriptivos.\n\n\n\nAnálisis descriptivo\n\n\nvariable\nmean\nse\nkurt\nskew\nmin\nmax\n\n\n\n\nPetal.Length\n3.76\n0.14\n-1.40\n-0.27\n1.0\n6.9\n\n\nPetal.Width\n1.20\n0.06\n-1.34\n-0.10\n0.1\n2.5\n\n\nSepal.Length\n5.84\n0.07\n-0.55\n0.31\n4.3\n7.9\n\n\nSepal.Width\n3.06\n0.04\n0.23\n0.32\n2.0\n4.4\n\n\n\n\n\n\n\nLa otra opción podría ser una figura boxplot"
  },
  {
    "objectID": "posts/agridata/index.html#análisis-de-varianza",
    "href": "posts/agridata/index.html#análisis-de-varianza",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "Análisis de varianza",
    "text": "Análisis de varianza\nGeneralmente en la mayoría de las tesis se presenta cada ANOVA para cada variable, la cual no es muy redundante en todo el documento. Además, en las revistas científicas de impacto, este tipo de tablas no es permitido si se tiene varias variables.\n\n\nAnalysis of Variance Table\n\nResponse: Sepal.Width\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 11.345  5.6725   49.16 &lt; 2.2e-16 ***\nResiduals 147 16.962  0.1154                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSin embargo, se podría resumir de esta manera. En la tabla solamente se presenta aquellas variables con significancia estadística.\n\n\n\nAnálisis de varianza\n\n\nFV\nDF\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\n\n\n\n\nSpecies\n2\n31.61 **\n5.67 **\n218.55 **\n40.21 **\n\n\nResiduals\n147\n0.27\n0.12\n0.19\n0.04\n\n\n\n\n\n\n\nUna vez identificada las diferencias estadisticas se procede a realizar pruebas de promedio para seleccionar genotipos sobresalientes.\nHay otra manera de representar comparaciones graficamente\n\n\n\n\n\n\n\n\n\nTambien existe otra opción con el paquete AgroR\n\n\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------\nNormality of errors\n-----------------------------------------------------------------\n                         Method Statistic   p.value\n Shapiro-Wilk normality test(W) 0.9878974 0.2188639\n\n\n-----------------------------------------------------------------\nHomogeneity of Variances\n-----------------------------------------------------------------\n                              Method Statistic      p.value\n Bartlett test(Bartlett's K-squared)   16.0057 0.0003345076\n\n\n-----------------------------------------------------------------\nIndependence from errors\n-----------------------------------------------------------------\n                 Method Statistic   p.value\n Durbin-Watson test(DW)  2.043002 0.5401261\n\n\n-----------------------------------------------------------------\nAdditional Information\n-----------------------------------------------------------------\n\nCV (%) =  8.81\nMStrat/MST =  0.99\nMean =  5.8433\nMedian =  5.8\nPossible outliers =  107\n\n-----------------------------------------------------------------\nAnalysis of Variance\n-----------------------------------------------------------------\n           Df   Sum Sq    Mean.Sq  F value        Pr(F)\ntrat        2 63.21213 31.6060667 119.2645 1.669669e-31\nResiduals 147 38.95620  0.2650082                      \n\n\n\n\n-----------------------------------------------------------------\nMultiple Comparison Test: Tukey HSD\n-----------------------------------------------------------------\n            resp groups\nvirginica  6.588      a\nversicolor 5.936      b\nsetosa     5.006      c"
  },
  {
    "objectID": "posts/agridata/index.html#análisis-multivariado",
    "href": "posts/agridata/index.html#análisis-multivariado",
    "title": "Análisis de datos experimentales agrícolas",
    "section": "Análisis multivariado",
    "text": "Análisis multivariado\n\n\n\n-------------------------------------------------------------------------------\nPrincipal Component Analysis\n-------------------------------------------------------------------------------\n# A tibble: 22 × 4\n   PC    Eigenvalues `Variance (%)` `Cum. variance (%)`\n   &lt;chr&gt;       &lt;dbl&gt;          &lt;dbl&gt;               &lt;dbl&gt;\n 1 PC1         14.2            64.4                64.4\n 2 PC2          5.24           23.8                88.2\n 3 PC3          2.58           11.8               100  \n 4 PC4          0               0                 100  \n 5 PC5          0               0                 100  \n 6 PC6          0               0                 100  \n 7 PC7          0               0                 100  \n 8 PC8          0               0                 100  \n 9 PC9          0               0                 100  \n10 PC10         0               0                 100  \n# … with 12 more rows\n-------------------------------------------------------------------------------\nFactor Analysis - factorial loadings after rotation-\n-------------------------------------------------------------------------------\n# A tibble: 22 × 6\n   VAR     FA1   FA2   FA3 Communality Uniquenesses\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 NNCF  -0.72 -0.67 -0.17           1            0\n 2 WNCF  -0.65 -0.75 -0.14           1            0\n 3 AWNCF  0.63  0.49  0.6            1            0\n 4 WUE    1     0.04  0.03           1            0\n 5 NDBF  -0.11  0.07 -0.99           1            0\n 6 NDFF  -0.16 -0.24 -0.96           1            0\n 7 NDBH  -0.17  0.07 -0.98           1            0\n 8 PHYL   0.6   0.79 -0.12           1            0\n 9 TA     0.84  0.1   0.54           1            0\n10 NCF   -0.84 -0.42 -0.35           1            0\n# … with 12 more rows\n-------------------------------------------------------------------------------\nComunalit Mean: 1 \n-------------------------------------------------------------------------------\nSelection differential \n-------------------------------------------------------------------------------\n# A tibble: 22 × 8\n   VAR    Factor       Xo       Xs        SD  SDperc sense     goal\n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1 NNCF   FA1        7.20     7.86    0.650    9.02  increase   100\n 2 AWNCF  FA1        8.74     8.70   -0.0425  -0.486 increase     0\n 3 WUE    FA1      116.     101.    -15.0    -12.9   increase     0\n 4 TA     FA1        1.42     1.35   -0.0700  -4.93  increase     0\n 5 NCF    FA1       23.5     25.0     1.52     6.48  increase   100\n 6 TNF    FA1       30.7     32.8     2.16     7.04  increase   100\n 7 WCF    FA1      359.     395.     36.9     10.3   increase   100\n 8 TWF    FA1      413.     442.     29.1      7.05  increase   100\n 9 FY     FA1    30957.   33137.   2180.       7.04  increase   100\n10 TSS_TA FA1        5.39     5.66    0.268    4.96  increase   100\n# … with 12 more rows\n------------------------------------------------------------------------------\nSelected genotypes\n-------------------------------------------------------------------------------\nNAC_CAM IMP_ALB\n-------------------------------------------------------------------------------\n\n\n\n\nJoining, by = \"VAR\""
  },
  {
    "objectID": "posts/MDA/index.html",
    "href": "posts/MDA/index.html",
    "title": "FAMD - Factor Analysis of Mixed Data in R",
    "section": "",
    "text": "Factor Analysis of Mixed Data (FAMD) is a powerful statistical technique used to analyze datasets that contain both numerical and categorical variables. It extends traditional factor analysis to handle mixed data types, providing a comprehensive understanding of the underlying structure of complex datasets. In this blog post, we will delve into the essentials of FAMD, exploring its computation, R packages, data format, R code, visualization, and interpretation."
  },
  {
    "objectID": "posts/MDA/index.html#introduction",
    "href": "posts/MDA/index.html#introduction",
    "title": "FAMD - Factor Analysis of Mixed Data in R",
    "section": "",
    "text": "Factor Analysis of Mixed Data (FAMD) is a powerful statistical technique used to analyze datasets that contain both numerical and categorical variables. It extends traditional factor analysis to handle mixed data types, providing a comprehensive understanding of the underlying structure of complex datasets. In this blog post, we will delve into the essentials of FAMD, exploring its computation, R packages, data format, R code, visualization, and interpretation."
  },
  {
    "objectID": "posts/MDA/index.html#computation",
    "href": "posts/MDA/index.html#computation",
    "title": "FAMD - Factor Analysis of Mixed Data in R",
    "section": "Computation",
    "text": "Computation\nFAMD combines multiple techniques, such as Principal Component Analysis (PCA) for numerical variables and Multiple Correspondence Analysis (MCA) for categorical variables. The integration of these methods allows FAMD to effectively capture the relationships between variables and reveal underlying patterns in mixed datasets.\n\nR Packages\nTo perform FAMD in R, the FactoMineR and factoextra package are widely used and offers a dedicated function for FAMD. Ensure that you have the package installed by running:\n\ninstall.packages(c(\"FactoMineR\", \"factoextra\"))\n\nload packages:\n\nlibrary(FactoMineR)\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\n\n\nData Format\nBefore applying FAMD, it’s crucial to prepare your data appropriately. Ensure that your dataset contains both numerical and categorical variables. Standardize numerical variables if necessary and encode categorical variables properly.\nWe’ll use a subset of the wine data set available in FactoMineR package:\n\nlibrary(\"FactoMineR\")\ndata(wine)\ndf &lt;- wine[,c(1,2, 16, 22, 29, 28, 30,31)]\nhead(df[, 1:7], 4)\n\n          Label Soil Plante Acidity Harmony Intensity Overall.quality\n2EL      Saumur Env1  2.000   2.107   3.143     2.857           3.393\n1CHA     Saumur Env1  2.000   2.107   2.964     2.893           3.214\n1FON Bourgueuil Env1  1.750   2.179   3.143     3.074           3.536\n1VAU     Chinon Env2  2.304   3.179   2.038     2.462           2.464\n\n\nTo see the structure of the data, type this:\n\nstr(df)\n\n'data.frame':   21 obs. of  8 variables:\n $ Label          : Factor w/ 3 levels \"Saumur\",\"Bourgueuil\",..: 1 1 2 3 1 2 2 1 3 1 ...\n $ Soil           : Factor w/ 4 levels \"Reference\",\"Env1\",..: 2 2 2 3 1 1 1 2 2 3 ...\n $ Plante         : num  2 2 1.75 2.3 1.76 ...\n $ Acidity        : num  2.11 2.11 2.18 3.18 2.57 ...\n $ Harmony        : num  3.14 2.96 3.14 2.04 3.64 ...\n $ Intensity      : num  2.86 2.89 3.07 2.46 3.64 ...\n $ Overall.quality: num  3.39 3.21 3.54 2.46 3.74 ...\n $ Typical        : num  3.25 3.04 3.18 2.25 3.44 ...\n\n\nThe data contains 21 rows (wines, individuals) and 8 columns (variables):\n\nThe first two columns are factors (categorical variables): label (Saumur, Bourgueil or Chinon) and soil (Reference, Env1, Env2 or Env4).\nThe remaining columns are numeric (continuous variables).\n\n\n\n\n\n\n\nTip\n\n\n\nThe goal of this study is to analyze the characteristics of the wines."
  },
  {
    "objectID": "posts/MDA/index.html#r-code",
    "href": "posts/MDA/index.html#r-code",
    "title": "FAMD - Factor Analysis of Mixed Data in R",
    "section": "R Code",
    "text": "R Code\nThe function FAMD() [FactoMiner package] can be used to compute FAMD. A simplified format is:\n\nFAMD (base, ncp = 5, sup.var = NULL, ind.sup = NULL, graph = TRUE)\n\n\nbase: a data frame with n rows (individuals) and p columns (variables).\nncp: the number of dimensions kept in the results (by default 5)\nsup.var: a vector indicating the indexes of the supplementary variables.\nind.sup: a vector indicating the indexes of the supplementary individuals.\ngraph: a logical value. If TRUE a graph is displayed.\n\nTo compute FAMD, type this:\n\nlibrary(FactoMineR)\nres.famd &lt;- FAMD(df, graph = FALSE)\n\nThe output of the FAMD() function is a list including:\n\nprint(res.famd)\n\n*The results are available in the following objects:\n\n  name          description                             \n1 \"$eig\"        \"eigenvalues and inertia\"               \n2 \"$var\"        \"Results for the variables\"             \n3 \"$ind\"        \"results for the individuals\"           \n4 \"$quali.var\"  \"Results for the qualitative variables\" \n5 \"$quanti.var\" \"Results for the quantitative variables\"\n\n\n\nVisualization and Interpretation\nWe’ll use the following factoextra functions:\n\nget_eigenvalue(res.famd): Extract the eigenvalues/variances retained by each dimension (axis).\nfviz_eig(res.famd): Visualize the eigenvalues/variances.\nget_famd_ind(res.famd): Extract the results for individuals.\nget_famd_var(res.famd): Extract the results for quantitative and qualitative variables.\nfviz_famd_ind(res.famd), fviz_famd_var(res.famd): Visualize the results for individuals and variables, respectively.\n\nIn the next sections, we’ll illustrate each of these functions.\n\n\n\n\n\n\nTip\n\n\n\nTo help in the interpretation of FAMD, we highly recommend to read the interpretation of principal component analysis (Chapter (???)(principal-component-analysis)) and multiple correspondence analysis (Chapter (???)(multiple-correspondence-analysis)). Many of the graphs presented here have been already described in our previous chapters.\n\n\n\n\nEigenvalues / Variances\nThe proportion of variances retained by the different dimensions (axes) can be extracted using the function get_eigenvalue() [factoextra package] as follow:\n\nlibrary(\"factoextra\")\neig.val &lt;- get_eigenvalue(res.famd)\nhead(eig.val)\n\n      eigenvalue variance.percent cumulative.variance.percent\nDim.1  4.8315174        43.922886                    43.92289\nDim.2  1.8568797        16.880724                    60.80361\nDim.3  1.5824794        14.386176                    75.18979\nDim.4  1.1491200        10.446546                    85.63633\nDim.5  0.6518053         5.925503                    91.56183\n\n\nThe function fviz_eig() or fviz_screeplot() [factoextra package] can be used to draw the scree plot (the percentages of inertia explained by each FAMD dimensions):\n\nfviz_screeplot(res.famd)\n\n\n\n\n\n\n\n\n\n\nGraph of Variables\n\nAll variables\nThe function get_mfa_var() [in factoextra] is used to extract the results for variables. By default, this function returns a list containing the coordinates, the cos2 and the contribution of all variables:\n\nvar &lt;- get_famd_var(res.famd)\nvar\n\nFAMD results for variables \n ===================================================\n  Name       Description                      \n1 \"$coord\"   \"Coordinates\"                    \n2 \"$cos2\"    \"Cos2, quality of representation\"\n3 \"$contrib\" \"Contributions\"                  \n\n\nThe different components can be accessed as follow:\n\n# Coordinates of variables\nhead(var$coord)\n\n                    Dim.1       Dim.2       Dim.3       Dim.4        Dim.5\nPlante          0.7344160 0.060551966 0.105902048 0.004011299 0.0010340559\nAcidity         0.1732738 0.491118153 0.126394029 0.115376784 0.0045862935\nHarmony         0.8943968 0.023628146 0.040124469 0.003653813 0.0086624633\nIntensity       0.6991811 0.134639254 0.065382234 0.023214984 0.0064730431\nOverall.quality 0.9115699 0.005246728 0.009336677 0.005445276 0.0007961880\nTypical         0.7808611 0.027094327 0.001549791 0.083446627 0.0005912942\n\n# Cos2: quality of representation on the factore map\nhead(var$cos2)\n\n                     Dim.1        Dim.2        Dim.3        Dim.4        Dim.5\nPlante          0.53936692 3.666541e-03 1.121524e-02 1.609052e-05 1.069272e-06\nAcidity         0.03002381 2.411970e-01 1.597545e-02 1.331180e-02 2.103409e-05\nHarmony         0.79994566 5.582893e-04 1.609973e-03 1.335035e-05 7.503827e-05\nIntensity       0.48885427 1.812773e-02 4.274836e-03 5.389355e-04 4.190029e-05\nOverall.quality 0.83095973 2.752815e-05 8.717353e-05 2.965103e-05 6.339153e-07\nTypical         0.60974400 7.341026e-04 2.401853e-06 6.963340e-03 3.496288e-07\n\n# Contributions to the  dimensions\nhead(var$contrib)\n\n                    Dim.1      Dim.2      Dim.3      Dim.4      Dim.5\nPlante          15.200526  3.2609526 6.69215972  0.3490757 0.15864490\nAcidity          3.586323 26.4485720 7.98708850 10.0404466 0.70362936\nHarmony         18.511716  1.2724651 2.53554453  0.3179662 1.32899551\nIntensity       14.471254  7.2508336 4.13163258  2.0202401 0.99309457\nOverall.quality 18.867156  0.2825562 0.59000304  0.4738648 0.12215119\nTypical         16.161818  1.4591321 0.09793437  7.2617850 0.09071638\n\n\nThe following figure shows the correlation between variables - both quantitative and qualitative variables - and the principal dimensions, as well as, the contribution of variables to the dimensions 1 and 2. The following functions [in the factoextra package] are used:\n\nfviz_famd_var() to plot both quantitative and qualitative variables\nfviz_contrib() to visualize the contribution of variables to the principal dimensions\n\n\n# Plot of variables\nfviz_famd_var(res.famd, repel = TRUE)\n\n\n\n\n\n\n\n# Contribution to the first dimension\nfviz_contrib(res.famd, \"var\", axes = 1)\n\n\n\n\n\n\n\n# Contribution to the second dimension\nfviz_contrib(res.famd, \"var\", axes = 2)\n\n\n\n\n\n\n\n\nThe red dashed line on the graph above indicates the expected average value, If the contributions were uniform. Read more in chapter (Chapter @ref(principal-component-analysis)).\n\n\n\n\n\n\nTip\n\n\n\nFrom the plots above, it can be seen that: - variables that contribute the most to the first dimension are: Overall.quality and Harmony. - variables that contribute the most to the second dimension are: Soil and Acidity.\n\n\n\n\nQuantitative variables\nTo extract the results for quantitative variables, type this:\n\nquanti.var &lt;- get_famd_var(res.famd, \"quanti.var\")\nquanti.var\n\nFAMD results for quantitative variables \n ===================================================\n  Name       Description                      \n1 \"$coord\"   \"Coordinates\"                    \n2 \"$cos2\"    \"Cos2, quality of representation\"\n3 \"$contrib\" \"Contributions\"                  \n\n\nIn this section, we’ll describe how to visualize quantitative variables. Additionally, we’ll show how to highlight variables according to either i) their quality of representation on the factor map or ii) their contributions to the dimensions.\nThe R code below plots quantitative variables. We use repel = TRUE, to avoid text overlapping.\n\nfviz_famd_var(res.famd, \"quanti.var\", repel = TRUE,\n              col.var = \"black\")\n\n\n\n\n\n\n\n\nBriefly, the graph of variables (correlation circle) shows the relationship between variables, the quality of the representation of variables, as well as, the correlation between variables and the dimensions. Read more at PCA (Chapter @ref(principal-component-analysis)), MCA (Chapter @ref(multiple-correspondence-analysis)) and MFA (Chapter @ref(multiple-factor-analysis)).\nThe most contributing quantitative variables can be highlighted on the scatter plot using the argument col.var = \"contrib\". This produces a gradient colors, which can be customized using the argument gradient.cols.\n\nfviz_famd_var(res.famd, \"quanti.var\", col.var = \"contrib\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE)\n\n\n\n\n\n\n\n\nSimilarly, you can highlight quantitative variables using their cos2 values representing the quality of representation on the factor map. If a variable is well represented by two dimensions, the sum of the cos2 is closed to one. For some of the items, more than 2 dimensions might be required to perfectly represent the data.\n\n# Color by cos2 values: quality on the factor map\nfviz_famd_var(res.famd, \"quanti.var\", col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE)\n\n\n\n\n\n\n\n\n\n\nGraph of qualitative variables\nLike quantitative variables, the results for qualitative variables can be extracted as follow:\n\nquali.var &lt;- get_famd_var(res.famd, \"quali.var\")\nquali.var\n\nFAMD results for qualitative variable categories \n ===================================================\n  Name       Description                      \n1 \"$coord\"   \"Coordinates\"                    \n2 \"$cos2\"    \"Cos2, quality of representation\"\n3 \"$contrib\" \"Contributions\"                  \n\n\nTo visualize qualitative variables, type this:\n\nfviz_famd_var(res.famd, \"quali.var\", col.var = \"contrib\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\n             )\n\n\n\n\n\n\n\n\nThe plot above shows the categories of the categorical variables.\n\n\nGraph of individuals\nTo get the results for individuals, type this:\n\nind &lt;- get_famd_ind(res.famd)\nind\n\nFAMD results for individuals \n ===================================================\n  Name       Description                      \n1 \"$coord\"   \"Coordinates\"                    \n2 \"$cos2\"    \"Cos2, quality of representation\"\n3 \"$contrib\" \"Contributions\"                  \n\n\nTo plot individuals, use the function fviz_mfa_ind() [in factoextra]. By default, individuals are colored in blue. However, like variables, it’s also possible to color individuals by their cos2 and contribution values:\n\nfviz_famd_ind(res.famd, col.ind = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn the plot above, the qualitative variable categories are shown in black. Env1, Env2, Env3 are the categories of the soil. Saumur, Bourgueuil and Chinon are the categories of the wine Label. If you don’t want to show them on the plot, use the argument invisible = \"quali.var\".\n\n\nIndividuals with similar profiles are close to each other on the factor map. For the interpretation, read more at Chapter @ref(multiple-correspondence-analysis) (MCA) and Chapter @ref(multiple-factor-analysis) (MFA).\nNote that, it’s possible to color the individuals using any of the qualitative variables in the initial data table. To do this, the argument habillage is used in the fviz_famd_ind() function. For example, if you want to color the wines according to the supplementary qualitative variable “Label”, type this:\n\nfviz_mfa_ind(res.famd, \n             habillage = \"Label\", # color by groups \n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             addEllipses = TRUE, ellipse.type = \"confidence\", \n             repel = TRUE # Avoid text overlapping\n             ) \n\n\n\n\n\n\n\n\nIf you want to color individuals using multiple categorical variables at the same time, use the function fviz_ellipses() [in factoextra] as follow:\n\nfviz_ellipses(res.famd, c(\"Label\", \"Soil\"), repel = TRUE)\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nℹ Please use `gather()` instead.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n\nAlternatively, you can specify categorical variable indices:\n\nfviz_ellipses(res.famd, 1:2, geom = \"point\")"
  },
  {
    "objectID": "posts/MDA/index.html#summary",
    "href": "posts/MDA/index.html#summary",
    "title": "FAMD - Factor Analysis of Mixed Data in R",
    "section": "Summary",
    "text": "Summary\nFactor Analysis of Mixed Data (FAMD) is a versatile tool for analyzing datasets with a mix of numerical and categorical variables. Through the ‘FactoMineR’ package in R, users can seamlessly perform FAMD, gaining valuable insights into the underlying structure of their data. By visualizing eigenvalues, variable contributions, and individual distributions, researchers can interpret complex relationships and make informed decisions based on the extracted factors. Incorporate FAMD into your analytical toolkit to unlock the full potential of mixed data analysis."
  },
  {
    "objectID": "posts/AI/index.html",
    "href": "posts/AI/index.html",
    "title": "Inteligencia artificial en la agricultura",
    "section": "",
    "text": "La inteligencia artificial es la capacidad de una máquina o sistema de software para realizar tareas que normalmente requerirían inteligencia humana, como el aprendizaje, la percepción, el razonamiento y la toma de decisiones. La IA puede ser utilizada en una amplia variedad de aplicaciones, desde el reconocimiento de imágenes hasta la automatización de procesos."
  },
  {
    "objectID": "posts/AI/index.html#qué-es-la-inteligencia-artificial",
    "href": "posts/AI/index.html#qué-es-la-inteligencia-artificial",
    "title": "Inteligencia artificial en la agricultura",
    "section": "",
    "text": "La inteligencia artificial es la capacidad de una máquina o sistema de software para realizar tareas que normalmente requerirían inteligencia humana, como el aprendizaje, la percepción, el razonamiento y la toma de decisiones. La IA puede ser utilizada en una amplia variedad de aplicaciones, desde el reconocimiento de imágenes hasta la automatización de procesos."
  },
  {
    "objectID": "posts/AI/index.html#el-estado-del-arte-de-la-ia",
    "href": "posts/AI/index.html#el-estado-del-arte-de-la-ia",
    "title": "Inteligencia artificial en la agricultura",
    "section": "El estado del arte de la IA",
    "text": "El estado del arte de la IA\nEn la actualidad, la IA se encuentra en un estado avanzado de desarrollo. Los sistemas de IA están mejorando rápidamente y se están utilizando en una amplia variedad de aplicaciones, incluyendo la medicina, el transporte, la banca y la industria agrícola."
  },
  {
    "objectID": "posts/AI/index.html#desafíos-y-oportunidades-futuras",
    "href": "posts/AI/index.html#desafíos-y-oportunidades-futuras",
    "title": "Inteligencia artificial en la agricultura",
    "section": "Desafíos y oportunidades futuras",
    "text": "Desafíos y oportunidades futuras\nA pesar del progreso significativo, todavía hay desafíos que se deben abordar en la aplicación de la IA en la agricultura. Algunos de estos desafíos incluyen la falta de datos de calidad y la resistencia de los agricultores a adoptar nuevas tecnologías. Sin embargo, la oportunidad de mejorar la producción y la sostenibilidad agrícola mediante la aplicación de la IA sigue siendo significativa."
  },
  {
    "objectID": "posts/AI/index.html#habilidades-en-programación-y-ciencia-de-datos-para-ingenieros-agrónomos",
    "href": "posts/AI/index.html#habilidades-en-programación-y-ciencia-de-datos-para-ingenieros-agrónomos",
    "title": "Inteligencia artificial en la agricultura",
    "section": "Habilidades en programación y ciencia de datos para ingenieros agrónomos",
    "text": "Habilidades en programación y ciencia de datos para ingenieros agrónomos\nLos ingenieros agrónomos pueden mejorar su trabajo en la agronomía aprendiendo habilidades en programación y ciencia de datos. Estas habilidades incluyen aprender los fundamentos de programación, lenguajes de programación populares, realizar cursos en línea o presenciales, trabajar en proyectos prácticos y participar en eventos y comunidades de programación y ciencia de datos."
  },
  {
    "objectID": "posts/AI/index.html#la-aplicación-de-la-ia-en-la-agricultura",
    "href": "posts/AI/index.html#la-aplicación-de-la-ia-en-la-agricultura",
    "title": "Inteligencia artificial en la agricultura",
    "section": "La aplicación de la IA en la agricultura",
    "text": "La aplicación de la IA en la agricultura\nLa aplicación de la IA en la agricultura puede proporcionar soluciones para mejorar la producción, la calidad y la sostenibilidad de los cultivos. Algunas de las aplicaciones de la IA en la agricultura incluyen el análisis de datos agrícolas para la toma de decisiones, la automatización de procesos agrícolas, el monitoreo y la detección de enfermedades en los cultivos, la identificación de plagas y la predicción de la calidad de los cultivos."
  },
  {
    "objectID": "posts/AI/index.html#conclusión",
    "href": "posts/AI/index.html#conclusión",
    "title": "Inteligencia artificial en la agricultura",
    "section": "Conclusión",
    "text": "Conclusión\nLa IA es una tecnología emergente que tiene el potencial de revolucionar la forma en que se aborda la producción agrícola. Los ingenieros agrónomos pueden mejorar su trabajo en la agronomía aprendiendo habilidades en programación y ciencia de datos, lo que les permitirá aprovechar la IA para mejorar la producción, la calidad y la sostenibilidad de los cultivos. A medida que la tecnología continúa avanzando, es probable que veamos aún más aplicaciones de la IA en la industria agrícola."
  },
  {
    "objectID": "posts/julia/index.html",
    "href": "posts/julia/index.html",
    "title": "JULIA: Lenguaje de programación del futuro",
    "section": "",
    "text": "Julia es un lenguaje de programación de alto nivel, diseñado específicamente para el cómputo científico y la estadística. Fue creado en el año 2009 por un grupo de investigadores en la Universidad de California, Berkeley, y se lanzó oficialmente en el año 2012.\nJulia combina la facilidad de uso de lenguajes como Python y R con el rendimiento de lenguajes de bajo nivel como C y Fortran. Utiliza una sintaxis clara y concisa, similar a la de MATLAB, y cuenta con una amplia variedad de paquetes y librerías especializadas para estadística y ciencias de datos."
  },
  {
    "objectID": "posts/julia/index.html#qué-es-julia",
    "href": "posts/julia/index.html#qué-es-julia",
    "title": "JULIA: Lenguaje de programación del futuro",
    "section": "",
    "text": "Julia es un lenguaje de programación de alto nivel, diseñado específicamente para el cómputo científico y la estadística. Fue creado en el año 2009 por un grupo de investigadores en la Universidad de California, Berkeley, y se lanzó oficialmente en el año 2012.\nJulia combina la facilidad de uso de lenguajes como Python y R con el rendimiento de lenguajes de bajo nivel como C y Fortran. Utiliza una sintaxis clara y concisa, similar a la de MATLAB, y cuenta con una amplia variedad de paquetes y librerías especializadas para estadística y ciencias de datos."
  },
  {
    "objectID": "posts/julia/index.html#ventajas-de-julia-para-estadística",
    "href": "posts/julia/index.html#ventajas-de-julia-para-estadística",
    "title": "JULIA: Lenguaje de programación del futuro",
    "section": "Ventajas de Julia para estadística",
    "text": "Ventajas de Julia para estadística\nJulia tiene varias ventajas que lo hacen especialmente útil para aplicaciones estadísticas y de ciencias de datos:\n\nRendimiento: Julia se ejecuta muy rápidamente gracias a su compilación just-in-time (JIT) y a la capacidad de definir tipos de datos específicos para mejorar el rendimiento. Esto significa que se puede trabajar con grandes conjuntos de datos y cálculos complejos de manera eficiente.\nSintaxis clara y expresiva: Julia tiene una sintaxis intuitiva y fácil de aprender, que se asemeja a la de otros lenguajes populares como MATLAB o Python. Esto hace que sea fácil para los nuevos usuarios comenzar a trabajar con él.\nInteroperabilidad: Julia es capaz de integrarse fácilmente con otros lenguajes como Python, R y C. Esto significa que los usuarios pueden aprovechar las bibliotecas y paquetes de otros lenguajes y combinarlos con Julia para obtener el mejor rendimiento posible.\nPaquetes especializados: Julia cuenta con una amplia variedad de paquetes especializados para estadística y ciencias de datos, como DataFrames.jl, Plots.jl, StatsModels.jl y muchas otras. Esto facilita mucho el trabajo con datos y cálculos estadísticos."
  },
  {
    "objectID": "posts/julia/index.html#ejemplos-de-uso-de-julia-en-estadística",
    "href": "posts/julia/index.html#ejemplos-de-uso-de-julia-en-estadística",
    "title": "JULIA: Lenguaje de programación del futuro",
    "section": "Ejemplos de uso de Julia en estadística",
    "text": "Ejemplos de uso de Julia en estadística\nAquí te presento algunos ejemplos de cómo se puede utilizar Julia para aplicaciones estadísticas:\n\nAnálisis de datos: Julia es especialmente útil para el análisis de grandes conjuntos de datos. Con paquetes como DataFrames.jl, se puede cargar, manipular y analizar datos de manera eficiente y con alta precisión.\nModelado estadístico: Julia cuenta con paquetes especializados para modelado estadístico, como StatsModels.jl y GLM.jl. Estos paquetes permiten ajustar modelos estadísticos complejos y realizar inferencias precisas sobre los datos.\nVisualización de datos: Julia cuenta con varias bibliotecas de visualización de datos, como Plots.jl y Gadfly.jl, que permiten crear visualizaciones de datos de alta calidad de manera eficiente.\nMachine learning: Julia cuenta con paquetes especializados para el aprendizaje automático, como Flux.jl y MLJ.jl, que permiten entrenar y evaluar modelos de aprendizaje automático de manera eficiente y escalable."
  },
  {
    "objectID": "posts/julia/index.html#conclusión",
    "href": "posts/julia/index.html#conclusión",
    "title": "JULIA: Lenguaje de programación del futuro",
    "section": "Conclusión",
    "text": "Conclusión\nJulia es un lenguaje de programación poderoso y versátil que ofrece muchas ventajas para aplicaciones estadísticas y científicas. Con su combinación de rendimiento, sintaxis clara y concisa, y paquetes especializados, Julia se está convirtiendo en una herramienta cada vez más popular para la comunidad estadística y de ciencias de datos.\nA medida que se continúa desarrollando y mejorando, se espera que Julia continúe creciendo en popularidad y se convierta en una herramienta aún más valiosa para aplicaciones estadísticas. Además, dado que Julia está diseñado para ser interoperable con otros lenguajes, se espera que su uso se expanda aún más a medida que se integra en ecosistemas de datos y herramientas existentes."
  },
  {
    "objectID": "posts/julia/index.html#resumen",
    "href": "posts/julia/index.html#resumen",
    "title": "JULIA: Lenguaje de programación del futuro",
    "section": "Resumen",
    "text": "Resumen\nSi estás buscando una herramienta poderosa y eficiente para aplicaciones estadísticas y científicas, definitivamente vale la pena considerar Julia. Con su rendimiento de alta velocidad y sus paquetes especializados, Julia tiene el potencial de acelerar y mejorar significativamente la investigación y el análisis en muchas áreas diferentes."
  },
  {
    "objectID": "posts/tidyexam/index.html",
    "href": "posts/tidyexam/index.html",
    "title": "Ordenar datos con el paquete Tidyverse",
    "section": "",
    "text": "La ordenación de datos es una de las tareas mas importantes despues de concluir la investigación. En las ciencias agrícolas, generalmente la investigación concluye con la evaluación de la cosecha del cultivo. Generalmente nuestros datos pueden estar organizados en un libro de campo; sin embargo, en otras áreas no es así.\nEn este blog replicaré un ejemplo de ordenación de datos con el paquete Tidyverse del libro R4DS. El dataset datos::oms contiene datos de tuberculosis (TB) detallados por año, país, edad, sexo y método de diagnóstico. Los datos provienen del Informe de Tuberculosis de la Organización Mundial de la Salud 2014, disponible en http://www.who.int/tb/country/data/download/en/."
  },
  {
    "objectID": "posts/tidyexam/index.html#introducción",
    "href": "posts/tidyexam/index.html#introducción",
    "title": "Ordenar datos con el paquete Tidyverse",
    "section": "",
    "text": "La ordenación de datos es una de las tareas mas importantes despues de concluir la investigación. En las ciencias agrícolas, generalmente la investigación concluye con la evaluación de la cosecha del cultivo. Generalmente nuestros datos pueden estar organizados en un libro de campo; sin embargo, en otras áreas no es así.\nEn este blog replicaré un ejemplo de ordenación de datos con el paquete Tidyverse del libro R4DS. El dataset datos::oms contiene datos de tuberculosis (TB) detallados por año, país, edad, sexo y método de diagnóstico. Los datos provienen del Informe de Tuberculosis de la Organización Mundial de la Salud 2014, disponible en http://www.who.int/tb/country/data/download/en/."
  },
  {
    "objectID": "posts/tidyexam/index.html#procedimiento-de-ordenación",
    "href": "posts/tidyexam/index.html#procedimiento-de-ordenación",
    "title": "Ordenar datos con el paquete Tidyverse",
    "section": "Procedimiento de ordenación",
    "text": "Procedimiento de ordenación\n\nCargar el paquete tidyverse\nEl primer paso es instalar el paquete tidyverse del CRAN de R. Posterior a esto es cargar el paquete en nuestra consola de R.\n\nlibrary(tidyverse)\n#En el paquete datos se encuentra la base de datos para este ejemplo\nlibrary(datos)\n\nA continuación observación el estado de los datos de oms.\n\nhead(oms)\n\n# A tibble: 6 × 60\n  pais       iso2  iso3   anio nuevos_…¹ nuevo…² nuevo…³ nuevo…⁴ nuevo…⁵ nuevo…⁶\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;int&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1 Afganistán AF    AFG    1980        NA      NA      NA      NA      NA      NA\n2 Afganistán AF    AFG    1981        NA      NA      NA      NA      NA      NA\n3 Afganistán AF    AFG    1982        NA      NA      NA      NA      NA      NA\n4 Afganistán AF    AFG    1983        NA      NA      NA      NA      NA      NA\n5 Afganistán AF    AFG    1984        NA      NA      NA      NA      NA      NA\n6 Afganistán AF    AFG    1985        NA      NA      NA      NA      NA      NA\n# … with 50 more variables: nuevos_fpp_h65 &lt;int&gt;, nuevos_fpp_m014 &lt;int&gt;,\n#   nuevos_fpp_m1524 &lt;int&gt;, nuevos_fpp_m2534 &lt;int&gt;, nuevos_fpp_m3544 &lt;int&gt;,\n#   nuevos_fpp_m4554 &lt;int&gt;, nuevos_fpp_m5564 &lt;int&gt;, nuevos_fpp_m65 &lt;int&gt;,\n#   nuevos_fpn_h014 &lt;int&gt;, nuevos_fpn_h1524 &lt;int&gt;, nuevos_fpn_h2534 &lt;int&gt;,\n#   nuevos_fpn_h3544 &lt;int&gt;, nuevos_fpn_h4554 &lt;int&gt;, nuevos_fpn_h5564 &lt;int&gt;,\n#   nuevos_fpn_h65 &lt;int&gt;, nuevos_fpn_m014 &lt;int&gt;, nuevos_fpn_m1524 &lt;int&gt;,\n#   nuevos_fpn_m2534 &lt;int&gt;, nuevos_fpn_m3544 &lt;int&gt;, nuevos_fpn_m4554 &lt;int&gt;, …\n\n\nEn la salida se observa un ejemplo muy típico de una base de datos de la vida real. Contiene columnas redundantes, códigos extraños de variables y muchos valores faltantes. Practicamente, la base de datos oms está desordenado, por tanto, se necesita ordenarlo de manera sencilla con tidyverse.\n\n\nPasos de ordenación\nNecesitamos agrupar todas las columnas desde nuevos_fpp_h014 hasta recaidas_m65. No sabemos aún qué representa esto, por lo que le daremos el nombre genérico de \"clave\". Sabemos que las celdas representan la cuenta de casos, por lo que usaremos la variable casos.\nExisten múltiples valores faltantes en la representación actual, por lo que de momento usaremos na.rm para centrarnos en los valores que están presentes.\n\noms1 &lt;- oms %&gt;%\n  pivot_longer(\n    cols = nuevos_fpp_h014:nuevosrecaida_m65, \n    names_to = \"clave\", \n    values_to = \"casos\", \n    values_drop_na = TRUE\n  )\noms1\n\n# A tibble: 76,046 × 6\n   pais       iso2  iso3   anio clave            casos\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt;            &lt;int&gt;\n 1 Afganistán AF    AFG    1997 nuevos_fpp_h014      0\n 2 Afganistán AF    AFG    1997 nuevos_fpp_h1524    10\n 3 Afganistán AF    AFG    1997 nuevos_fpp_h2534     6\n 4 Afganistán AF    AFG    1997 nuevos_fpp_h3544     3\n 5 Afganistán AF    AFG    1997 nuevos_fpp_h4554     5\n 6 Afganistán AF    AFG    1997 nuevos_fpp_h5564     2\n 7 Afganistán AF    AFG    1997 nuevos_fpp_h65       0\n 8 Afganistán AF    AFG    1997 nuevos_fpp_m014      5\n 9 Afganistán AF    AFG    1997 nuevos_fpp_m1524    38\n10 Afganistán AF    AFG    1997 nuevos_fpp_m2534    36\n# … with 76,036 more rows\n\n\nPara visualizar el conteo de valores en la nueva columna clave:\n\noms1 %&gt;%\n  count(clave)\n\n# A tibble: 56 × 2\n   clave               n\n   &lt;chr&gt;           &lt;int&gt;\n 1 nuevos_ep_h014   1038\n 2 nuevos_ep_h1524  1026\n 3 nuevos_ep_h2534  1020\n 4 nuevos_ep_h3544  1024\n 5 nuevos_ep_h4554  1020\n 6 nuevos_ep_h5564  1015\n 7 nuevos_ep_h65    1018\n 8 nuevos_ep_m014   1032\n 9 nuevos_ep_m1524  1021\n10 nuevos_ep_m2534  1021\n# … with 46 more rows\n\n\nPara entender el significado de cada variable, se dispone de un diccionario de datos a mano. Este dice lo siguiente:\n\nLo que aparece antes del primer _ en las columnas indica si la columna contiene casos nuevos o antiguos de tuberculosis. En este dataset, cada columna contiene nuevos casos.\nLo que aparece luego de indicar si se refiere casos nuevos o antiguos es el tipo de tuberculosis:\n\n\nrecaida se refiere a casos reincidentes\nep se refiere a tuberculosis extra pulmonar\nfpn se refiere a casos de tuberculosis pulmonar que no se pueden detectar mediante examen de frotis pulmonar (frotis pulmonar negativo)\nfpp se refiere a casos de tuberculosis pulmonar que se pueden detectar mediante examen de frotis pulmonar (frotis pulmonar positivo)\n\n\nLa letra que aparece después del último _ se refiere al sexo de los pacientes. El conjunto de datos agrupa en hombres (h) y mujeres (m).\nLos números finales se refieren al grupo etareo que se ha organizado en siete categorías:\n\n\n014 = 0 - 14 años de edad\n1524 = 15 – 24 años de edad\n2534 = 25 – 34 años de edad\n3544 = 35 – 44 años de edad\n4554 = 45 – 54 años de edad\n5564 = 55 – 64 años de edad\n65 = 65 o más años de edad\n\nNecesitamos hacer un pequeño cambio al formato de los nombres de las columnas: desafortunadamente lo nombres de las columnas son ligeramente inconsistentes debido a que en lugar de nuevos_recaida tenemos nuevosrecaida (es difícil darse cuenta de esto en esta parte, pero si no lo arreglas habrá errores en los pasos siguientes). Para esto, la idea básica es bastante simple: reemplazar los caracteres “nuevosrecaida” por “nuevos_recaida”. Esto genera nombres de variables consistentes.\n\noms2 &lt;- oms1 %&gt;%\n  mutate(clave = stringr::str_replace(clave, \"nuevosrecaida\", \"nuevos_recaida\"))\noms2\n\n# A tibble: 76,046 × 6\n   pais       iso2  iso3   anio clave            casos\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt;            &lt;int&gt;\n 1 Afganistán AF    AFG    1997 nuevos_fpp_h014      0\n 2 Afganistán AF    AFG    1997 nuevos_fpp_h1524    10\n 3 Afganistán AF    AFG    1997 nuevos_fpp_h2534     6\n 4 Afganistán AF    AFG    1997 nuevos_fpp_h3544     3\n 5 Afganistán AF    AFG    1997 nuevos_fpp_h4554     5\n 6 Afganistán AF    AFG    1997 nuevos_fpp_h5564     2\n 7 Afganistán AF    AFG    1997 nuevos_fpp_h65       0\n 8 Afganistán AF    AFG    1997 nuevos_fpp_m014      5\n 9 Afganistán AF    AFG    1997 nuevos_fpp_m1524    38\n10 Afganistán AF    AFG    1997 nuevos_fpp_m2534    36\n# … with 76,036 more rows\n\n\nUna vez reemplazado, nos facilita separar los valores en cada código aplicando separate() dos veces. La primera aplicación dividirá los códigos en cada _.\n\noms3 &lt;- oms2 %&gt;%\n  separate(clave, c(\"nuevos\", \"tipo\", \"sexo_edad\"), sep = \"_\")\noms3\n\n# A tibble: 76,046 × 8\n   pais       iso2  iso3   anio nuevos tipo  sexo_edad casos\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;\n 1 Afganistán AF    AFG    1997 nuevos fpp   h014          0\n 2 Afganistán AF    AFG    1997 nuevos fpp   h1524        10\n 3 Afganistán AF    AFG    1997 nuevos fpp   h2534         6\n 4 Afganistán AF    AFG    1997 nuevos fpp   h3544         3\n 5 Afganistán AF    AFG    1997 nuevos fpp   h4554         5\n 6 Afganistán AF    AFG    1997 nuevos fpp   h5564         2\n 7 Afganistán AF    AFG    1997 nuevos fpp   h65           0\n 8 Afganistán AF    AFG    1997 nuevos fpp   m014          5\n 9 Afganistán AF    AFG    1997 nuevos fpp   m1524        38\n10 Afganistán AF    AFG    1997 nuevos fpp   m2534        36\n# … with 76,036 more rows\n\n\nA continuación podemos eliminar la columna nuevos, ya que es constante en este dataset. Además eliminaremos iso2 e iso3 ya que son redundantes.\n\noms3 %&gt;%\n  count(nuevos)\n\n# A tibble: 1 × 2\n  nuevos     n\n  &lt;chr&gt;  &lt;int&gt;\n1 nuevos 76046\n\noms4 &lt;- oms3 %&gt;%\n  select(-nuevos, -iso2, -iso3)\n\nLuego separamos sexo_edad en sexo y edad dividiendo luego del primer carácter:\n\noms5 &lt;- oms4 %&gt;%\n  separate(sexo_edad, c(\"sexo\", \"edad\"), sep = 1)\noms5\n\n# A tibble: 76,046 × 6\n   pais        anio tipo  sexo  edad  casos\n   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 Afganistán  1997 fpp   h     014       0\n 2 Afganistán  1997 fpp   h     1524     10\n 3 Afganistán  1997 fpp   h     2534      6\n 4 Afganistán  1997 fpp   h     3544      3\n 5 Afganistán  1997 fpp   h     4554      5\n 6 Afganistán  1997 fpp   h     5564      2\n 7 Afganistán  1997 fpp   h     65        0\n 8 Afganistán  1997 fpp   m     014       5\n 9 Afganistán  1997 fpp   m     1524     38\n10 Afganistán  1997 fpp   m     2534     36\n# … with 76,036 more rows\n\n\n¡Ahora la base de datos oms está ordenado!"
  },
  {
    "objectID": "posts/tidyexam/index.html#resumen",
    "href": "posts/tidyexam/index.html#resumen",
    "title": "Ordenar datos con el paquete Tidyverse",
    "section": "Resumen",
    "text": "Resumen\nEn la anterior sección se hizo el procedimiento de ordenación paso a paso, asignando los resultados intermedios a nuevas variables. Esta no es la forma típica de trabajo. En realidad, los códigos debería ser de manera incremental usando pipes (\"%&gt;%):\n\nfsdata&lt;- oms %&gt;%\n  pivot_longer(\n    cols = nuevos_fpp_h014:nuevosrecaida_m65,\n    names_to = \"clave\", \n    values_to = \"valor\", \n    values_drop_na = TRUE) %&gt;%\n  mutate(clave = stringr::str_replace(clave, \"nuevosrecaida\", \"nuevos_recaida\")) %&gt;%\n  separate(clave, c(\"nuevos\", \"tipo\", \"sexo_edad\")) %&gt;%\n  select(-nuevos, -iso2, -iso3) %&gt;%\n  separate(sexo_edad, c(\"sexo\", \"edad\"), sep = 1)\nfsdata\n\n# A tibble: 76,046 × 6\n   pais        anio tipo  sexo  edad  valor\n   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 Afganistán  1997 fpp   h     014       0\n 2 Afganistán  1997 fpp   h     1524     10\n 3 Afganistán  1997 fpp   h     2534      6\n 4 Afganistán  1997 fpp   h     3544      3\n 5 Afganistán  1997 fpp   h     4554      5\n 6 Afganistán  1997 fpp   h     5564      2\n 7 Afganistán  1997 fpp   h     65        0\n 8 Afganistán  1997 fpp   m     014       5\n 9 Afganistán  1997 fpp   m     1524     38\n10 Afganistán  1997 fpp   m     2534     36\n# … with 76,036 more rows"
  },
  {
    "objectID": "posts/tidyexam/index.html#conclusión",
    "href": "posts/tidyexam/index.html#conclusión",
    "title": "Ordenar datos con el paquete Tidyverse",
    "section": "Conclusión",
    "text": "Conclusión\nEs un ejemplo muy bueno para practicar y usar las diferentes funciones de tidyverse en la ordenación de datos."
  },
  {
    "objectID": "posts/chile/index.html",
    "href": "posts/chile/index.html",
    "title": "Un Viaje de Sueños: Camino a la Universidad Austral de Chile",
    "section": "",
    "text": "Hace un año, mi vida transcurría entre las investigaciones del Instituto Nacional de Innovación Agropecuaria y Forestal (INIAF). Sin embargo, un anhelo latía en mi interior: estudiar un posgrado en el extranjero. Este sueño se consolidó durante un viaje transformador a Corea del Sur, financiado por KOICA, el Gobierno de Corea del Sur. Allí, mi determinación se afianzó y, al regresar, me dediqué a preparar meticulosamente mi postulación a la Beca AGCID de la Embajada de la República de Chile.\n\n\n\nFranklin junto a Mustafa de Gambia"
  },
  {
    "objectID": "posts/chile/index.html#un-anhelo-que-cobra-vida",
    "href": "posts/chile/index.html#un-anhelo-que-cobra-vida",
    "title": "Un Viaje de Sueños: Camino a la Universidad Austral de Chile",
    "section": "",
    "text": "Hace un año, mi vida transcurría entre las investigaciones del Instituto Nacional de Innovación Agropecuaria y Forestal (INIAF). Sin embargo, un anhelo latía en mi interior: estudiar un posgrado en el extranjero. Este sueño se consolidó durante un viaje transformador a Corea del Sur, financiado por KOICA, el Gobierno de Corea del Sur. Allí, mi determinación se afianzó y, al regresar, me dediqué a preparar meticulosamente mi postulación a la Beca AGCID de la Embajada de la República de Chile.\n\n\n\nFranklin junto a Mustafa de Gambia"
  },
  {
    "objectID": "posts/chile/index.html#un-sueño-hecho-realidad",
    "href": "posts/chile/index.html#un-sueño-hecho-realidad",
    "title": "Un Viaje de Sueños: Camino a la Universidad Austral de Chile",
    "section": "Un sueño hecho realidad",
    "text": "Un sueño hecho realidad\nAl cerrar el año 2023, llegó la noticia que cambiaría mi vida: ¡había sido seleccionado para cursar el Magíster en Ciencias Vegetales en la prestigiosa Universidad Austral de Chile! La emoción me invadió al dar el primer paso hacia esta nueva etapa académica."
  },
  {
    "objectID": "posts/chile/index.html#matriculación-y-agradecimiento",
    "href": "posts/chile/index.html#matriculación-y-agradecimiento",
    "title": "Un Viaje de Sueños: Camino a la Universidad Austral de Chile",
    "section": "Matriculación y agradecimiento",
    "text": "Matriculación y agradecimiento\nMatriculado oficialmente en esta reconocida institución, siento un profundo agradecimiento por la oportunidad que se me presenta. La expectativa de sumergirme en el vasto campo de las ciencias vegetales despierta en mí una pasión insaciable por el conocimiento. La oportunidad de adquirir nuevas perspectivas y habilidades es un privilegio del que me siento profundamente agradecido.\n\n\n\nFacultad de Ciencias, donde tendré clases de Bioinformática aplicada a la transcriptómica"
  },
  {
    "objectID": "posts/chile/index.html#un-honor-y-una-guía-invaluable",
    "href": "posts/chile/index.html#un-honor-y-una-guía-invaluable",
    "title": "Un Viaje de Sueños: Camino a la Universidad Austral de Chile",
    "section": "Un honor y una guía invaluable",
    "text": "Un honor y una guía invaluable\nContar con la Dra. Daniela Bustos Korts como mentora de tesis es un verdadero honor. Su trayectoria, que incluye un doctorado y postdoctorado en la Universidad de Wageningen, sumada a su dedicación y experiencia, son una invaluable guía en mi camino académico. Estoy ansioso por aprender de ella y colaborar en este apasionante proyecto.\n\n\n\nProf. Daniela Bustos y Franklin Santos"
  },
  {
    "objectID": "posts/chile/index.html#reconocimiento-y-compromiso",
    "href": "posts/chile/index.html#reconocimiento-y-compromiso",
    "title": "Un Viaje de Sueños: Camino a la Universidad Austral de Chile",
    "section": "Reconocimiento y compromiso",
    "text": "Reconocimiento y compromiso\nQuiero expresar mi sincero agradecimiento a la Beca AGCID Chile, cuyo apoyo ha hecho posible que continúe persiguiendo mis sueños académicos. Su respaldo es un pilar fundamental en este viaje, y estoy comprometido a aprovechar al máximo esta oportunidad."
  },
  {
    "objectID": "posts/chile/index.html#un-desafío-con-entusiasmo-y-determinación",
    "href": "posts/chile/index.html#un-desafío-con-entusiasmo-y-determinación",
    "title": "Un Viaje de Sueños: Camino a la Universidad Austral de Chile",
    "section": "Un desafío con entusiasmo y determinación",
    "text": "Un desafío con entusiasmo y determinación\nEstoy listo para abrazar este desafío con entusiasmo y determinación. ¡Aquí voy, Universidad Austral de Chile, listo para crecer, aprender y contribuir al fascinante mundo de las ciencias vegetales!"
  },
  {
    "objectID": "posts/chile/index.html#agradecimiento-final-e-inspiración",
    "href": "posts/chile/index.html#agradecimiento-final-e-inspiración",
    "title": "Un Viaje de Sueños: Camino a la Universidad Austral de Chile",
    "section": "Agradecimiento final e inspiración",
    "text": "Agradecimiento final e inspiración\nAgradezco a Dios, a mi familia y amigos por su constante apoyo. Que esta pequeña historia inspire a otros profesionales a perseguir sus sueños académicos en el extranjero. Recuerden: donde hay voluntad, hay camino, y con la guía divina, los límites son solo el comienzo de nuevas posibilidades. ¡Adelante, valientes aspirantes al mundo de las ciencias!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Franklin Santos",
    "section": "",
    "text": "Franklin Santos is a Master’s student in Plant Sciences at the Austral University of Chile. He is currently part of the GEM (Genotype by Environment by Management) research team, led by the distinguished scientist Daniela Bustos. His research focuses on flowering time modeling and prediction to adapt crops to emerging climate challenges."
  },
  {
    "objectID": "about.html#biography",
    "href": "about.html#biography",
    "title": "Franklin Santos",
    "section": "",
    "text": "Franklin Santos is a Master’s student in Plant Sciences at the Austral University of Chile. He is currently part of the GEM (Genotype by Environment by Management) research team, led by the distinguished scientist Daniela Bustos. His research focuses on flowering time modeling and prediction to adapt crops to emerging climate challenges."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Franklin Santos",
    "section": "Education",
    "text": "Education\n\nStatistics with R | 2020\n\nDuke University | Coursera Specialization\n\nAnimal Breeding and Genetics | 2020\n\nWageningen University & Research | edX Professional Certificate\n\nData Science | 2020\n\nJohns Hopkins University | Coursera Specialization\n\nAgronomist Engineer, B.Sc. | 2017\n\nEl Alto Public University"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Franklin Santos",
    "section": "Experience",
    "text": "Experience\n\nINIAF | Research and Technology Generation Professional| February 2023 - December 2023\nINIAF | Technical extensionist | May 2022 - December 2022\nINIAF | Research Technician | January 2018 - June 2020\nGAM-Licoma | Director of Agricultural Development | June 2016 - Dec 2017\nINE | Municipal Census Chief | August 2013 - November 2013"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "Franklin Santos",
    "section": "Interests",
    "text": "Interests\n\nGxExM modelling\nFlowering time prediction\nData Science\nR programming"
  },
  {
    "objectID": "posts/dca/index.html",
    "href": "posts/dca/index.html",
    "title": "Diseño Completamente al Azar",
    "section": "",
    "text": "El diseño más sencillo desde el punto de vista de la asignación de unidades experimentales a los tratamientos.\n\n\n\nDistribución normal de residuos\nHomogeneidad de varianzas\nObservaciones independientes\n\n\n\n\n\nSimplicidad\nEvita hacer suposiciones estadísticas dudosas.\nPocas violaciones de los supuestos.\nEl análisis estadístico es sencillo.\n\n\n\n\n\nFalta de precisión o incapacidad para estimar con precisión los efectos del tratamiento."
  },
  {
    "objectID": "posts/dca/index.html#diseño-completamente-aleatorizado-dca",
    "href": "posts/dca/index.html#diseño-completamente-aleatorizado-dca",
    "title": "Diseño Completamente al Azar",
    "section": "",
    "text": "El diseño más sencillo desde el punto de vista de la asignación de unidades experimentales a los tratamientos.\n\n\n\nDistribución normal de residuos\nHomogeneidad de varianzas\nObservaciones independientes\n\n\n\n\n\nSimplicidad\nEvita hacer suposiciones estadísticas dudosas.\nPocas violaciones de los supuestos.\nEl análisis estadístico es sencillo.\n\n\n\n\n\nFalta de precisión o incapacidad para estimar con precisión los efectos del tratamiento."
  },
  {
    "objectID": "posts/dca/index.html#paquetes",
    "href": "posts/dca/index.html#paquetes",
    "title": "Diseño Completamente al Azar",
    "section": "Paquetes",
    "text": "Paquetes\n\n# (install &) load packages\npacman::p_load(\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  multcomp,\n  multcompView,\n  tidyverse,\n  lme4)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \n\n[conflicted] Will prefer dplyr::filter over any other package.\n\nconflicts_prefer(dplyr::select)\n\n[conflicted] Will prefer dplyr::select over any other package."
  },
  {
    "objectID": "posts/dca/index.html#base-de-datos",
    "href": "posts/dca/index.html#base-de-datos",
    "title": "Diseño Completamente al Azar",
    "section": "Base de datos",
    "text": "Base de datos\nEste ejemplo procede del “Ejemplo 4.3” del material didáctico “Métodos cuantitativos en biociencias (3402-420)” del Prof. Dr. Hans-Peter Piepho. Considera los datos publicados en la p.52 de Mead, Curnow, y Hasted (2002) de un ensayo de rendimiento con melones. El ensayo tenía 4 variedades de melón (variedad). Cada variedad se probó en seis parcelas de campo. La asignación de los tratamientos (variedades) a las unidades experimentales (parcelas) fue completamente al azar. Así, el experimento se planteó como un diseño completamente aleatorizado (DCA).\n\n# data is available online:\npath &lt;- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/Mead1993.csv\"\n\ndat &lt;- read_csv(path) # use path from above\n\nRows: 24 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): variety\ndbl (3): yield, row, col\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(dat)\n\n# A tibble: 6 × 4\n  variety yield   row   col\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 v1       25.1     4     2\n2 v1       17.2     1     6\n3 v1       26.4     4     1\n4 v1       16.1     1     4\n5 v1       22.2     1     2\n6 v1       15.9     2     4\n\n\n\nFormato\nAntes de nada, la variedad de columna debe codificarse como un factor, ya que R por defecto la codifica como una variable de carácter. Hay múltiples maneras de hacer esto - aquí hay dos:\n\ndat &lt;- dat %&gt;% \n  mutate(variety = as.factor(variety))\n\n\ndat &lt;- dat %&gt;% \n  mutate(across(variety, ~ as.factor(.x)))"
  },
  {
    "objectID": "posts/dca/index.html#distribución-de-los-tratamientos",
    "href": "posts/dca/index.html#distribución-de-los-tratamientos",
    "title": "Diseño Completamente al Azar",
    "section": "Distribución de los tratamientos",
    "text": "Distribución de los tratamientos\nDado que se trata de un experimento que se estableció con un determinado diseño experimental (= un diseño completamente aleatorizado; DCA) - tiene sentido obtener también un plano de campo. Esto se puede hacer a través de desplot() de {desplot}:\n\ndesplot(\n  data = dat, \n  flip = TRUE, # row 1 on top, not on bottom\n  form = variety ~ col + row, # fill color per variety\n  text = variety, # variety names per plot\n  cex = 1, # variety names: font size\n  main = \"Field layout\", # plot title\n  show.key = FALSE # hide legend \n  )     \n\n\n\n\n\n\n\n\n\ndesplot(\n  data = dat, \n  flip = TRUE, # row 1 on top, not on bottom\n  form = yield ~ col + row, # fill color per variety\n  text = variety, # variety names per plot\n  cex = 1, # variety names: font size\n  main = \"Yield per plot\", # plot title\n  show.key = FALSE # hide legend \n  )"
  },
  {
    "objectID": "posts/dca/index.html#modelo",
    "href": "posts/dca/index.html#modelo",
    "title": "Diseño Completamente al Azar",
    "section": "Modelo",
    "text": "Modelo\nPor último, podemos decidir ajustar un modelo lineal con el rendimiento como variable de respuesta y efectos (fijos) de variedad.\n\nmod &lt;- lm(yield ~ variety, data = dat)"
  },
  {
    "objectID": "posts/dca/index.html#anova",
    "href": "posts/dca/index.html#anova",
    "title": "Diseño Completamente al Azar",
    "section": "ANOVA",
    "text": "ANOVA\nBasándonos en nuestro modelo, podemos realizar un ANOVA:\n\nANOVA &lt;- anova(mod)\nANOVA\n\nAnalysis of Variance Table\n\nResponse: yield\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nvariety    3 1291.48  430.49  23.418 9.439e-07 ***\nResiduals 20  367.65   18.38                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/dca/index.html#comparación-de-medias",
    "href": "posts/dca/index.html#comparación-de-medias",
    "title": "Diseño Completamente al Azar",
    "section": "Comparación de medias",
    "text": "Comparación de medias\nAdemás de un ANOVA, también se pueden comparar las medias de rendimiento ajustadas entre variedades mediante pruebas post hoc (prueba t, prueba de Tukey, etc.).\n\nmean_comp &lt;- mod %&gt;% \n  emmeans(specs = ~ variety) %&gt;% # adj. mean per variety\n  cld(adjust = \"Tukey\", Letters = letters) # compact letter display (CLD)\n\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\n\nmean_comp\n\n variety emmean   SE df lower.CL upper.CL .group\n v3        19.5 1.75 20     14.7     24.3  a    \n v1        20.5 1.75 20     15.7     25.3  a    \n v4        29.9 1.75 20     25.1     34.7   b   \n v2        37.4 1.75 20     32.6     42.2    c  \n\nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 4 estimates \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nTenga en cuenta que si desea ver los contrastes/diferencias individuales subyacentes entre medias ajustadas, simplemente añada details = TRUE a la sentencia cld(). Además, consulte el artículo de resumen “Visualización compacta de letras”.\nPor último, podemos crear un gráfico que muestre tanto los datos brutos como los resultados, es decir, las comparaciones de las medias ajustadas que se basan en el modelo lineal.\n\nmy_caption &lt;- \"Los puntos negros representan los datos brutos. Los puntos rojos y las barras de error representan medias ajustadas con límites de confianza del 95% por variedad. Las medias seguidas de una letra común no son significativamente diferentes según la prueba de Tukey.\"\n\nggplot() +\n  aes(x = variety) +\n  # black dots representing the raw data\n  geom_point(\n    data = dat,\n    aes(y = yield)\n  ) +\n  # red dots representing the adjusted means\n  geom_point(\n    data = mean_comp,\n    aes(y = emmean),\n    color = \"red\",\n    position = position_nudge(x = 0.1)\n  ) +\n  # red error bars representing the confidence limits of the adjusted means\n  geom_errorbar(\n    data = mean_comp,\n    aes(ymin = lower.CL, ymax = upper.CL),\n    color = \"red\",\n    width = 0.1,\n    position = position_nudge(x = 0.1)\n  ) +\n  # red letters \n  geom_text(\n    data = mean_comp,\n    aes(y = emmean, label = str_trim(.group)),\n    color = \"red\",\n    position = position_nudge(x = 0.2),\n    hjust = 0\n  ) +\n  scale_x_discrete(\n    name = \"Variety\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  theme_classic() +\n  labs(caption = my_caption) +\n  theme(plot.caption = element_textbox_simple(margin = margin(t = 5)),\n        plot.caption.position = \"plot\")\n\n\n\n\n\n\n\n\nPara calcular la heredabilidad y los Best Linear Unbiased Estimators (BLUE) en R, puedes seguir los pasos que se detallan a continuación. Utilizaremos el paquete lme4 para ajustar un modelo de efectos mixtos y obtener las estimaciones necesarias."
  },
  {
    "objectID": "posts/conglomerados/index.html",
    "href": "posts/conglomerados/index.html",
    "title": "Potato Diversity Analysis:A Step-by-Step Guide",
    "section": "",
    "text": "In this article, we will delve into a comprehensive approach to analyzing the genetic diversity of potato using various statistical and multivariate analysis methods in R. The analysis encompasses techniques such as k-means clustering, hierarchical clustering, DBSCAN, and Principal Component Analysis (PCA), supported by clear visualizations to facilitate the interpretation of results. This guide aims to provide a robust framework for researchers and practitioners working in the field of plant genetics and diversity analysis."
  },
  {
    "objectID": "posts/conglomerados/index.html#introduction",
    "href": "posts/conglomerados/index.html#introduction",
    "title": "Potato Diversity Analysis:A Step-by-Step Guide",
    "section": "",
    "text": "In this article, we will delve into a comprehensive approach to analyzing the genetic diversity of potato using various statistical and multivariate analysis methods in R. The analysis encompasses techniques such as k-means clustering, hierarchical clustering, DBSCAN, and Principal Component Analysis (PCA), supported by clear visualizations to facilitate the interpretation of results. This guide aims to provide a robust framework for researchers and practitioners working in the field of plant genetics and diversity analysis."
  },
  {
    "objectID": "posts/conglomerados/index.html#data-loading-and-preparation",
    "href": "posts/conglomerados/index.html#data-loading-and-preparation",
    "title": "Potato Diversity Analysis:A Step-by-Step Guide",
    "section": "Data Loading and Preparation",
    "text": "Data Loading and Preparation\nThe first step in any analysis is to load and prepare the data. Here, we use R to read the data from an Excel file, select relevant columns, and scale the data. Scaling ensures that each variable contributes equally to the analysis, which is particularly important in clustering and PCA.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Load the dataset from an Excel file\ndf &lt;- read_excel(\"/Users/franklin/Documents/R/myblog/posts/conglomerados/DATOS_R_JUAN JOSE.xlsx\")\nhead(df)\n\n# A tibble: 6 × 54\n   `N°` codigo_colecta nombre_comun  Departamento Provincia Municipio Localidad\n  &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    \n1     1 JCH 001        JACHA         POTOSI       Bustillos Chayanta  Jacha    \n2     2 JCH 002        TANI TANI     POTOSI       Bustillos Chayanta  Jacha    \n3     3 JCH 003        TUNANTE       POTOSI       Bustillos Chayanta  Jacha    \n4     4 JCH 004        QOLLU  PEPINO POTOSI       Bustillos Chayanta  Jacha    \n5     5 JCH 005        QHOLLU YURAQ  POTOSI       Bustillos Chayanta  Jacha    \n6     6 JCH 006        QHOLLU PUCA   POTOSI       Bustillos Chayanta  Jacha    \n# ℹ 47 more variables: Latitud &lt;dbl&gt;, Longitud &lt;dbl&gt;, Altitud &lt;dbl&gt;,\n#   CAMPAÑA &lt;dbl&gt;, REGENERACION &lt;chr&gt;, BLOQ &lt;dbl&gt;, SURC &lt;dbl&gt;, RDTO &lt;dbl&gt;,\n#   NTUBER &lt;dbl&gt;, N_caja &lt;dbl&gt;, Ev_Almacen &lt;chr&gt;, HPL &lt;dbl&gt;, CTL &lt;dbl&gt;,\n#   FAL &lt;dbl&gt;, TDS &lt;dbl&gt;, NFL &lt;dbl&gt;, NIF &lt;dbl&gt;, NIP &lt;dbl&gt;, GFL &lt;dbl&gt;,\n#   FCL &lt;dbl&gt;, CPF &lt;dbl&gt;, INT &lt;dbl&gt;, CSF &lt;dbl&gt;, DCS &lt;dbl&gt;, CPL &lt;dbl&gt;,\n#   CLZ &lt;dbl&gt;, PAN &lt;dbl&gt;, PPT &lt;dbl&gt;, CBY &lt;dbl&gt;, FBY &lt;dbl&gt;, FGR &lt;dbl&gt;,\n#   FRA &lt;dbl&gt;, POJ &lt;dbl&gt;, CPPL &lt;dbl&gt;, INTC &lt;dbl&gt;, CSP &lt;dbl&gt;, DSCPL &lt;dbl&gt;, …\n\n# Column selection for the analysis (excluding TDS due to lack of variation)\ndb &lt;- df %&gt;% \n  select(2, 4, 15, 16, 19:21, 23:47)\n\n# Data preparation: remove non-numeric columns and scale the data\nsdf &lt;- db %&gt;% \n  select(-2) %&gt;% \n  column_to_rownames(\"codigo_colecta\")\n\nsdf &lt;- scale(sdf)\nhead(sdf)\n\n              RDTO     NTUBER         HPL        CTL        FAL        NFL\nJCH 001 -1.0364935  2.3631177 -2.33535670  1.5974622  0.2021708 -0.2949048\nJCH 002 -1.0332533  1.3262372 -2.33535670  2.3526261 -3.0325614  1.3193108\nJCH 003 -0.8518015  0.2422258 -1.21610741 -0.6680296 -1.4151953 -0.2949048\nJCH 004 -0.4208535 -1.4309223 -0.09685811  0.0871343  0.2021708  2.9335265\nJCH 005 -1.2114649 -1.1010057 -0.09685811 -0.6680296 -1.4151953 -1.9091204\nJCH 006 -0.5277805 -0.2526490 -0.09685811  0.0871343  0.2021708 -0.2949048\n               NIF        NIP        GFL        FCL        CPF         INT\nJCH 001  2.0689872  1.3677450  2.6330006  0.3720828  0.3365174  0.08133921\nJCH 002  0.3475899  1.3677450  2.6330006 -2.7236462  0.3365174 -1.12712898\nJCH 003 -1.3738075 -0.7241003 -0.3761429  0.3720828 -0.4081168 -1.12712898\nJCH 004  2.0689872 -0.7241003 -0.3761429  0.3720828  0.3365174  1.28980739\nJCH 005 -1.3738075 -0.7241003 -0.3761429  0.3720828  0.3365174  0.08133921\nJCH 006 -1.3738075  1.3677450 -0.3761429  0.3720828  0.3365174 -1.12712898\n               CSF        DCS        CPL        CLZ        PAN         PPT\nJCH 001 -0.2849313 -0.3623917 -0.5231525 -0.4795775 -0.4143268  0.09001093\nJCH 002 -0.8140895 -1.2189538  1.4919534  0.9454527 -0.4143268  0.09001093\nJCH 003 -0.2849313  1.3507325 -1.5307055 -1.1920925 -0.4143268 -1.31415965\nJCH 004 -0.2849313 -0.3623917  1.4919534  0.9454527 -0.4143268  1.02612466\nJCH 005 -0.2849313 -0.3623917  0.9881770 -0.4795775  2.9002873 -0.84610279\nJCH 006 -0.2849313 -0.3623917 -0.5231525 -0.4795775  2.9002873  1.02612466\n               CBY        FBY       FGR        FRA       POJ       CPPL\nJCH 001  0.6475703 -0.2236502  0.364163 -0.6069687 -0.558308 -0.2817993\nJCH 002  0.6475703 -0.2236502  0.364163 -0.6069687 -0.558308 -1.3284824\nJCH 003  0.6475703 -0.2236502  0.364163 -0.6069687 -2.086309  0.4159894\nJCH 004  0.6475703 -0.2236502 -1.568130 -0.2798975 -0.558308 -0.9795880\nJCH 005 -1.2362705 -0.2236502  1.137080  0.7013162 -0.558308 -0.9795880\nJCH 006 -0.7653103 -0.2236502  1.137080  0.7013162 -2.086309  0.7648838\n              INTC        CSP      DSCPL        CPP       CSPL       DCSP\nJCH 001 -0.9149821  0.4009646  0.1217388  0.8023403 -0.4827367 -0.4143496\nJCH 002 -0.9149821 -0.2506029  0.1217388  0.8023403 -0.4827367 -0.4143496\nJCH 003  1.4940847 -1.8795218 -2.0801452  0.8023403 -0.4827367 -0.4143496\nJCH 004  0.2895513  0.4009646  0.1217388  2.4070210 -0.4827367 -0.4143496\nJCH 005  0.2895513  0.4009646 -1.5296742 -0.8023403 -0.4827367 -0.4143496\nJCH 006  0.2895513 -0.5763867  1.7731517  0.8023403 -0.4827367 -0.4143496"
  },
  {
    "objectID": "posts/conglomerados/index.html#clustering-k-means-method",
    "href": "posts/conglomerados/index.html#clustering-k-means-method",
    "title": "Potato Diversity Analysis:A Step-by-Step Guide",
    "section": "Clustering: K-means Method",
    "text": "Clustering: K-means Method\nK-means clustering is a partitioning method that divides data into a predetermined number of clusters, minimizing the within-cluster variance. The following steps include determining the optimal number of clusters and performing the clustering analysis.\n\nDetermining the Optimal Number of Clusters\nDetermining the optimal number of clusters is crucial for meaningful clustering. We employ several methods, including the within-cluster sum of squares (WSS), silhouette analysis, and Gap Statistic to identify the most appropriate number of clusters for the dataset.\n\nlibrary(factoextra)\n\n# Compute distance matrix and visualize\ndistance &lt;- get_dist(sdf)\nfviz_dist(distance, gradient = list(low = \"#00AFBB\", mid = \"white\", high = \"#FC4E07\"))\n\n\n\n\n\n\n\n# Elbow method for WSS\nfviz_nbclust(sdf, kmeans, method = \"wss\")\n\n\n\n\n\n\n\n# Silhouette method\nfviz_nbclust(sdf, kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n# Gap Statistic\nlibrary(cluster)\nset.seed(123)\ngap_stat &lt;- clusGap(sdf, FUN = kmeans, nstart = 25, K.max = 10, B = 50)\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = sdf, FUNcluster = kmeans, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 5.279958 5.652502 0.3725440 0.01149949\n [2,] 5.228562 5.598937 0.3703751 0.01042953\n [3,] 5.183834 5.560603 0.3767692 0.01100671\n [4,] 5.148115 5.529050 0.3809349 0.01071279\n [5,] 5.116327 5.501342 0.3850155 0.01052125\n [6,] 5.088564 5.476447 0.3878835 0.01032929\n [7,] 5.063318 5.453360 0.3900420 0.01027040\n [8,] 5.039872 5.431245 0.3913732 0.01002459\n [9,] 5.015518 5.410428 0.3949102 0.01009456\n[10,] 4.996567 5.390239 0.3936718 0.01019443\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\nThe WSS method, often referred to as the “elbow method,” helps to identify the point where the marginal gain in explanatory power diminishes significantly, indicating the optimal number of clusters. The silhouette method provides a measure of how similar an object is to its own cluster compared to other clusters. The Gap Statistic compares the total within intra-cluster variation for different numbers of clusters with their expected values under null reference distribution of the data.\n\n\nApplying the K-means Algorithm\nAfter identifying the optimal number of clusters, we apply the k-means algorithm and visualize the clusters.\n\nset.seed(123)\nkmeans_resultados &lt;- kmeans(sdf, centers = 4)  # Replace 4 with the optimal number of clusters\n\n# Visualization of clusters\nfviz_cluster(kmeans_resultados, data = sdf)\n\n\n\n\n\n\n\n\nThe results from k-means clustering provide insight into how the samples are grouped based on their genetic characteristics. Each cluster represents a group of samples that share similar traits, and the visualizations help in understanding the distribution of these clusters within the dataset."
  },
  {
    "objectID": "posts/conglomerados/index.html#hierarchical-clustering",
    "href": "posts/conglomerados/index.html#hierarchical-clustering",
    "title": "Potato Diversity Analysis:A Step-by-Step Guide",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering is another clustering technique that creates a hierarchy of clusters, which can be visualized as a tree or dendrogram. This method is particularly useful for visualizing the relationships between clusters and understanding the structure of the data.\n\nGenerating the Dendrogram\nWe begin by calculating the distance matrix using the Euclidean distance and applying the hierarchical clustering algorithm using Ward’s method, which minimizes the variance within clusters.\n\nlibrary(dendextend)\n\n# Compute distance matrix\ndistancias &lt;- dist(sdf, method = \"euclidean\")\n\n# Perform hierarchical clustering\nres.hc &lt;- hclust(d = distancias, method = \"ward.D2\")\n\n# Plot dendrogram\nplot(res.hc, cex = 0.6, hang = -1)\n\n\n\n\n\n\n\n# Cut the dendrogram to form clusters\ngrupos &lt;- cutree(res.hc, k = 4)  # Replace 4 with the desired number of clusters\n\nThe dendrogram allows us to visualize how the clusters are formed and how individual data points or groups of points merge as the level of clustering increases. By cutting the dendrogram at a particular height, we can extract a specific number of clusters that represent the underlying structure of the data.\n\n\nAssessment and Visualization\nDifferent methods of hierarchical clustering can be compared to evaluate which method best captures the structure of the data. We then visualize the dendrogram with colored clusters for better interpretability.\n\n# Comparison of different linkage methods\nm &lt;- c(\"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c(\"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(sdf, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.5463403 0.4630005 0.6197599 0.7498075 \n\n# Visualize dendrogram with colored clusters\nfviz_dend(res.hc, cex = 0.5, k = 4, \n          rect = TRUE, \n          k_colors = \"jco\", \n          rect_border = \"jco\", \n          rect_fill = TRUE)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n# Circular\nfviz_dend(res.hc, cex = 0.6, lwd = 0.7, k = 4,\n                 rect = TRUE,\n                 k_colors = \"jco\",\n                 rect_border = \"jco\",\n                 rect_fill = TRUE,\n                 type = \"circular\")\n\n\n\n\n\n\n\n\nHere, we assess different linkage methods such as average, single, complete, and Ward’s method. The agglomerative coefficient (AC) is computed for each method, with higher values indicating a better fit. The final dendrogram is color-coded to represent the different clusters, providing a clear visual summary of the hierarchical structure."
  },
  {
    "objectID": "posts/conglomerados/index.html#density-based-clustering-dbscan",
    "href": "posts/conglomerados/index.html#density-based-clustering-dbscan",
    "title": "Potato Diversity Analysis:A Step-by-Step Guide",
    "section": "Density-Based Clustering: DBSCAN",
    "text": "Density-Based Clustering: DBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a powerful clustering technique that identifies clusters based on density, allowing for the detection of outliers. Unlike k-means, DBSCAN does not require the number of clusters to be specified a priori, making it particularly useful for datasets with irregular cluster shapes.\n\nFinding the Optimal Epsilon and Applying DBSCAN\nThe epsilon parameter controls the neighborhood radius for density estimation. We begin by identifying an appropriate epsilon value through a k-NN distance plot.\n\nlibrary(dbscan)\n\n# Identify optimal epsilon using k-NN distance plot\nkNNdistplot(sdf, k = 5)\nabline(h = 0.5, col = \"red\")  # Adjust according to the plot\n\n\n\n\n\n\n\n# Apply DBSCAN algorithm\ndbscan_resultados &lt;- dbscan(sdf, eps = 0.5, minPts = 5)\n\n# Visualize DBSCAN results\nfviz_cluster(dbscan_resultados, data = sdf)\n\n\n\n\n\n\n\n\nThe k-NN distance plot helps to identify a natural choice for epsilon by observing where the plot exhibits the most pronounced change in slope. The DBSCAN algorithm is then applied, and the results are visualized. Clusters are identified based on dense regions of points, with outliers being points that do not belong to any cluster."
  },
  {
    "objectID": "posts/conglomerados/index.html#principal-component-analysis-pca",
    "href": "posts/conglomerados/index.html#principal-component-analysis-pca",
    "title": "Potato Diversity Analysis:A Step-by-Step Guide",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique that transforms the data into a set of orthogonal components, explaining the maximum variance with the fewest components. PCA is especially useful for visualizing high-dimensional data in two or three dimensions.\n\nPerforming PCA and Visualizing Results\nWe perform PCA on the scaled data and visualize the results, focusing on the variance explained by each component and the relationship between variables and samples.\n\n# Perform PCA\nacp_resultados &lt;- prcomp(sdf, center = TRUE, scale. = TRUE)\n\n# Summary of PCA results\nsummary(acp_resultados)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.8711 1.7506 1.57675 1.44454 1.32993 1.26967 1.25193\nProportion of Variance 0.1167 0.1022 0.08287 0.06956 0.05896 0.05374 0.05224\nCumulative Proportion  0.1167 0.2189 0.30173 0.37129 0.43024 0.48398 0.53622\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.14515 1.09504 1.04044 1.01979 0.97386 0.93209 0.92048\nProportion of Variance 0.04371 0.03997 0.03608 0.03467 0.03161 0.02896 0.02824\nCumulative Proportion  0.57994 0.61991 0.65599 0.69066 0.72227 0.75123 0.77947\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.87393 0.81854 0.81299 0.77542 0.75455 0.68666 0.65538\nProportion of Variance 0.02546 0.02233 0.02203 0.02004 0.01898 0.01572 0.01432\nCumulative Proportion  0.80493 0.82726 0.84930 0.86934 0.88832 0.90403 0.91835\n                         PC22    PC23    PC24    PC25    PC26   PC27    PC28\nStandard deviation     0.6318 0.61667 0.58666 0.54725 0.53177 0.4837 0.48009\nProportion of Variance 0.0133 0.01268 0.01147 0.00998 0.00943 0.0078 0.00768\nCumulative Proportion  0.9316 0.94433 0.95580 0.96579 0.97521 0.9830 0.99069\n                          PC29    PC30\nStandard deviation     0.43358 0.30199\nProportion of Variance 0.00627 0.00304\nCumulative Proportion  0.99696 1.00000\n\n# Scree plot showing explained variance by each component\nfviz_eig(acp_resultados)\n\n\n\n\n\n\n\n# Plot of individuals in the space of the first two principal components\nfviz_pca_ind(acp_resultados, geom.ind = \"point\", pointshape = 21, pointsize = 2, fill.ind = \"blue\", col.ind = \"black\", repel = TRUE)\n\n\n\n\n\n\n\n# Plot of variables in the space of the first two principal components\nfviz_pca_var(acp_resultados, col.var = \"contrib\", gradient.cols = c(\"blue\", \"yellow\", \"red\"), repel = TRUE)\n\n\n\n\n\n\n\n# Biplot showing both individuals and variables\nfviz_pca_biplot(acp_resultados, repel = TRUE)\n\n\n\n\n\n\n\n\nThe scree plot shows the proportion of variance explained by each principal component, aiding in the selection of the number of components to retain. The PCA biplots allow us to interpret the contribution of variables to the principal components and observe the distribution of samples along these components. This visualization is key to understanding the underlying structure of the data and the relationships between different genetic traits."
  },
  {
    "objectID": "posts/conglomerados/index.html#multiple-factor-analysis-mfa",
    "href": "posts/conglomerados/index.html#multiple-factor-analysis-mfa",
    "title": "Potato Diversity Analysis:A Step-by-Step Guide",
    "section": "Multiple Factor Analysis (MFA)",
    "text": "Multiple Factor Analysis (MFA)\nFor datasets containing both quantitative and qualitative variables, Multiple Factor Analysis (MFA) is applied. MFA is an extension of PCA that can handle mixed data types and is particularly useful in genetics studies where both types of data are often present.\n\nPerforming MFA and Interpreting Results\nWe perform MFA on the mixed dataset and visualize the results, focusing on the contributions of quantitative variables and the distribution of individuals across groups.\n\n# Convert character variables to factors\ndf2 &lt;-\n\n db %&gt;% \n  column_to_rownames(\"codigo_colecta\") %&gt;% \n  mutate_if(is.character, as.factor)\n\nlibrary(FactoMineR)\n\n# Perform MFA\nres.famd &lt;- FAMD(df2, graph = F)\nprint(res.famd)\n\n*The results are available in the following objects:\n\n  name          description                             \n1 \"$eig\"        \"eigenvalues and inertia\"               \n2 \"$var\"        \"Results for the variables\"             \n3 \"$ind\"        \"results for the individuals\"           \n4 \"$quali.var\"  \"Results for the qualitative variables\" \n5 \"$quanti.var\" \"Results for the quantitative variables\"\n\n# Scree plot showing the percentage of variance explained by each component\nfviz_screeplot(res.famd)\n\n\n\n\n\n\n\n# Plot showing the contribution of quantitative variables\nquanti.var &lt;- get_famd_var(res.famd, \"quanti.var\")\nfviz_famd_var(res.famd, \"quanti.var\", repel = TRUE, col.var = \"black\")\n\n\n\n\n\n\n\n# Visualization of individuals by group with confidence ellipses\nfviz_mfa_ind(res.famd, \n             habillage = \"Departamento\",\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\", \"darkgreen\"),\n             addEllipses = F, \n             ellipse.type = \"confidence\", \n             repel = TRUE,\n             label = \"none\"\n             )\n\n\n\n\n\n\n\n\nThe MFA analysis allows us to decompose the data into factors that explain the most variation, considering both types of variables. The scree plot helps determine how many factors to retain, while the visualizations provide insights into the contributions of individual variables and the grouping of samples based on both qualitative and quantitative characteristics."
  },
  {
    "objectID": "posts/conglomerados/index.html#conclusion",
    "href": "posts/conglomerados/index.html#conclusion",
    "title": "Potato Diversity Analysis:A Step-by-Step Guide",
    "section": "Conclusion",
    "text": "Conclusion\nThis workflow provides a comprehensive and detailed framework for conducting genetic diversity analysis in potato using various clustering and multivariate analysis techniques in R. By employing different methods, researchers can gain a deeper understanding of the genetic structure within their dataset, identify meaningful patterns of variation, and ultimately make informed decisions in breeding programs and conservation efforts. The choice of method depends on the specific nature of the data and the research objectives, whether it is to identify distinct genetic clusters, reduce data dimensionality, or both."
  },
  {
    "objectID": "posts/dca/index.html#cálculo-de-la-heredabilidad",
    "href": "posts/dca/index.html#cálculo-de-la-heredabilidad",
    "title": "Diseño Completamente al Azar",
    "section": "Cálculo de la Heredabilidad",
    "text": "Cálculo de la Heredabilidad\nLa heredabilidad en sentido amplio se puede estimar como la proporción de la varianza genética (entre variedades) sobre la varianza total (varianza genética + varianza residual). Para esto, ajustaremos un modelo mixto donde la variedad se considere como un efecto aleatorio."
  },
  {
    "objectID": "posts/dca/index.html#cálculo-de-blues",
    "href": "posts/dca/index.html#cálculo-de-blues",
    "title": "Diseño Completamente al Azar",
    "section": "Cálculo de BLUEs",
    "text": "Cálculo de BLUEs\nLos BLUEs se obtienen cuando tratamos los efectos de la variedad como fijos en un modelo lineal. En el contexto del diseño completamente aleatorizado (DCA), los BLUEs son simplemente las medias ajustadas (predicciones) para cada variedad."
  },
  {
    "objectID": "posts/dca/index.html#ejemplo",
    "href": "posts/dca/index.html#ejemplo",
    "title": "Diseño Completamente al Azar",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n# Ajustar el modelo mixto para calcular la heredabilidad\nlibrary(lme4)\n\nmod_mixed &lt;- lmer(yield ~ (1|variety), data = dat)\n\n# Resumen del modelo para obtener las varianzas\nsummary(mod_mixed)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: yield ~ (1 | variety)\n   Data: dat\n\nREML criterion at convergence: 144.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.95560 -0.58695 -0.03299  0.77728  1.48538 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n variety  (Intercept) 68.68    8.288   \n Residual             18.38    4.288   \nNumber of obs: 24, groups:  variety, 4\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   26.820      4.235   6.333\n\n# Extraer las varianzas para calcular la heredabilidad\nvar_comp &lt;- as.data.frame(VarCorr(mod_mixed))\nvar_genetic &lt;- var_comp$vcov[1]  # Varianza genética (entre variedades)\nvar_residual &lt;- var_comp$vcov[2] # Varianza residual\n\n# Calcular la heredabilidad en sentido amplio (H^2)\nH2 &lt;- var_genetic / (var_genetic + var_residual)\nH2\n\n[1] 0.7888692\n\n\n\nCalcular los BLUEs\nSi la variedad es un efecto fijo, los BLUEs coinciden con las medias ajustadas obtenidas en tu código anterior. Sin embargo, si quieres obtener los BLUEs explícitamente:\n\n# Ajustar un modelo con efectos fijos para la variedad\nmod_fixed &lt;- lm(yield ~ variety, data = dat)\n\n# Obtener los BLUEs\nBLUEs &lt;- emmeans(mod_fixed, ~ variety)\nBLUEs\n\n variety emmean   SE df lower.CL upper.CL\n v1        20.5 1.75 20     16.8     24.1\n v2        37.4 1.75 20     33.8     41.1\n v3        19.5 1.75 20     15.8     23.1\n v4        29.9 1.75 20     26.2     33.5\n\nConfidence level used: 0.95 \n\n\n\n\nAdición de Heredabilidad y BLUEs al Gráfico\nSi deseas agregar la heredabilidad y los BLUEs al gráfico, puedes hacerlo de la siguiente manera:\n\n# Calcular heredabilidad y obtener BLUEs\nH2 &lt;- round(H2, 2)\n\n# Calcular los BLUEs y las letras de Tukey\nBLUEs &lt;- emmeans(mod_fixed, ~ variety) %&gt;% \n  cld(adjust = \"Tukey\", Letters = letters) %&gt;% \n  as.data.frame()\n\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\n\n# Añadir heredabilidad como un texto en el gráfico\nmy_caption &lt;- paste(\"Los puntos negros representan los datos brutos. Los puntos rojos y las barras de error representan medias ajustadas con límites de confianza del 95% por variedad. Las medias seguidas de una letra común no son significativamente diferentes según la prueba de Tukey.\\nHeredabilidad: \", H2)\n\n# Crear el gráfico\nggplot() +\n  aes(x = variety) +\n  # black dots representing the raw data\n  geom_point(\n    data = dat,\n    aes(y = yield)\n  ) +\n  # red dots representing the adjusted means\n  geom_point(\n    data = BLUEs,\n    aes(y = emmean),\n    color = \"red\",\n    position = position_nudge(x = 0.1)\n  ) +\n  # red error bars representing the confidence limits of the adjusted means\n  geom_errorbar(\n    data = BLUEs,\n    aes(ymin = lower.CL, ymax = upper.CL),\n    color = \"red\",\n    width = 0.1,\n    position = position_nudge(x = 0.1)\n  ) +\n  # red letters \n  geom_text(\n    data = BLUEs,\n    aes(y = emmean, label = str_trim(.group)),\n    color = \"red\",\n    position = position_nudge(x = 0.2),\n    hjust = 0\n  ) +\n  scale_x_discrete(\n    name = \"Variety\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  theme_classic() +\n  labs(caption = my_caption) +\n  theme(plot.caption = element_textbox_simple(margin = margin(t = 5)),\n        plot.caption.position = \"plot\")\n\n\n\n\n\n\n\n\nEste código te permite visualizar las medias ajustadas junto con la heredabilidad calculada y los BLUEs."
  },
  {
    "objectID": "posts/back-transformation/index.html",
    "href": "posts/back-transformation/index.html",
    "title": "Back-transformations with emmeans() in R",
    "section": "",
    "text": "En el análisis de datos, especialmente en disciplinas como la estadística y el machine learning, es común aplicar transformaciones a los datos para mejorar la calidad de los análisis. Estas transformaciones ayudan a cumplir con supuestos estadísticos como la normalidad o la homogeneidad de varianzas, y a menudo permiten obtener modelos más robustos y precisos. Sin embargo, una vez que hemos realizado el análisis, surge la necesidad de interpretar los resultados en la escala original de los datos. Aquí es donde entran en juego las transformaciones inversas.\nLas transformaciones inversas son las operaciones matemáticas que revertimos para devolver los datos transformados a su forma original. Esto es crucial cuando queremos comunicar los hallazgos de manera que tengan sentido práctico y comprensible. Por ejemplo, si aplicamos una transformación logarítmica para estabilizar la varianza de un conjunto de datos, la transformación inversa —en este caso, la exponenciación— nos permite expresar los resultados en la misma escala en la que los datos fueron originalmente recolectados.\nAlgunas de las transformaciones más comunes incluyen la logarítmica, la raíz cuadrada y la Box-Cox, cada una con su correspondiente transformación inversa. Estas técnicas son especialmente útiles en áreas como la biología, la economía y la ingeniería, donde los datos a menudo no cumplen con las distribuciones ideales para un análisis directo."
  },
  {
    "objectID": "posts/back-transformation/index.html#introducción",
    "href": "posts/back-transformation/index.html#introducción",
    "title": "Back-transformations with emmeans() in R",
    "section": "",
    "text": "En el análisis de datos, especialmente en disciplinas como la estadística y el machine learning, es común aplicar transformaciones a los datos para mejorar la calidad de los análisis. Estas transformaciones ayudan a cumplir con supuestos estadísticos como la normalidad o la homogeneidad de varianzas, y a menudo permiten obtener modelos más robustos y precisos. Sin embargo, una vez que hemos realizado el análisis, surge la necesidad de interpretar los resultados en la escala original de los datos. Aquí es donde entran en juego las transformaciones inversas.\nLas transformaciones inversas son las operaciones matemáticas que revertimos para devolver los datos transformados a su forma original. Esto es crucial cuando queremos comunicar los hallazgos de manera que tengan sentido práctico y comprensible. Por ejemplo, si aplicamos una transformación logarítmica para estabilizar la varianza de un conjunto de datos, la transformación inversa —en este caso, la exponenciación— nos permite expresar los resultados en la misma escala en la que los datos fueron originalmente recolectados.\nAlgunas de las transformaciones más comunes incluyen la logarítmica, la raíz cuadrada y la Box-Cox, cada una con su correspondiente transformación inversa. Estas técnicas son especialmente útiles en áreas como la biología, la economía y la ingeniería, donde los datos a menudo no cumplen con las distribuciones ideales para un análisis directo."
  },
  {
    "objectID": "posts/back-transformation/index.html#tipos-de-transformación",
    "href": "posts/back-transformation/index.html#tipos-de-transformación",
    "title": "Back-transformations with emmeans() in R",
    "section": "Tipos de transformación",
    "text": "Tipos de transformación\nA continuación, te presento algunas de las transformaciones más comunes y sus inversas:\n\n1. Transformación Logarítmica (log)\n\nTransformación: Se utiliza para reducir la asimetría positiva o estabilizar la varianza cuando los datos tienen valores grandes.\n\nFórmula: \\(y' = \\log(y)\\), generalmente en base natural (\\(\\log_e\\)) o base 10.\n\nInversa: La operación inversa es la exponenciación.\n\nFórmula: \\(y = e^{y'}\\) o \\(y = 10^{y'}\\), dependiendo de la base logarítmica utilizada.\n\n\n\n\n2. Transformación de Raíz Cuadrada\n\nTransformación: Es útil cuando los datos tienen una distribución con una leve asimetría positiva o cuando se busca estabilizar la varianza.\n\nFórmula: \\(y' = \\sqrt{y}\\)\n\nInversa: La inversa de esta transformación es elevar al cuadrado.\n\nFórmula: \\(y = (y')^2\\)\n\n\n\n\n3. Transformación Inversa (1/y)\n\nTransformación: Se utiliza cuando los datos tienen una tendencia lineal negativa o para ajustar relaciones hiperbólicas.\n\nFórmula: \\(y' = \\frac{1}{y}\\)\n\nInversa: La transformación inversa es la operación recíproca.\n\nFórmula: \\(y = \\frac{1}{y'}\\)\n\n\n\n\n4. Transformación de Potencia (Box-Cox)\n\nTransformación: Box-Cox es una familia de transformaciones que busca estabilizar la varianza y normalizar los datos. Se controla con un parámetro \\(\\lambda\\).\n\nFórmula: \\(y' = \\frac{y^\\lambda - 1}{\\lambda}\\) (si \\(\\lambda \\neq 0\\)), o \\(y' = \\log(y)\\) (si \\(\\lambda = 0\\)).\n\nInversa: La inversa depende del valor de \\(\\lambda\\).\n\nSi \\(\\lambda = 0\\): \\(y = e^{y'}\\)\nSi \\(\\lambda \\neq 0\\): \\(y = (y' \\cdot \\lambda + 1)^{1/\\lambda}\\)\n\n\n\n\n5. Transformación Logística\n\nTransformación: Se utiliza en modelos de regresión logística, donde se transforma una probabilidad \\(p\\) en el logit (logaritmo de las probabilidades).\n\nFórmula: \\(y' = \\log \\left( \\frac{p}{1-p} \\right)\\)\n\nInversa: La inversa se conoce como la función logística o sigmoide.\n\nFórmula: \\(p = \\frac{1}{1 + e^{-y'}}\\)\n\n\n\n\n6. Transformación de Arcoseno (arcsin)\n\nTransformación: Es usada comúnmente en proporciones o datos de frecuencia que están entre 0 y 1, para normalizarlos.\n\nFórmula: \\(y' = \\arcsin(\\sqrt{y})\\)\n\nInversa: La inversa es la función seno al cuadrado.\n\nFórmula: \\(y = (\\sin(y'))^2\\)\n\n\n\n\n7. Transformación Log-Log\n\nTransformación: Se utiliza cuando tanto la variable dependiente como la independiente están en una escala logarítmica.\n\nFórmula: \\(y' = \\log(y)\\) y \\(x' = \\log(x)\\)\n\nInversa: La exponenciación de ambas variables devuelve a su escala original.\n\nFórmula: \\(y = e^{y'}\\), \\(x = e^{x'}\\)"
  },
  {
    "objectID": "posts/back-transformation/index.html#back-transformations-con-emmeans",
    "href": "posts/back-transformation/index.html#back-transformations-con-emmeans",
    "title": "Back-transformations with emmeans() in R",
    "section": "Back-transformations con emmeans()",
    "text": "Back-transformations con emmeans()\nEn el paquete emmeans de R, las back-transformations (transformaciones inversas) se utilizan para devolver las estimaciones de medias marginales de los efectos al espacio original de los datos después de que se haya aplicado una transformación en el modelo. Esto es útil cuando se ha ajustado un modelo con una transformación, como una transformación logarítmica o de raíz cuadrada, y deseas interpretar los resultados en la escala original de los datos.\n\n¿Cómo funciona?\nCuando aplicas una transformación a los datos (por ejemplo, logaritmo), el modelo ajustado trabaja en la escala transformada, y por lo tanto, las estimaciones de medias marginales predichas estarán en esa misma escala. Sin embargo, emmeans() permite aplicar automáticamente la transformación inversa, devolviendo las medias estimadas a su escala original para una interpretación más fácil.\nEl paquete emmeans detecta automáticamente si la transformación se usó en el modelo y aplica la transformación inversa al calcular los Estimated Marginal Means (EMMs). Esto se puede controlar utilizando el argumento type = \"response\" para realizar la transformación inversa al solicitar los EMMs.\n\n\nEjemplo de uso de emmeans() con back-transformations\nSupongamos que tienes un modelo lineal generalizado (GLM) con una transformación logarítmica aplicada a la variable de respuesta. A continuación te muestro cómo podrías aplicar una back-transformation usando emmeans():\n\nPaso 1: Ajustar el modelo\n# Ejemplo de un GLM con transformación logarítmica\nmodelo &lt;- glm(y ~ tratamiento, family = gaussian(link = \"log\"), data = datos)\nEn este caso, y es la variable de respuesta transformada mediante un enlace logarítmico.\n\n\nPaso 2: Obtener los EMMs con la transformación inversa\nlibrary(emmeans)\n\n# Obtener las medias marginales estimadas (EMMs) y aplicar back-transformation\nem_means &lt;- emmeans(modelo, ~ tratamiento, type = \"response\")\n\nEl argumento type = \"response\" le indica a emmeans() que aplique la transformación inversa (en este caso, exponenciación, ya que se usó una transformación logarítmica).\nSi no se especifica type = \"response\", emmeans() devolvería las estimaciones en la escala transformada (logarítmica en este caso).\n\n\n\nPaso 3: Interpretar los resultados\n# Mostrar los resultados back-transformados\nsummary(em_means)\nEsto te mostrará las estimaciones en la escala original de los datos, lo cual es más fácil de interpretar en términos del contexto del problema. Además, los intervalos de confianza también serán transformados inversamente.\n\n\n\nEjemplo con Transformación de Raíz Cuadrada\nSi usas una transformación de raíz cuadrada en lugar de logarítmica, la back-transformation consistirá en elevar al cuadrado los resultados.\n\nPaso 1: Ajustar el modelo\nmodelo_sqrt &lt;- lm(sqrt(y) ~ tratamiento, data = datos)\n\n\nPaso 2: Obtener las medias marginales estimadas con la transformación inversa\n# Obtener las medias marginales estimadas con back-transformation (cuadrado)\nem_means_sqrt &lt;- emmeans(modelo_sqrt, ~ tratamiento, type = \"response\")\nEn este caso, emmeans() elevará las medias estimadas al cuadrado para devolverlas a la escala original.\n\n\nPaso 3: Ver los resultados\nsummary(em_means_sqrt)\nCuando usas el paquete emmeans() en R, puedes realizar back-transformations fácilmente con el argumento type = \"response\". Este enfoque es útil cuando has ajustado un modelo con una transformación en la variable de respuesta y deseas interpretar las medias estimadas en la escala original de los datos."
  },
  {
    "objectID": "posts/back-transformation/index.html#ejercicio-practico",
    "href": "posts/back-transformation/index.html#ejercicio-practico",
    "title": "Back-transformations with emmeans() in R",
    "section": "Ejercicio practico",
    "text": "Ejercicio practico\n\nlibrary(emmeans)\n\nWarning: package 'emmeans' was built under R version 4.4.1\n\n\nWelcome to emmeans.\nCaution: You lose important information if you filter this package's results.\nSee '? untidy'\n\nfileName &lt;- \"http://www.casaonofri.it/_datasets/insects.csv\"\ndataset &lt;- read.csv(fileName)\ndataset$Insecticide &lt;- as.factor(dataset$Insecticide)\ndataset$Count &lt;- as.numeric(dataset$Count)\nhead(dataset)\n\n  Insecticide Rep Count\n1          T1   1   448\n2          T1   2   906\n3          T1   3   484\n4          T1   4   477\n5          T1   5   634\n6          T2   1   211\n\n\n\nmodel &lt;- lm(log(Count) ~ Insecticide, data = dataset)\nemmeans(model, ~Insecticide, type = \"response\")\n\n Insecticide response     SE df lower.CL upper.CL\n T1             568.6 101.01 12    386.1    837.3\n T2             335.1  59.54 12    227.6    493.5\n T3              51.9   9.22 12     35.2     76.4\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\n\nDesafortunadamente, no todas las transformaciones son autodetectadas; por ejemplo, consideremos el conjunto de datos ‘Hours_to_failure.csv’, donde tenemos el tiempo hasta el fallo (en horas) de un dispositivo, afectado por la temperatura de funcionamiento. Si consideramos la temperatura como un factor, podemos ajustar un modelo ANOVA; una comprobación con la función boxcox() del paquete MASS sugiere que este conjunto de datos podría requerir una transformación estabilizadora en el valor inverso.\n\nlibrary(MASS)\nlibrary(emmeans)\nfileName &lt;- \"http://www.casaonofri.it/_datasets/Hours_to_failure.csv\"\ndataset &lt;- read.csv(fileName)\ndataset$Temp &lt;- factor(dataset$Temp)\nhead(dataset)\n\n  Temp Hours_to_failure\n1 1520             1953\n2 1520             2135\n3 1520             2471\n4 1520             4727\n5 1520             6134\n6 1520             6314\n\n\n\nmodel &lt;- lm(Hours_to_failure ~ Temp, data = dataset)\ntp &lt;- boxcox(model)\n\n\n\n\n\n\n\n\n\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo vemos a continuación, la transformación inversa no se detecta automáticamente.\n\nmodel2 &lt;- lm(I(1/Hours_to_failure) ~ Temp, data = dataset)\nemmeans(model2, ~ Temp, type = \"response\")\n\n Temp response       SE df lower.CL upper.CL\n 1520 0.000320 9.52e-05 20 0.000121 0.000518\n 1620 0.000579 9.52e-05 20 0.000381 0.000778\n 1660 0.001043 9.52e-05 20 0.000844 0.001241\n 1708 0.001565 9.52e-05 20 0.001366 0.001763\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the identity scale \n\n\nEn esta situación, hay que utilizar un enfoque alternativo. La transformación puede realizarse antes de ajustar el modelo; a continuación, tenemos que actualizar la «rejilla de referencia» del modelo, especificando qué tipo de transformación hemos realizado (tran = \"inverse\"). Por último, podemos pasar la rejilla actualizada a la función emmeans(). Y… ¡la transformación inversa está servida!\n\ndataset$invHours &lt;- 1/dataset$Hours_to_failure\nmodel3 &lt;- lm(invHours ~ Temp, data = dataset)\nupdGrid &lt;- update(ref_grid(model3), tran = \"inverse\")\nemmeans(updGrid, ~Temp, type = \"response\")\n\n Temp response    SE df lower.CL upper.CL\n 1520     3128 931.6 20     1930     8258\n 1620     1726 283.6 20     1285     2626\n 1660      959  87.6 20      806     1185\n 1708      639  38.9 20      567      732\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the inverse scale \n\n\n\nplot(model3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA veces, necesito un extra de flexibilidad. Por ejemplo, si nos fijamos en el gráfico ‘boxcox’ anterior, vemos que la transformación inversa, aunque aceptable (el valor \\(-1\\) está dentro de los límites de confianza para (\\(\\lambda\\)) no es la mejor. De hecho, el valor de máxima verosimilitud es -0,62:\n\ntp$x[which.max(tp$y)]\n\n[1] -0.6262626\n\n\nPor lo tanto, podríamos utilizar la siguiente transformación, que está dentro de la familia ‘boxcox’:\n\\[\nW = \\frac{Y^{-0,62} - 1}{-0,62}\n\\]\nEste tipo de transformación no es manejable con el código anterior y necesitamos utilizar la función make.tran(), especificando el valor de \\(\\lambda\\) (alpha = -0.62) y el valor del parámetro desplazamiento (beta = 1).\n\ndataset$bcHours &lt;- (dataset$Hours_to_failure^(-0.62) - 1)/(-0.62)\nmodel4 &lt;- lm(bcHours ~ Temp, data = dataset)\nupdGrid &lt;- update(ref_grid(model4), \n                  tran = make.tran(\"boxcox\", alpha = -0.62,\n                                   beta = 1))\nemmeans(updGrid, ~Temp, type = \"response\")\n\n Temp response    SE df lower.CL upper.CL\n 1520     3265 708.8 20     2191     5556\n 1620     1763 260.9 20     1329     2483\n 1660      976 100.0 20      798     1227\n 1708      642  50.7 20      549      764\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the Box-Cox (lambda = -0.62) of (y - 1) scale \n\n\n\nplot(model4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa función make.tran() puede utilizarse para especificar otras funciones de transformación, como la transformación angular que se utiliza a menudo para porcentajes y proporciones. Puede obtener una lista completa buscando en help (?make.tran) desde dentro de R."
  },
  {
    "objectID": "posts/back-transformation/index.html#conclusión",
    "href": "posts/back-transformation/index.html#conclusión",
    "title": "Back-transformations with emmeans() in R",
    "section": "Conclusión",
    "text": "Conclusión\nLas transformaciones estabilizadoras, a pesar de su antigüedad, pueden seguir siendo útiles para ajustar modelos heteroscedásticos; ¡no las infravalore sólo porque ya no estén de moda!"
  }
]